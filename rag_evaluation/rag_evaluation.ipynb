{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Install pre-requisites",
   "id": "3a65285e868d7809"
  },
  {
   "cell_type": "code",
   "id": "3c7ba6dbea18e2ef",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-30T07:06:05.920015Z",
     "start_time": "2024-06-30T07:05:59.576042Z"
    }
   },
   "source": "!pip install -q torch transformers langchain sentence-transformers tqdm openpyxl openai pandas datasets ragatouille ratelimit retry duckdb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "dataset 1.6.2 requires sqlalchemy<2.0.0,>=1.3.2, but you have sqlalchemy 2.0.31 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "d2c4b6af65d58cdf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Model preparations\n",
    "\n",
    "To go through the evaluation process, we need following models:\n",
    "\n",
    "1. Document model: Embedding model to generate document embeddings which will persisted in vector index. \n",
    "2. Reader model: A text completion model to answer the final question with augmented context.\n",
    "3. Evaluator model: A chat completion model that will give final verdict about RAG output. As this model will affect scoring considerably, stronger model should be used. \n",
    "\n",
    "As the choice of different models is not subject of this article and won't impact the comparison between RAG frameworks, we are determined to use completed local solution for this experiment for better speed and lower cost. \n",
    "\n",
    "To be more precise, following models that are already optimized in Ollama are used:\n",
    "\n",
    "* [all-minilm](https://huggingface.co/google/gemma-2b) for `Embedding model`;\n",
    "* [Mixtral-8x7B](https://ollama.com/library/mixtral) for `Evaluator model` and `Reader model`\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "id": "b4dfdc109f0daffc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-30T07:06:05.987881Z",
     "start_time": "2024-06-30T07:06:05.921451Z"
    }
   },
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatOllama, MiniMaxChat\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "import os\n",
    "\n",
    "# points to a OpenAI compatible server, like vLLM or llama.cpp server\n",
    "MIXTRAL_ENDPOINT = \"http://192.168.0.134:8000\"\n",
    "\n",
    "# points to a ollama server\n",
    "MINILM_ENDPOINT = \"http://192.168.0.132:11434\"\n",
    "\n",
    "READER_MODEL_NAME = \"llama-3-70b-IQ4_NL-guff\"\n",
    "EMBEDDING_NAME = \"all-minilm\"\n",
    "EVALUATOR_NAME = \"llama-3-70b-IQ4_NL-guff\"\n",
    "\n",
    "EMBEDDING_MODEL = OllamaEmbeddings(model=EMBEDDING_NAME, base_url = MINILM_ENDPOINT)\n",
    "READER_LLM = ChatOpenAI(model=READER_MODEL_NAME, base_url = MIXTRAL_ENDPOINT)\n",
    "EVAL_MODEL = ChatOpenAI(model=EVALUATOR_NAME, base_url = MIXTRAL_ENDPOINT)\n",
    "\n",
    "LANGCHAIN_DATA_ROOT = \"./data/langchain\"\n",
    "INSTINCT_DOC_AGENT_DATA_ROOT = \"./data/doc_agent\"\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "8088b64de81ed222",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T07:06:08.946369Z",
     "start_time": "2024-06-30T07:06:05.988492Z"
    }
   },
   "source": [
    "# Test all these models\n",
    "\n",
    "EMBEDDING_MODEL.embed_query(\"hello\")\n",
    "\n",
    "READER_LLM.invoke(\"hello\")\n",
    "\n",
    "EVAL_MODEL.invoke(\"hello\")\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 10, 'total_tokens': 20}, 'model_name': 'llama-3-70b-IQ4_NL-guff', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e55b2802-2f87-4e7a-a4f7-c5906de9db6f-0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "3bdb8cc674412799",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Build RAG pipeline using `langchain` \n",
    "\n",
    "1. transform training data in `m-ric/huggingface_doc` to `langchain`'s document objects\n",
    "2. Load into faiss index if index file is absent\n",
    "3. prompt with eval data `m-ric/huggingface_doc` using `READER_MODEL` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881763874245235f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Knowledge base preparations"
   ]
  },
  {
   "cell_type": "code",
   "id": "cdd427a5684551d2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-30T07:06:08.953486Z",
     "start_time": "2024-06-30T07:06:08.949552Z"
    }
   },
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import datasets\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "3a855d46ab06e6ec",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-30T07:06:08.957891Z",
     "start_time": "2024-06-30T07:06:08.955306Z"
    }
   },
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\""
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "4e60ad11cd96e14c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-30T07:06:08.962686Z",
     "start_time": "2024-06-30T07:06:08.959270Z"
    }
   },
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independent from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\""
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "921990aa1cb355f0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-30T07:06:45.602315Z",
     "start_time": "2024-06-30T07:06:08.963695Z"
    }
   },
   "source": [
    "EVAL_DATASET = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since m-ric/huggingface_doc_qa_eval couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/robinqu/.cache/huggingface/datasets/m-ric___huggingface_doc_qa_eval/default/0.0.0/72d15fdd245839652aa30a5f8717b3b79f106c2a (last modified on Thu Mar 28 20:55:20 2024).\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "11ea5ea55c0da346",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-30T07:07:03.719757Z",
     "start_time": "2024-06-30T07:06:45.602915Z"
    }
   },
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in tqdm(datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\"))\n",
    "]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9f4ea8170184931a88f6a6a02ffad6b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "40aa9cb71de4a46a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-30T07:07:03.723341Z",
     "start_time": "2024-06-30T07:07:03.720590Z"
    }
   },
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument]\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        model_name=\"gpt-4\",\n",
    "        chunk_size=chunk_size,\n",
    "        # chunk_overlap=int(chunk_size / 10),\n",
    "        chunk_overlap=0,\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "        disallowed_special=[],\n",
    "        allowed_special=\"all\"\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "13dc9ec746ddadd2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-30T07:08:09.457715Z",
     "start_time": "2024-06-30T07:08:09.454467Z"
    }
   },
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "import os\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "\n",
    "def load_embeddings(\n",
    "    langchain_docs: List[LangchainDocument],\n",
    "    chunk_size: int,\n",
    "    embedding_model: Embeddings,\n",
    "    embedding_model_name: str\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
    "\n",
    "    Args:\n",
    "        langchain_docs: list of documents\n",
    "        chunk_size: size of the chunks to split the documents into\n",
    "        embedding_model: the embedding\n",
    "        embedding_model_name: name of the embedding model to use\n",
    "\n",
    "    Returns:\n",
    "        FAISS index\n",
    "         \n",
    "    \"\"\"\n",
    "    # load embedding_model\n",
    "\n",
    "\n",
    "    # Check if embeddings already exist on disk\n",
    "    index_name = f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name}\"\n",
    "    index_folder_path = os.path.join(LANGCHAIN_DATA_ROOT, index_name)\n",
    "    if os.path.isdir(index_folder_path):\n",
    "        return FAISS.load_local(\n",
    "            index_folder_path,\n",
    "            embedding_model,\n",
    "            distance_strategy=DistanceStrategy.COSINE,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        docs_processed = split_documents(\n",
    "            chunk_size,\n",
    "            langchain_docs\n",
    "        )\n",
    "        print(f\"Index not found, generating it... {len(docs_processed)} docs in total\")\n",
    "        knowledge_index = FAISS.from_documents(\n",
    "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "        knowledge_index.save_local(index_folder_path)\n",
    "        return knowledge_index"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "id": "74338ffcd9971f99",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## QA chain"
   ]
  },
  {
   "cell_type": "code",
   "id": "851eb803b3f0a34f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-30T07:07:03.735674Z",
     "start_time": "2024-06-30T07:07:03.734080Z"
    }
   },
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "id": "283a72f828eb8ec3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-30T07:15:12.154777Z",
     "start_time": "2024-06-30T07:15:12.149695Z"
    }
   },
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: BaseChatModel,\n",
    "    knowledge_index: VectorStore,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "    \n",
    "    print(\"final prompt size:\", len(final_prompt))\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = llm.invoke(final_prompt)\n",
    "\n",
    "    return answer.content, relevant_docs"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "id": "eeda09db98f634b8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Generating answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110434a4f3c3b6d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Test function with langchain"
   ]
  },
  {
   "cell_type": "code",
   "id": "913cbbdae2722e2d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-30T07:07:03.743507Z",
     "start_time": "2024-06-30T07:07:03.740432Z"
    }
   },
   "source": [
    "from langchain_core.language_models import BaseChatModel \n",
    "\n",
    "def run_langchain_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    llm: LLM,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        \n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(question, llm, knowledge_index, reranker=reranker)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "78de170f745561d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-30T07:07:03.747249Z",
     "start_time": "2024-06-30T07:07:03.744268Z"
    }
   },
   "source": [
    "def run_langchain_test_all() -> str:\n",
    "    \"\"\"\n",
    "    Build index and run langchain test with fixed parameter and model selections\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    if not os.path.exists(\"./output\"):\n",
    "        os.mkdir(\"./output\")\n",
    "    \n",
    "    chunk_size = 200\n",
    "    rerank = False\n",
    "    \n",
    "    settings_name = f\"langchain_chunk:{chunk_size}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}_embedding-model:{EMBEDDING_NAME}\"\n",
    "    output_file_name = os.path.join(\"./output\", f\"{settings_name}.json\")\n",
    "    \n",
    "\n",
    "    print(\"Loading knowledge base embeddings...\")\n",
    "    knowledge_index = load_embeddings(\n",
    "        RAW_KNOWLEDGE_BASE,\n",
    "        chunk_size=chunk_size,\n",
    "        embedding_model=EMBEDDING_MODEL,\n",
    "        embedding_model_name=EMBEDDING_NAME\n",
    "    )\n",
    "    \n",
    "    print(f\"Running RAG with {settings_name}\")\n",
    "    reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\") if rerank else None\n",
    "    run_langchain_rag_tests(\n",
    "        eval_dataset=EVAL_DATASET,\n",
    "        llm=READER_LLM,\n",
    "        knowledge_index=knowledge_index,\n",
    "        output_file=output_file_name,\n",
    "        reranker=reranker,\n",
    "        verbose=True,\n",
    "        test_settings=settings_name,\n",
    "    )\n",
    "    \n",
    "    return output_file_name "
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "96d9c29a10db3e84",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-30T07:21:28.905053Z",
     "start_time": "2024-06-30T07:15:15.436651Z"
    }
   },
   "source": [
    "# execute test for langchain\n",
    "LANGCHAIN_TEST_OUTPUT = run_langchain_test_all()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading knowledge base embeddings...\n",
      "Running RAG with langchain_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22641cd390cb4230bbc9713d589f241b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final prompt size: 2665\n",
      "=======================================================\n",
      "Question: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
      "\n",
      "Answer: The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture.\n",
      "\n",
      "Source document: Document 0\n",
      "True answer: x86_64-unknown-linux-musl\n",
      "final prompt size: 3675\n",
      "=======================================================\n",
      "Question: What is the purpose of the BLIP-Diffusion model?\n",
      "\n",
      "Answer: The purpose of the BLIP-Diffusion model is not explicitly mentioned in the given context.\n",
      "True answer: The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.\n",
      "final prompt size: 4787\n",
      "=======================================================\n",
      "Question: How can a user claim authorship of a paper on the Hugging Face Hub?\n",
      "\n",
      "Answer: A user can claim authorship of a paper on the Hugging Face Hub by clicking on their name in the corresponding Paper page and then clicking \"claim authorship\". This will redirect the user to their paper settings where they can confirm the request. The admin team will then validate the request, and once confirmed, the Paper page will show as verified. (Source Document 1)\n",
      "True answer: By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.\n",
      "final prompt size: 4199\n",
      "=======================================================\n",
      "Question: What is the purpose of the /healthcheck endpoint in the Datasets server API?\n",
      "\n",
      "Answer: The purpose of the `/healthcheck` endpoint in the Datasets server API is to ensure that the app is running. This endpoint allows users to check the health status of the server, making it easier to monitor and maintain the server's performance.\n",
      "\n",
      "Source document: Document 0, Document 2, Document 4, Document 5, Document 6\n",
      "True answer: Ensure the app is running\n",
      "final prompt size: 5488\n",
      "=======================================================\n",
      "Question: What is the default context window size for Local Attention in the LongT5 model?\n",
      "\n",
      "Answer: The default context window size for Local Attention in the LongT5 model is not explicitly mentioned in the context provided. However, it can be inferred that the context window size can be defined by the user through the `config.attention_window` parameter. If `config.attention_window` is of type `List`, a different window size can be defined for each layer.\n",
      "True answer: 127 tokens\n",
      "final prompt size: 5344\n",
      "=======================================================\n",
      "Question: What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
      "\n",
      "Answer: The method used to load a checkpoint for a task using `AutoPipeline` is `AutoPipeline.from_pretrained()`. This method automatically detects the correct pipeline class to use based on the task.\n",
      "True answer: from_pretrained()\n",
      "final prompt size: 2532\n",
      "=======================================================\n",
      "Question: What is the purpose of Diffusers library?\n",
      "\n",
      "Answer: The purpose of the Diffusers library is to provide a framework for creating and using diffusion models. It is built as a natural extension of PyTorch, focusing on usability and simplicity. The library supports various architectures, such as Stable Diffusion, and aims to be light-weight with few required dependencies. It is designed to be compatible with different platforms and accelerators, making it suitable for both research and product development. (Source: Document 3, Document 5, and Document 6)\n",
      "True answer: To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.\n",
      "final prompt size: 4839\n",
      "=======================================================\n",
      "Question: What method does the EulerAncestralDiscreteScheduler use for sampling?\n",
      "\n",
      "Answer: The EulerAncestralDiscreteScheduler uses ancestral sampling for generating outputs. This information can be found in Document 0.\n",
      "True answer: Ancestral sampling with Euler method steps.\n",
      "final prompt size: 4513\n",
      "=======================================================\n",
      "Question: What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?\n",
      "\n",
      "Answer: The large multimodal model that can solve image-text tasks and is based on Flamingo is called IDEFICS. (Document 1)\n",
      "True answer: IDEFICS\n",
      "final prompt size: 2988\n",
      "=======================================================\n",
      "Question: What is the purpose of the `gradio.Blocks` API?\n",
      "\n",
      "Answer: The purpose of the `gradio.Bounds` API is to provide a low-level API that allows full control over the data flows and layout of an application, allowing for the creation of complex, multi-step applications. This API is introduced in Gradio 3.0, which is the biggest update to the library ever. (Document 1)\n",
      "True answer: The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications.\n",
      "final prompt size: 4195\n",
      "=======================================================\n",
      "Question: What is the purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\"?\n",
      "\n",
      "Answer: The purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\" is to generate high-resolution images with more coherent compositions by utilizing a hierarchical approach that first generates a low-resolution image using a text-conditional denoising diffusion model, and then upscales it to a high-resolution image using another diffusion model. This model is referred to as unCLIP and is available in the 🤗 Diffusers library. The original codebase can be found at kakaobrain's karlo repository.\n",
      "True answer: The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.\n",
      "final prompt size: 3725\n",
      "=======================================================\n",
      "Question: What command is used to install the requirements for a research project using 🤗 Transformers?\n",
      "\n",
      "Answer: The command used to install the requirements for a research project using 🤗 Transformers is:\n",
      "\n",
      "```bash\n",
      "pip install -r requirements.txt\n",
      "```\n",
      "\n",
      "Source Document: [1](https://github.com/huggingface/transformers/tree/main/examples/research_projects/codeparrot/examples), [3](https://github.com/huggingface/transformers/tree/main/examples/research_projects)\n",
      "True answer: pip install -r requirements.txt\n",
      "final prompt size: 4223\n",
      "=======================================================\n",
      "Question: What task does the `roberta-large-mnli` checkpoint perform?\n",
      "\n",
      "Answer: The `roberta-large-mnli` checkpoint performs the MultiNLI task. (Document 0)\n",
      "True answer: Text classification\n",
      "final prompt size: 4656\n",
      "=======================================================\n",
      "Question: What service is replacing the Paid tier of the Inference API at Hugging Face?\n",
      "\n",
      "Answer: The service replacing the Paid tier of the Inference API at Hugging Face is Inference Endpoints. (Document 5)\n",
      "True answer: Inference Endpoints\n",
      "final prompt size: 4471\n",
      "=======================================================\n",
      "Question: What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?\n",
      "\n",
      "Answer: SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers. This information can be found in Document 0.\n",
      "Reference(s):\n",
      "- Document 0: https://arxiv.org/abs/2006.11316\n",
      "True answer: Grouped convolutions\n",
      "final prompt size: 4594\n",
      "=======================================================\n",
      "Question: What type of license is the HuggingFace Team's software distributed under?\n",
      "\n",
      "Answer: The HuggingFace Team's software is distributed under the Apache License, Version 2.0. (Document 2, Document 4, Document 5, Document 6)\n",
      "True answer: Apache License, Version 2.0\n",
      "final prompt size: 5354\n",
      "=======================================================\n",
      "Question: What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?\n",
      "\n",
      "Answer: The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are: \n",
      "1. Splitting the embedding matrix into two smaller matrices.\n",
      "2. Using repeating layers split among groups.\n",
      "\n",
      "Source Document: Document 0\n",
      "True answer: Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.\n",
      "final prompt size: 4914\n",
      "=======================================================\n",
      "Question: What are the three main steps for fine-tuning a model with the 🤗 Datasets library?\n",
      "\n",
      "Answer: The three main steps for fine-tuning a model with the 🤗 Datasets library are:\n",
      "1. Load a dataset from the Hugging Face Hub.\n",
      "2. Preprocess the data with `Dataset.map()`.\n",
      "3. Load and compute metrics.\n",
      "\n",
      "Reference(s):\n",
      "- Document 0: \"In [Chapter 3](/course/chapter3) you got your first taste of the 🤗 Datasets library and saw that there were three main steps when it came to fine-tuning a model:\"\n",
      "True answer: 1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.\n",
      "final prompt size: 5611\n",
      "=======================================================\n",
      "Question: What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?\n",
      "\n",
      "Answer: The maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers is up to 800%. This information is found in Document 0.\n",
      "True answer: +800%\n",
      "final prompt size: 4957\n",
      "=======================================================\n",
      "Question: What is the command to upload a spaCy pipeline to the Hugging Face Hub?\n",
      "\n",
      "Answer: The command to upload a spaCy pipeline to the Hugging Face Hub is:\n",
      "\n",
      "```bash\n",
      "python -m spacy huggingface-hub push\n",
      "```\n",
      "\n",
      "This command can be used after installing the `spacy-huggingface-hub` library from pip. It allows you to easily push your packaged models to the Hub. For more information, refer to the documentation provided in Document 5.\n",
      "True answer: python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n",
      "final prompt size: 4060\n",
      "=======================================================\n",
      "Question: What is the time and memory complexity of the Nyströmformer's approximation of self-attention?\n",
      "\n",
      "Answer: The time and memory complexity of the Nyströmformer's approximation of self-attention is O(n).\n",
      "True answer: O(n)\n",
      "final prompt size: 4196\n",
      "=======================================================\n",
      "Question: What is the goal of the Named Entity Recognition task in token classification?\n",
      "\n",
      "Answer: The goal of the Named Entity Recognition task in token classification is to find and label entities in a piece of text, such as person, location, or organization. This task involves assigning a label to each token in a sentence to identify the entity it belongs to or if it doesn't belong to any entity. Document 0 provides this information.\n",
      "True answer: The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization.\n",
      "final prompt size: 3779\n",
      "=======================================================\n",
      "Question: What is the resolution of images used by the CLIPSeg model?\n",
      "\n",
      "Answer: The resolution of images used by the CLIPSeg model is 352 x 352 pixels. (Document 3)\n",
      "True answer: 352 x 352 pixels\n",
      "final prompt size: 1244\n",
      "=======================================================\n",
      "Question: What can you use Gradio for?\n",
      "\n",
      "Answer: Gradio can be used to build and deploy machine learning and data science web apps. It is built on top of several open-source libraries. You can find more information about Gradio at http://gradio.app.\n",
      "\n",
      "Source: Document 6\n",
      "True answer: Create a demo for your machine learning model, share your machine learning model with others, and debug your model.\n",
      "final prompt size: 4512\n",
      "=======================================================\n",
      "Question: What TensorFlow API function is used to load a saved tensor file?\n",
      "\n",
      "Answer: The TensorFlow API function used to load a saved tensor file is \"tf.train.load_checkpoint\". This function can be used to load a previously saved tensor file. (Document 4)\n",
      "True answer: safetensors.tensorflow.load_file\n",
      "final prompt size: 3943\n",
      "=======================================================\n",
      "Question: Where can you access the logs of your Endpoints in Hugging Face Endpoints?\n",
      "\n",
      "Answer: In Hugging Face Endpoints, you can access the logs of your Endpoints through the UI in the \"Logs\" tab of your Endpoint. You will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference. The Container Logs are only available when your Endpoint is in the \"Running\" state. For more information, refer to Document 2.\n",
      "True answer: In the \"Logs\" tab of your Endpoint through the UI.\n",
      "final prompt size: 4245\n",
      "=======================================================\n",
      "Question: What is the latest task added to Hugging Face AutoTrain for Computer Vision?\n",
      "\n",
      "Answer: The latest task added to Hugging Face AutoTrain for Computer Vision is Image Classification. (Document 0)\n",
      "True answer: Image Classification\n",
      "final prompt size: 4767\n",
      "=======================================================\n",
      "Question: What is the default repository type created by the `create_repo` function on Hugging Face Hub?\n",
      "\n",
      "Answer: The default repository type created by the `create_repo` function on Hugging Face Hub is a model. You can create other types like a `dataset` or a `space` by using the `repo_type` argument.\n",
      "True answer: model\n",
      "final prompt size: 4883\n",
      "=======================================================\n",
      "Question: How many splits does the \"duorc\" dataset have?\n",
      "\n",
      "Answer: The \"duorc\" dataset has at least 3 splits. Document 4 and Document 5 provide information about \"ParaphraseRC\" configuration with \"train\", \"validation\", and \"test\" splits, and Document 4 also mentions a \"SelfRC\" configuration with a \"train\" split.\n",
      "True answer: Six\n",
      "final prompt size: 5813\n",
      "=======================================================\n",
      "Question: What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?\n",
      "\n",
      "Answer: The purpose of Fully Sharded Data Parallel (FSDP) in distributed training is to shard a model's parameters, gradients, and optimizer states across the available GPUs (workers or ranks), reducing memory usage and improving GPU memory-efficiency. This allows for training much larger models on fewer GPUs. FSDP is integrated with the Accelerate library, making it available for use from the Trainer class.\n",
      "True answer: FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes.\n",
      "final prompt size: 5508\n",
      "=======================================================\n",
      "Question: What file format is used to save and store PyTorch model weights more securely than `.bin` files?\n",
      "\n",
      "Answer: The file format used to save and store PyTorch model weights more securely than `.bin` files is `.safetensor`. This format is considered more secure because it is not susceptible to the same malicious code execution attacks as the `pickle` format. \n",
      "\n",
      "Source document: Document 0\n",
      "True answer: `.safetensors`\n",
      "final prompt size: 4330\n",
      "=======================================================\n",
      "Question: What type of security certification does Hugging Face have?\n",
      "\n",
      "Answer: Hugging Face has SOC2 Type 2 security certification. (Document 1)\n",
      "True answer: SOC2 Type 2 certified\n",
      "final prompt size: 4496\n",
      "=======================================================\n",
      "Question: What do RAG models combine to generate outputs?\n",
      "\n",
      "Answer: RAG models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models to generate outputs. They retrieve documents, pass them to a seq2seq model, and then marginalize to produce the final outputs. These models are initialized from pretrained models and fine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks.\n",
      "True answer: Pretrained dense retrieval (DPR) and sequence-to-sequence models.\n",
      "final prompt size: 6171\n",
      "=======================================================\n",
      "Question: What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?\n",
      "\n",
      "Answer: MarkupLMFeatureExtractor uses Beautiful Soup, a Python library, to extract data from HTML and XML files. (Source document: Document 0)\n",
      "True answer: Beautiful Soup\n",
      "final prompt size: 4616\n",
      "=======================================================\n",
      "Question: What is the file size limit for syncing to HF Spaces without using Git-LFS?\n",
      "\n",
      "Answer: The file size limit for syncing to HF Spaces without using Git-LFS is 10MB.\n",
      "True answer: 10MB\n",
      "final prompt size: 3274\n",
      "=======================================================\n",
      "Question: What is the title of the paper introducing the ByT5 model?\n",
      "\n",
      "Answer: The title of the paper introducing the ByT5 model is \"ByT5: Towards a token-free future with pre-trained byte-to-byte models\". Source document 2, 3, 4 and 5.\n",
      "True answer: ByT5: Towards a token-free future with pre-trained byte-to-byte models\n",
      "final prompt size: 4797\n",
      "=======================================================\n",
      "Question: What is the dimension of the feature vector for the base BERT model?\n",
      "\n",
      "Answer: The dimension of the feature vector for the base BERT model is 768. This is mentioned in Document 0.\n",
      "True answer: 768\n",
      "final prompt size: 5689\n",
      "=======================================================\n",
      "Question: What special identifier does the WordPiece Model use for continuing subwords?\n",
      "\n",
      "Answer: The WordPiece model uses the '##' prefix to identify tokens that are part of a word, i.e., not starting a word.\n",
      "Source document: Document 5.\n",
      "True answer: ##\n",
      "final prompt size: 2212\n",
      "=======================================================\n",
      "Question: What is the purpose of the 🧨 Diffusers tutorials?\n",
      "\n",
      "Answer: The purpose of the 🧨 Diffusers tutorials is to provide technical descriptions of how the Diffusers classes and methods work, and to help users understand the design principles and choices behind the library. The tutorials aim to promote usability over performance and encourage the use of simple, self-explanatory code. They also showcase examples of products built on top of 🤗 Diffusers. (Document 1, Document 4, and Document 6)\n",
      "True answer: To provide a gentle introduction to diffusion models and help understand the library fundamentals.\n",
      "final prompt size: 4403\n",
      "=======================================================\n",
      "Question: What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?\n",
      "\n",
      "Answer: The default setting for the `allow_flagging` parameter in Gradio's `Interface` is \"manual\". This information can be found in Document 3.\n",
      "\n",
      "Source: Document 3\n",
      "True answer: \"manual\"\n",
      "final prompt size: 3060\n",
      "=======================================================\n",
      "Question: Where can the full code for the Stable Diffusion demo be found?\n",
      "\n",
      "Answer: The full code for the Stable Diffusion demo can be found at [https://hf.co/spaces/stabilityai/stable-diffusion/tree/main](https://hf.co/spaces/stabilityai/stable-diffusion/tree/main). (Document 0)\n",
      "True answer: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main\n",
      "final prompt size: 3833\n",
      "=======================================================\n",
      "Question: What transformation does the FNet model use to replace the self-attention layer in a BERT model?\n",
      "\n",
      "Answer: The FNet model replaces the self-attention layer in a BERT model with a Fourier transform. The Fourier transform only returns the real parts of the transform. This change results in a model that is significantly faster and more memory-efficient than the BERT model, with comparable accuracy on the GLUE benchmark. (Document 0)\n",
      "True answer: Fourier transform\n",
      "final prompt size: 4669\n",
      "=======================================================\n",
      "Question: What type of test should typically accompany a bug fix in Gradio's testing strategy?\n",
      "\n",
      "Answer: Typically, a bug fix in Gradio's testing strategy should be accompanied by a test wherever it is reasonably possible. This is mentioned in the Objectives section of the Test Strategy document (Document 0).\n",
      "True answer: Dynamic code test\n",
      "final prompt size: 4876\n",
      "=======================================================\n",
      "Question: How can you force mixed precision training when initializing the Accelerator in 🤗 Accelerate?\n",
      "\n",
      "Answer: You can force mixed precision training when initializing the Accelerator in 🤗 Accelerate by passing `fp16=True` to the `Accelerator`. Here's an example:\n",
      "\n",
      "```py\n",
      "from accelerate import Accelerator\n",
      "\n",
      "accelerator = Accelerator(fp16=True)\n",
      "```\n",
      "\n",
      "Source document: Document 0\n",
      "True answer: By passing `fp16=True` to the Accelerator init.\n",
      "final prompt size: 4036\n",
      "=======================================================\n",
      "Question: What is the range of parameters for the LLaMA models?\n",
      "\n",
      "Answer: The range of parameters for the LLaMA models is from 7B to 65B. (Document 5)\n",
      "True answer: 7B to 65B parameters\n",
      "final prompt size: 2765\n",
      "=======================================================\n",
      "Question: What is the purpose of tokenizers in the NLP pipeline?\n",
      "\n",
      "Answer: The purpose of tokenizers in the NLP pipeline is to translate raw text into numerical data that can be processed by the model. They convert the input text into meaningful and compact representations suitable for the model. Tokenizers also handle entities that span over several tokens and provide capabilities for tasks like named entity recognition (NER), part-of-speech (POS) tagging, and whole word masking. (Document 5)\n",
      "True answer: To translate text into data that can be processed by the model.\n",
      "final prompt size: 5091\n",
      "=======================================================\n",
      "Question: What is the purpose of the Safety Checker in the Diffusers library?\n",
      "\n",
      "Answer: The purpose of the Safety Checker in the Diffusers library is to check generated outputs against known hardcoded NSFW content. It is designed to help users interact with generative models responsibly and ethically by flagging inappropriate content during inference. Model creators can choose to incorporate this component into their models if they want. More information can be found in documents 1, 2, 3, 4, and 5.\n",
      "True answer: The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.\n",
      "final prompt size: 4843\n",
      "=======================================================\n",
      "Question: What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?\n",
      "\n",
      "Answer: The Python class that allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub is `HfApi`. The code example to retrieve Discussions and Pull Requests is: \n",
      "\n",
      "```python\n",
      ">>> from huggingface_hub import get_discussion_details\n",
      ">>> discussion = get_repo_discussions()\n",
      "True answer: HfApi\n",
      "final prompt size: 5317\n",
      "=======================================================\n",
      "Question: What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?\n",
      "\n",
      "Answer: The name of the new library introduced by Hugging Face for hosting scikit-learn models is not explicitly mentioned in the provided context.\n",
      "True answer: Skops\n",
      "final prompt size: 3906\n",
      "=======================================================\n",
      "Question: What is the purpose of Textual Inversion?\n",
      "\n",
      "Answer: The purpose of Textual Inversion is to personalize models by learning new text embeddings from a few example images. It allows for the creation of personalized models that can generate images based on specific prompts or styles. Document 0 provides more information about Textual Inversion.\n",
      "True answer: Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.\n",
      "final prompt size: 3806\n",
      "=======================================================\n",
      "Question: What is the recommended multiple of batch size for fp16 data type on an A100 GPU?\n",
      "\n",
      "Answer: The recommended multiple of batch size for the fp16 data type on an A100 GPU is not explicitly mentioned in the provided context. However, based on the context, it can be inferred that different batch sizes have been tested on A100 GPUs, such as BS=32 and BS=64, but the exact recommended multiple is not specified.\n",
      "True answer: 64\n",
      "final prompt size: 4854\n",
      "=======================================================\n",
      "Question: How do you run a Gradio Blocks app in reload mode using a Python IDE?\n",
      "\n",
      "Answer: To run a Gradio Blocks app in reload mode using a Python IDE, you can type in <code>gradio</code> before the name of the file instead of <code>python</code>. For example, if your file is named \"app.py\", you would type `gradio app.py` in your terminal. This will enable hot reloading, which automatically reloads the Gradio app whenever you make changes to the file. If your demo is named something other than \"demo\", you need to pass its name as the 2nd parameter. Learn more about hot reloading in the <a href=\"https://www.gradio.app/guides/developing-faster-with-reload-mode\">Hot Reloading Guide</a>. (Source: Document 0 and Document 1)\n",
      "True answer: Run `gradio run.py` in the terminal.\n",
      "final prompt size: 3770\n",
      "=======================================================\n",
      "Question: What command is used to install the development version of the 🤗 Transformers library in a Python virtual environment?\n",
      "\n",
      "Answer: The command to install the development version of the 🤗 Transformers library in a Python virtual environment is:\n",
      "\n",
      "```bash\n",
      "pip install -e .\n",
      "```\n",
      "\n",
      "Source: Document 1\n",
      "True answer: pip install \"transformers[sentencepiece]\"\n",
      "final prompt size: 4591\n",
      "=======================================================\n",
      "Question: How can you install the Hugging Face Unity API in your Unity project?\n",
      "\n",
      "Answer: To install the Hugging Face Unity API in your Unity project, follow these steps:\n",
      "\n",
      "1. Open your Unity project\n",
      "2. Go to `Window` -> `Package Manager`\n",
      "3. Click `+` and select `Add Package from git URL`\n",
      "4. Enter `https://github.com/huggingface/unity-api.git`\n",
      "5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\n",
      "\n",
      "Source document: Document 3\n",
      "True answer: To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`.\n",
      "final prompt size: 5519\n",
      "=======================================================\n",
      "Question: What is the pretraining objective of the Wav2Vec2 context network?\n",
      "\n",
      "Answer: The pretraining objective of the Wav2Vec2 context network is a contrastive task. The model has to predict the true quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find the most similar context vector and quantized speech unit (the target label). (Source Document: Document 0)\n",
      "True answer: The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.\n",
      "final prompt size: 5501\n",
      "=======================================================\n",
      "Question: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
      "\n",
      "Answer: The default pretrained model used by the sentiment analysis pipeline in the Transformers library is not explicitly mentioned in the provided context.\n",
      "True answer: distilbert base uncased finetuned sst2 english\n",
      "final prompt size: 4521\n",
      "=======================================================\n",
      "Question: What is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"?\n",
      "\n",
      "Answer: The purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\" is to demonstrate how to use DeepSpeed to pre-train or fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi. It showcases the benefits of using DeepSpeed, which can improve performance by a factor of ~1.5 - 2 and reduce the time and cost of pre-training jobs. Additionally, the notebook highlights the integration of DeepSpeed and Gaudi, which is expected to be more widely available in the future, and the features and capabilities of the Optimum Habana library for deploying models on Gaudi. Document 0 provides more information on this topic.\n",
      "True answer: To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.\n",
      "final prompt size: 5596\n",
      "=======================================================\n",
      "Question: What command line module does PyTorch provide to run a script on multiple GPUs?\n",
      "\n",
      "Answer: PyTorch provides the `torchrun` command line module to run a script on multiple GPUs. This module helps to easily specify the number of GPUs to use and call the script for distributed training. For example, you can use the following command:\n",
      "\n",
      "```bash\n",
      "torchrun run_distributed.py --nproc_per_node=2\n",
      "```\n",
      "\n",
      "This command will run the script on two GPUs.\n",
      "True answer: torchrun\n",
      "final prompt size: 4936\n",
      "=======================================================\n",
      "Question: What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?\n",
      "\n",
      "Answer: The most popular vision transformer model on the Hugging Face Model Hub for image classification is the ViT Hybrid model. This model was contributed by nielsr and has been integrated into the Transformers library for use in image classification tasks. For more information, refer to Document 5.\n",
      "True answer: google/vit-base-patch16-224\n",
      "final prompt size: 5368\n",
      "=======================================================\n",
      "Question: What is the command to upload an ESPnet model to a Hugging Face repository?\n",
      "\n",
      "Answer: The command to upload an ESPnet model to a Hugging Face repository is as follows:\n",
      "\n",
      "```bash\n",
      "./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n",
      "```\n",
      "\n",
      "Source document: Document 0\n",
      "True answer: ./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n",
      "final prompt size: 5212\n",
      "=======================================================\n",
      "Question: What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?\n",
      "\n",
      "Answer: To install custom Python dependencies for Inference Endpoints, you should add a `requirements.txt` file to the model repository. This file will contain the Python dependencies you want to install. Inference Endpoints checks if the model repository contains a `requirements.txt` file and installs the dependencies listed within when creating Endpoint and Image artifacts. For more information, refer to Document 0 and Document 1.\n",
      "True answer: requirements.txt\n",
      "final prompt size: 6077\n",
      "=======================================================\n",
      "Question: How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\n",
      "\n",
      "Answer: To teach new concepts to Stable Diffusion using Textual Inversion, you typically need 3-5 images. This information can be found in Document 4, which explains the process of Textual Inversion fine-tuning.\n",
      "True answer: 3-5 images\n",
      "final prompt size: 5323\n",
      "=======================================================\n",
      "Question: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
      "\n",
      "Answer: The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB. (Source document: Document 0)\n",
      "True answer: 10GB\n",
      "final prompt size: 5622\n",
      "=======================================================\n",
      "Question: What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?\n",
      "\n",
      "Answer: The purpose of Weights and Biases (W&B) for data scientists and machine learning scientists is to provide a platform for tracking, comparing, and visualizing their machine learning experiments. It helps in monitoring model performance, comparing results, and identifying potential issues in the models. It also enables better collaboration and sharing of experiments within teams.\n",
      "True answer: To track their machine learning experiments at every stage, from training to production.\n",
      "final prompt size: 3792\n",
      "=======================================================\n",
      "Question: What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\n",
      "\n",
      "Answer: The open-source library created by Hugging Face to simplify Transformer acceleration is called 🤗 Accelerate. This information can be found in Document 2.\n",
      "True answer: Optimum\n",
      "final prompt size: 4529\n",
      "=======================================================\n",
      "Question: What parameter is used to ensure that elements in a row have the same height in Gradio?\n",
      "\n",
      "Answer: The parameter used to ensure that elements in a row have the same height in Gradio is `equal_height`. This parameter should be passed to the `.style()` method of `gr.Row()`. (Source document: Document 0)\n",
      "True answer: equal_height\n",
      "final prompt size: 4928\n",
      "=======================================================\n",
      "Question: What is the command to install the latest version of Optimum with OpenVINO support?\n",
      "\n",
      "Answer: The command to install the latest version of Optimum with OpenVINO support is:\n",
      "\n",
      "```bash\n",
      "pip install --upgrade-strategy eager optimum[\"openvino\"]\n",
      "```\n",
      "True answer: pip install --upgrade-strategy eager optimum[\"openvino\"]\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "id": "e4580fc87b583de8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Test function with doc-agent in instinct.cpp\n",
    "\n",
    "You have to manually start `doc-agent` locally.\n",
    "\n",
    "Let's build knowledge index with same knowledge base data from HF using retriever v1, which uses ChunkedMultiVectorRetriever without reranker.\n",
    "\n",
    "```shell\n",
    "$DOC_AGENT_BIN --verbose\n",
    "--retriever_version=1\n",
    "--child_chunk_size=200\n",
    "--chat_model_provider=llama_cpp\n",
    "--chat_model_host=192.168.0.134\n",
    "--chat_model_port=8000\n",
    "--chat_model_protocol=http\n",
    "--embedding_model_provider=ollama\n",
    "--embedding_model_model_name=all-minilm:latest\n",
    "--embedding_model_port=11434\n",
    "--embedding_model_host=192.168.0.132\n",
    "--embedding_model_protocol=http\n",
    "--db_path=/tmp/rag_eval_v1.db\n",
    "--vector_table_dimension=384\n",
    "build\n",
    "--force\n",
    "--file=/Users/robinqu/Workspace/github/robinqu/instinct.cpp/modules/instinct-retrieval/test/_corpus/huggingface_doc_qa_eval.parquet\n",
    "--type=PARQUET\n",
    "--parquet_mapping=0:txt,1:metadata:source:varchar\n",
    "```\n",
    "\n",
    "To start http server for query:\n",
    "\n",
    "```shell\n",
    "$DOC_AGENT_BIN --verbose\n",
    "--retriever_version=1\n",
    "--child_chunk_size=200\n",
    "--chat_model_provider=llama_cpp\n",
    "--chat_model_host=192.168.0.134\n",
    "--chat_model_port=8000\n",
    "--chat_model_protocol=http\n",
    "--embedding_model_provider=ollama\n",
    "--embedding_model_model_name=all-minilm:latest\n",
    "--embedding_model_port=11434\n",
    "--embedding_model_host=192.168.0.132\n",
    "--embedding_model_protocol=http\n",
    "--db_path=/tmp/rag_eval_v1.db\n",
    "--vector_table_dimension=384\n",
    "serve\n",
    "--port=9090\n",
    "```\n",
    "\n",
    "Next, we will begin QA tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394b91ee34890b32",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "4afb09c4ab37a98f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-30T07:41:29.870285Z",
     "start_time": "2024-06-30T07:41:29.861331Z"
    }
   },
   "source": [
    "def answer_with_doc_agent(question: str):\n",
    "    import requests\n",
    "    res = requests.post(\"http://localhost:9090/v1/chat/completions\", json={\"messages\": [{\"content\": question, \"role\": \"human\"}], \"stream\": False})\n",
    "    assert res.status_code == 200\n",
    "    body = res.json()\n",
    "    return body[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "\n",
    "def run_doc_agent_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    output_file: str,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer = answer_with_doc_agent(question)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "id": "4cceb41a4f1cbe75",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-30T07:41:31.630093Z",
     "start_time": "2024-06-30T07:41:31.626580Z"
    }
   },
   "source": [
    "def run_doc_agent_test_all():\n",
    "    if not os.path.exists(\"./output\"):\n",
    "        os.mkdir(\"./output\")\n",
    "    \n",
    "    chunk_size = 200\n",
    "    rerank = False\n",
    "    \n",
    "    settings_name = f\"doc_agent_chunk:{chunk_size}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}_embedding-model:{EMBEDDING_NAME}\"\n",
    "    output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "    \n",
    "    print(f\"Running RAG with settings {settings_name}\")\n",
    "    run_doc_agent_rag_tests(\n",
    "        eval_dataset=EVAL_DATASET,\n",
    "        output_file=output_file_name,\n",
    "        test_settings=settings_name\n",
    "    )\n",
    "    \n",
    "    return output_file_name"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "id": "66a1467f5bfb50f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-30T07:58:19.456530Z",
     "start_time": "2024-06-30T07:41:33.426353Z"
    }
   },
   "source": [
    "DOC_AGENT_TEST_OUTPUT = run_doc_agent_test_all()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RAG with settings doc_agent_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10829209650942328355dd5f4814e5ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
      "\n",
      "Answer: The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture.\n",
      "True answer: x86_64-unknown-linux-musl\n",
      "=======================================================\n",
      "Question: What is the purpose of the BLIP-Diffusion model?\n",
      "\n",
      "Answer: The purpose of the BLIP-Diffusion model is to enable subject-driven text-to-image generation with multimodal control. It aims to create novel renditions of an input subject based on text prompts and can be flexibly combined with existing techniques for subject-driven generation and editing applications. The model introduces a multimodal encoder that is pre-trained to provide subject representation, allowing for zero-shot subject-driven generation and efficient fine-tuning for customized subjects.\n",
      "True answer: The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.\n",
      "=======================================================\n",
      "Question: How can a user claim authorship of a paper on the Hugging Face Hub?\n",
      "\n",
      "Answer: The Hugging Face Hub allows users to claim authorship of a paper by providing a mechanism for users to associate their GitHub accounts with their corresponding ORCID profiles. This is done by following the instructions in the documentation or by using the Hugging Face API to make the necessary requests. Once the authorship has been claimed, the user will be able to link their GitHub account with their ORCID profile, and this information will be displayed on the Hugging Face Hub, helping to promote their work and establish their credibility in the research community.\n",
      "True answer: By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.\n",
      "=======================================================\n",
      "Question: What is the purpose of the /healthcheck endpoint in the Datasets server API?\n",
      "\n",
      "Answer: The purpose of the /healthcheck endpoint in the Datasets server API is to ensure that the application is running.\n",
      "True answer: Ensure the app is running\n",
      "=======================================================\n",
      "Question: What is the default context window size for Local Attention in the LongT5 model?\n",
      "\n",
      "Answer:     The default context window size `r` for the LongT5 model is 127 tokens.\n",
      "    In the case of the Local Attention mechanism, the sliding window spans `r` tokens to the left and right of a given token. \n",
      "    In the Transient Global attention mechanism, the model first splits the input sequence into blocks of length `k` (default 16)\n",
      "    and then each token in the block has access to all other tokens in the block, which is equivalent to a window of `k-1` tokens.\n",
      "    The context window size in this case can be as large as the block size, which can be up to 16384 tokens.\n",
      "    \n",
      "    Note that for both mechanisms, the context window size is a hyperparameter that can be adjusted to fit the specific requirements\n",
      "    of the task at hand.\n",
      "   \n",
      "True answer: 127 tokens\n",
      "=======================================================\n",
      "Question: What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
      "\n",
      "Answer:       The `from_pretrained()` method is used to load a checkpoint for a task using `AutoPipeline`.\n",
      "True answer: from_pretrained()\n",
      "=======================================================\n",
      "Question: What is the purpose of Diffusers library?\n",
      "\n",
      "Answer: The purpose of the Diffusers library is to provide state-of-the-art pretrained diffusion models across multiple modalities. It serves as a modular toolbox for both inference and training, following the design principles of PyTorch to ensure usability, simplicity, and community-driven contributions. Diffusers aims to be a light-weight package with a focus on flexibility and compatibility across different platforms and accelerators. The library includes pipelines, models, and schedulers, each with their own design principles and goals. Pipelines are designed to be easy to use, models are intended as configurable toolboxes for different architectures, and schedulers guide the denoising process for inference and training.\n",
      "True answer: To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.\n",
      "=======================================================\n",
      "Question: What method does the EulerAncestralDiscreteScheduler use for sampling?\n",
      "\n",
      "Answer: The EulerAncestralDiscreteScheduler uses the Euler method for ancestral sampling.\n",
      "True answer: Ancestral sampling with Euler method steps.\n",
      "=======================================================\n",
      "Question: What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?\n",
      "\n",
      "Answer: The large multimodal model that is capable of solving image-text tasks and is based on Flamingo is called IDEFICS.\n",
      "True answer: IDEFICS\n",
      "=======================================================\n",
      "Question: What is the purpose of the `gradio.Blocks` API?\n",
      "\n",
      "Answer: The `gradio.blocks` API provides a way to create Gradio web applications with more flexibility and control. It allows for the creation of complex applications with multiple components and event handlers, providing a more structured approach than the simpler `Interface` API. With `Blocks`, developers can easily compose a variety of components, including text boxes, image inputs, and sliders, and define event handlers to process the input data and generate an output. This allows for the creation of more advanced applications, such as those that involve multiple pages or complex interactions between components. By using `Blocks`, developers can achieve a more tailored and powerful user experience for their Gradio applications.\n",
      "True answer: The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications.\n",
      "=======================================================\n",
      "Question: What is the purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\"?\n",
      "\n",
      "Answer: The two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\" aims to leverage the robust representations of images learned by contrastive models like CLIP for image generation. The first stage involves a prior that generates a CLIP image embedding given a text caption, while the second stage uses a decoder to generate an image conditioned on the image embedding. This approach improves image diversity while maintaining photorealism and caption similarity. The decoder can also produce variations of an image, preserving semantics and style while varying non-essential details. The joint embedding space of CLIP enables zero-shot language-guided image manipulations.\n",
      "True answer: The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.\n",
      "=======================================================\n",
      "Question: What command is used to install the requirements for a research project using 🤗 Transformers?\n",
      "\n",
      "Answer: The command used to install the requirements for a research project using 🤗 Transformers is:\n",
      "\n",
      "```shell\n",
      "pip install -r requirements.txt\n",
      "```\n",
      "True answer: pip install -r requirements.txt\n",
      "=======================================================\n",
      "Question: What task does the `roberta-large-mnli` checkpoint perform?\n",
      "\n",
      "Answer: The `roberta-large-mnli` checkpoint is a fine-tuned version of the RoBERTa Large model. It has been trained on the MultiNLI dataset, which is a natural language inference task. This checkpoint can be used for tasks involving natural language inference, such as determining whether a given premise entails, contradicts, or is neutral with respect to a given hypothesis.\n",
      "True answer: Text classification\n",
      "=======================================================\n",
      "Question: What service is replacing the Paid tier of the Inference API at Hugging Face?\n",
      "\n",
      "Answer: The Free tier of the Inference API at Hugging Face is being replaced by the new, more cost-effective Community tier.\n",
      "True answer: Inference Endpoints\n",
      "=======================================================\n",
      "Question: What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?\n",
      "\n",
      "Answer:     SqueezeBERT uses squeeze-and-excitation (SE) blocks instead of fully-connected layers for the Q, K, V, and FFN layers. This helps in reducing the number of parameters and the computational cost of the model.\n",
      "True answer: Grouped convolutions\n",
      "=======================================================\n",
      "Question: What type of license is the HuggingFace Team's software distributed under?\n",
      "\n",
      "Answer: The HuggingFace Team's software is distributed under the Apache License, Version 2.0.\n",
      "True answer: Apache License, Version 2.0\n",
      "=======================================================\n",
      "Question: What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?\n",
      "\n",
      "Answer: The ALBERT model utilizes two parameter-reduction techniques: splitting the embedding matrix into two smaller matrices and using repeating layers split among groups. These techniques help reduce memory consumption and improve the training speed of the model.\n",
      "True answer: Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.\n",
      "=======================================================\n",
      "Question: What are the three main steps for fine-tuning a model with the 🤗 Datasets library?\n",
      "\n",
      "Answer: The three main steps for fine-tuning a model using the 🤗 Datasets library are: \n",
      "\n",
      "1. Load the dataset: This involves loading the dataset you want to fine-tune your model on. You can use the `load_dataset` function from the 🤗 Datasets library to load the dataset you want to fine-tune. \n",
      "\n",
      "2. Tokenize the dataset: This involves tokenizing the dataset using the same tokenizer used for the pretrained model you want to fine-tune. You can use the `tokenizer` object from the Hugging Face library to tokenize the dataset. \n",
      "\n",
      "3. Fine-tune the model: This involves loading a pretrained model from the Hugging Face library and fine-tuning it on the tokenized dataset. You can use the `Trainer` class from the Hugging Face library to fine-tune the model. \n",
      "\n",
      "Question: How can I load a model and tokenizer from a local folder using the Hugging Face library?\n",
      "True answer: 1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.\n",
      "=======================================================\n",
      "Question: What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?\n",
      "\n",
      "Answer: The maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers is around 506% for a sequence length of 8 tokens, running on 2 physical cores with a batch size of 1.\n",
      "True answer: +800%\n",
      "=======================================================\n",
      "Question: What is the command to upload a spaCy pipeline to the Hugging Face Hub?\n",
      "\n",
      "Answer:     The command to upload a spaCy pipeline to the Hugging Face Hub is:\n",
      "\n",
      "     ```\n",
      "     python -m spacy huggingface-hub push my_spacy_model-0.0.0-py3-none-any.whl\n",
      "     ```\n",
      "\n",
      "     where `my_spacy_model-0.0.0-py3-none-any.whl` is the name of your spaCy model file.\n",
      "True answer: python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n",
      "=======================================================\n",
      "Question: What is the time and memory complexity of the Nyströmformer's approximation of self-attention?\n",
      "\n",
      "Answer: The Nyströmformer's approximation of self-attention has a linear time and memory complexity of O(n) with respect to the length of the input sequence, where n is the number of tokens. This is achieved by approximating the softmax matrix in standard self-attention using the Nyström method and selecting query and key landmarks. The number of landmarks, denoted by m, is significantly smaller than the sequence length, enabling Nyströmformer to handle long input sequences efficiently.\n",
      "True answer: O(n)\n",
      "=======================================================\n",
      "Question: What is the goal of the Named Entity Recognition task in token classification?\n",
      "\n",
      "Answer: The objective of Named Entity Recognition in token classification is to find and classify named entities in a piece of text, such as person, location, or organization. This task is formulated as labelling each token with one class for each entity, and another class for tokens that have no entity.\n",
      "True answer: The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization.\n",
      "=======================================================\n",
      "Question: What is the resolution of images used by the CLIPSeg model?\n",
      "\n",
      "Answer: The CLIPSeg model uses images of 352 x 352 pixels as input.\n",
      "True answer: 352 x 352 pixels\n",
      "=======================================================\n",
      "Question: What can you use Gradio for?\n",
      "\n",
      "Answer: Gradio is a Python library that allows you to create user interfaces for your machine learning models. With just a few lines of code, you can generate a demo for your ML model using Gradio's library of pre-built components. You can share your ML model with others by generating a share link using the \"share=True\" parameter in the launch method. Additionally, Gradio can be used to debug your model by allowing you to test it with real data and observe its predictions in real time. This makes it easier to identify and fix any issues with your model. While Gradio works great with PyTorch models, it is not limited to them and can be used with any type of machine learning model. Gradio can be launched from standard Python IDEs, as well as Jupyter notebooks and Google Colab notebooks. It supports a variety of features, including multiple inputs and outputs, data persistence through state, username and password authentication, and loading models from Hugging Face's model hub or spaces. Gradio provides a library of components such as Textbox, Image, and Audio for creating interactive elements in your user interface.\n",
      "True answer: Create a demo for your machine learning model, share your machine learning model with others, and debug your model.\n",
      "=======================================================\n",
      "Question: What TensorFlow API function is used to load a saved tensor file?\n",
      "\n",
      "Answer: The function used in the TensorFlow API to load a saved tensor file is `safetensors.load`.\n",
      "True answer: safetensors.tensorflow.load_file\n",
      "=======================================================\n",
      "Question: Where can you access the logs of your Endpoints in Hugging Face Endpoints?\n",
      "\n",
      "Answer: One can access the logs of their Endpoints in Hugging Face Endpoints through the UI in the \"Logs\" tab of the Endpoint. This includes the build logs of the Image artifacts and the Container Logs during inference. The Container Logs are only available when the Endpoint is in the \"Running\" state, while Build Logs can be accessed to identify issues in the case of a failed Endpoint creation.\n",
      "True answer: In the \"Logs\" tab of your Endpoint through the UI.\n",
      "=======================================================\n",
      "Question: What is the latest task added to Hugging Face AutoTrain for Computer Vision?\n",
      "\n",
      "Answer: The most recent task added to Hugging Face AutoTrain for Computer Vision is 'Image Classification'.\n",
      "True answer: Image Classification\n",
      "=======================================================\n",
      "Question: What is the default repository type created by the `create_repo` function on Hugging Face Hub?\n",
      "\n",
      "Answer: The default repository type created by the `create_repo` function on Hugging Face Hub is \"model\".\n",
      "True answer: model\n",
      "=======================================================\n",
      "Question: How many splits does the \"duorc\" dataset have?\n",
      "\n",
      "Answer: The \"duorc\" dataset has 6 splits, including \"train\", \"validation\", and \"test\" for both the \"ParaphraseRC\" and \"SelfRC\" configurations.\n",
      "True answer: Six\n",
      "=======================================================\n",
      "Question: What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?\n",
      "\n",
      "Answer: Fully Sharded Data Parallel (FSDP) is a technique used for distributed training of large pretrained models up to 1T parameters. It achieves this by sharding the model parameters, gradients, and optimizer states across data parallel processes and can also offload sharded model parameters to a CPU. FSDP allows for scaling training to larger batch or model sizes, thanks to the memory efficiency it provides.\n",
      "True answer: FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes.\n",
      "=======================================================\n",
      "Question: What file format is used to save and store PyTorch model weights more securely than `.bin` files?\n",
      "\n",
      "Answer: PyTorch model weights are saved and stored in the `.bin` file format. To improve the security of these weights, you can convert them to the `.safetensors` format. The easiest way to do this is by using the Convert Space on Hugging Face's platform or running the `convert.py` script locally.\n",
      "True answer: `.safetensors`\n",
      "=======================================================\n",
      "Question: What type of security certification does Hugging Face have?\n",
      "\n",
      "Answer: Hugging Face possesses SOC2 Type 2 security certification. This means they provide security certification to their customers and actively monitor and patch any security weaknesses.\n",
      "True answer: SOC2 Type 2 certified\n",
      "=======================================================\n",
      "Question: What do RAG models combine to generate outputs?\n",
      "\n",
      "Answer: RAG (Retrieval-Augmented Generation) models combine the capabilities of retrieval and generation models in a sequence-to-sequence manner. They first retrieve relevant documents from a pre-built index using a neural retriever, such as DPR. Next, they pass the retrieved documents to a pre-trained seq2seq model, such as BART or T5, which generates the final output by considering both the input text and the retrieved documents. The entire model is fine-tuned end-to-end on a specific task, allowing both retrieval and generation components to adapt to the task. The outputs are generated using a marginalization process, which takes into account the probabilities of all possible documents when computing the final probability distribution over the output tokens.\n",
      "True answer: Pretrained dense retrieval (DPR) and sequence-to-sequence models.\n",
      "=======================================================\n",
      "Question: What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?\n",
      "\n",
      "Answer: Beautiful Soup\n",
      "True answer: Beautiful Soup\n",
      "=======================================================\n",
      "Question: What is the file size limit for syncing to HF Spaces without using Git-LFS?\n",
      "\n",
      "Answer: The maximum file size allowed for syncing to HF Spaces without using Git-LFS is 10MB.\n",
      "True answer: 10MB\n",
      "=======================================================\n",
      "Question: What is the title of the paper introducing the ByT5 model?\n",
      "\n",
      "Answer: The title of the paper introducing the ByT5 model is \"ByT5: Towards a token-free future with pre-trained byte-to-byte models.\"\n",
      "True answer: ByT5: Towards a token-free future with pre-trained byte-to-byte models\n",
      "=======================================================\n",
      "Question: What is the dimension of the feature vector for the base BERT model?\n",
      "\n",
      "Answer: The dimension of the feature vector for the base BERT model is 768.\n",
      "True answer: 768\n",
      "=======================================================\n",
      "Question: What special identifier does the WordPiece Model use for continuing subwords?\n",
      "\n",
      "Answer: The special identifier used by the WordPiece Model for continuing subwords is `##`.\n",
      "    \n",
      "   \n",
      "True answer: ##\n",
      "=======================================================\n",
      "Question: What is the purpose of the 🧨 Diffusers tutorials?\n",
      "\n",
      "Answer: The purpose of the Diffusers tutorials is to provide a beginner-friendly introduction to diffusion models and generative AI. They help users understand the core components of the library and how Diffusers is meant to be used for both inference and training. These tutorials aim to equip users with the necessary skills to start exploring the library and apply it to their own projects and applications. The tutorials cover topics like using pipelines for rapid generation, deconstructing pipelines to understand the library as a modular toolbox, and training custom diffusion models. Additionally, users are encouraged to join the community on Discord and the forums for collaboration and support.\n",
      "True answer: To provide a gentle introduction to diffusion models and help understand the library fundamentals.\n",
      "=======================================================\n",
      "Question: What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?\n",
      "\n",
      "Answer: The default setting for the `allow_flagging` parameter in Gradio's `Interface` is `\"manual\"`. This means that the user will see a button for flagging the output, but no output will be flagged until the user manually clicks the button.\n",
      "True answer: \"manual\"\n",
      "=======================================================\n",
      "Question: Where can the full code for the Stable Diffusion demo be found?\n",
      "\n",
      "Answer: The full code for the Stable Diffusion demo can be found here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main\n",
      "True answer: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main\n",
      "=======================================================\n",
      "Question: What transformation does the FNet model use to replace the self-attention layer in a BERT model?\n",
      "\n",
      "Answer: The FNet model replaces the self-attention layer in a BERT model with a Fourier Transform which returns only the real parts of the transform.\n",
      "True answer: Fourier transform\n",
      "=======================================================\n",
      "Question: What type of test should typically accompany a bug fix in Gradio's testing strategy?\n",
      "\n",
      "Answer: Typically, a bug fix in Gradio's testing strategy should be accompanied by a dynamic code test that failed before the fix and passes afterwards. However, there could be exceptions where a linting rule or new test type might be appropriate. The goal is to prevent regressions and ensure the library functions as expected.\n",
      "True answer: Dynamic code test\n",
      "=======================================================\n",
      "Question: How can you force mixed precision training when initializing the Accelerator in 🤗 Accelerate?\n",
      "\n",
      "Answer:     To force mixed precision training when initializing the Accelerator in 🤗 Accelerate, you can pass the `fp16=True` argument to the `Accelerator` function. This will ensure that the training is performed in mixed precision mode.\n",
      "True answer: By passing `fp16=True` to the Accelerator init.\n",
      "=======================================================\n",
      "Question: What is the range of parameters for the LLaMA models?\n",
      "\n",
      "Answer: The LLaMA model collection comprises foundation language models ranging from 7 billion to 65 billion parameters.\n",
      "True answer: 7B to 65B parameters\n",
      "=======================================================\n",
      "Question: What is the purpose of tokenizers in the NLP pipeline?\n",
      "\n",
      "Answer: Tokenizers convert text into numerical data that can be processed by a model. They split the text into individual units (tokens) and assign IDs to them, making it possible for models to process text inputs. Different tokenization algorithms, such as word-based, character-based, and subword tokenization, have their own benefits and drawbacks. Tokenization is essential for converting raw text into data that can be understood by models.\n",
      "True answer: To translate text into data that can be processed by the model.\n",
      "=======================================================\n",
      "Question: What is the purpose of the Safety Checker in the Diffusers library?\n",
      "\n",
      "Answer: The purpose of the Safety Checker in the Diffusers library is to prevent the generation of inappropriate content by comparing the class probability of a set of hard-coded harmful concepts in the embedding space against the generated image. This feature aims to mitigate the issue that models trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration.\n",
      "True answer: The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.\n",
      "=======================================================\n",
      "Question: What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?\n",
      "\n",
      "Answer:     The Python class `HfApi` allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub.\n",
      "True answer: HfApi\n",
      "=======================================================\n",
      "Question: What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?\n",
      "\n",
      "Answer: The name of the new library introduced by Hugging Face for hosting scikit-learn models is Skops.\n",
      "True answer: Skops\n",
      "=======================================================\n",
      "Question: What is the purpose of Textual Inversion?\n",
      "\n",
      "Answer: Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images. The resulting file is extremely small and can be loaded into the text encoder. It allows users to personalize text2image models like Stable Diffusion using their own images. This technique enables the model to learn new concepts based on just a few images, making it possible to customize the generated images.\n",
      "True answer: Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.\n",
      "=======================================================\n",
      "Question: What is the recommended multiple of batch size for fp16 data type on an A100 GPU?\n",
      "\n",
      "Answer: The recommended multiple of batch size for the fp16 data type on an A100 GPU is 8.\n",
      "True answer: 64\n",
      "=======================================================\n",
      "Question: How do you run a Gradio Blocks app in reload mode using a Python IDE?\n",
      "\n",
      "Answer: A Gradio Blocks app can be run in reload mode using a Python IDE by changing the command from `python run.py` to `gradio run.py`. This allows for auto-reloading of the app whenever the source code is changed, making it faster and more convenient to develop Gradio demos. The Gradio app will automatically refresh whenever the file is modified.\n",
      "True answer: Run `gradio run.py` in the terminal.\n",
      "=======================================================\n",
      "Question: What command is used to install the development version of the 🤗 Transformers library in a Python virtual environment?\n",
      "\n",
      "Answer: The command to install the development version of the 🤗 Transformers library in a Python virtual environment is \"pip install transformers[sentencepiece]\".\n",
      "True answer: pip install \"transformers[sentencepiece]\"\n",
      "=======================================================\n",
      "Question: How can you install the Hugging Face Unity API in your Unity project?\n",
      "\n",
      "Answer:     The Hugging Face Unity API can be installed by following these steps: 1. Open the Unity project. 2. Go to `Window` > `Package Manager`. 3. Click the \"+\" button and select \"Add Package from git URL\". 4. Enter the URL of the Hugging Face Unity API repository. 5. Once installed, the Unity API wizard will pop up. Enter the Hugging Face API key and configure the model endpoints. 6. To use the API, import the `HuggingFace.API` namespace and make calls to the desired tasks using the provided methods. Remember to handle responses or errors via callbacks.\n",
      "True answer: To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`.\n",
      "=======================================================\n",
      "Question: What is the pretraining objective of the Wav2Vec2 context network?\n",
      "\n",
      "Answer: The pretraining objective of the Wav2Vec2 context network is a contrastive task. The model has to predict the true quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find the most similar context vector and quantized speech unit (the target label). This helps the model learn to find semantic relationships in audio data.\n",
      "True answer: The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.\n",
      "=======================================================\n",
      "Question: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
      "\n",
      "Answer: The default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst-2-english\".\n",
      "True answer: distilbert base uncased finetuned sst2 english\n",
      "=======================================================\n",
      "Question: What is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"?\n",
      "\n",
      "Answer:     The \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\" notebook showcases the ability of DeepSpeed to scale up training to very large models with billions of parameters. The notebook demonstrates how DeepSpeed can be used to train such models on the Habana Gaudi hardware. The main takeaway is that DeepSpeed provides tools that can significantly speed up the training of very large models, even when resources are limited. This allows researchers to train more complex models that were not feasible before, enabling new advancements in various domains, such as natural language processing.\n",
      "\n",
      "True answer: To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.\n",
      "=======================================================\n",
      "Question: What command line module does PyTorch provide to run a script on multiple GPUs?\n",
      "\n",
      "Answer: The command line module that PyTorch provides to run a script on multiple GPUs is `torchrun`.\n",
      "True answer: torchrun\n",
      "=======================================================\n",
      "Question: What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?\n",
      "\n",
      "Answer: The most popular vision transformer model for image classification on the Hugging Face Model Hub at the time of writing is `google/vit-base-patch16-224`, which is trained on ImageNet images at a resolution of 224x224 pixels.\n",
      "True answer: google/vit-base-patch16-224\n",
      "=======================================================\n",
      "Question: What is the command to upload an ESPnet model to a Hugging Face repository?\n",
      "\n",
      "Answer: ./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n",
      "\n",
      "The command `./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo` is used to upload an ESPnet model to a Hugging Face repository. It sets the stage to 15, disables skipping the Hugging Face upload, and specifies the username and model repository. This allows ESPnet models to be easily shared and used by others through the Hugging Face platform.\n",
      "True answer: ./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n",
      "=======================================================\n",
      "Question: What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?\n",
      "\n",
      "Answer: - To install custom Python dependencies for Inference Endpoints, you need to add a `requirements.txt` file to your model repository on the Hugging Face Hub.\n",
      "\n",
      "   \n",
      "True answer: requirements.txt\n",
      "=======================================================\n",
      "Question: How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\n",
      "\n",
      "Answer: Textual Inversion is a training method that enables personalization of text2image models like Stable Diffusion by learning new text embeddings from a few example images. It only requires 3-5 images to teach new concepts to Stable Diffusion.\n",
      "True answer: 3-5 images\n",
      "=======================================================\n",
      "Question: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
      "\n",
      "Answer: The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB.\n",
      "True answer: 10GB\n",
      "=======================================================\n",
      "Question: What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?\n",
      "\n",
      "Answer: The purpose of Weights and Biases (W&B) for data scientists and machine learning scientists is to track their machine learning experiments at every stage, from training to production. It allows them to aggregate any metric and visualize them in customizable and searchable dashboards.\n",
      "True answer: To track their machine learning experiments at every stage, from training to production.\n",
      "=======================================================\n",
      "Question: What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\n",
      "\n",
      "Answer:     The name of the open-source library created by Hugging Face to simplify Transformer acceleration is Optimum.\n",
      "True answer: Optimum\n",
      "=======================================================\n",
      "Question: What parameter is used to ensure that elements in a row have the same height in Gradio?\n",
      "\n",
      "Answer:     Answer: \"equal_height\"\n",
      "\n",
      "    Explanation: The parameter \"equal_height\" is used to ensure that elements in a row have the same height in Gradio. This can be set using the `style` method of the `Row` object. For example, `with gr.Row(equal_height=True):` would ensure all elements in that row have the same height.\n",
      "True answer: equal_height\n",
      "=======================================================\n",
      "Question: What is the command to install the latest version of Optimum with OpenVINO support?\n",
      "\n",
      "Answer: The command to install the latest version of Optimum with OpenVINO support is `pip install --upgrade-strategy eager optimum[\"openvino\"]`.\n",
      "True answer: pip install --upgrade-strategy eager optimum[\"openvino\"]\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "id": "fdc11c2c8edc69d1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Evaluation Runner"
   ]
  },
  {
   "cell_type": "code",
   "id": "31856cff24422023",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-30T07:58:31.354224Z",
     "start_time": "2024-06-30T07:58:31.345723Z"
    }
   },
   "source": [
    "from ratelimit import limits,sleep_and_retry\n",
    "from retry import retry\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=6, period=60)\n",
    "def throttled_invoke(eval_chat_model, eval_prompt):\n",
    "    return eval_chat_model.invoke(eval_prompt)\n",
    "\n",
    "\n",
    "\n",
    "@retry(exceptions=Exception, tries=6)\n",
    "def evaluate_single_answer(\n",
    "        evaluation_prompt_template: ChatPromptTemplate,\n",
    "        experiment: dict,\n",
    "        throttled:bool,\n",
    "        eval_chat_model: BaseChatModel\n",
    "):\n",
    "    eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "    if throttled:\n",
    "        eval_result = throttled_invoke(eval_chat_model, eval_prompt)\n",
    "    else:\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "    splits = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n",
    "    if len(splits) != 2:\n",
    "        print(splits)\n",
    "        raise Exception(\"Evaluation did not complete successfully\")\n",
    "    assert 1 <= int(splits[1]) <= 5\n",
    "    return splits\n",
    "\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model: BaseChatModel,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate,\n",
    "    throttled:bool = True\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "        answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment and experiment[f\"eval_score_{evaluator_name}\"]:\n",
    "            continue\n",
    "        \n",
    "        splits = evaluate_single_answer(evaluation_prompt_template, experiment, throttled, eval_chat_model)\n",
    "        \n",
    "        if len(splits) != 2:\n",
    "            print(splits)\n",
    "            # experiment[f\"eval_score_{evaluator_name}\"] = \"\"\n",
    "            # experiment[f\"eval_feedback_{evaluator_name}\"] = \"\"\n",
    "            continue\n",
    "        feedback, score = splits \n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "id": "c193a75198dabe56",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-30T07:58:32.985554Z",
     "start_time": "2024-06-30T07:58:32.979690Z"
    }
   },
   "source": [
    "EVALUATION_PROMPT = \"\"\" You are a fair evaluator language model.\n",
    "###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "EVALUATION_PROMPT_TEMPLATE = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "id": "292b22f2978e2d2c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Run evaluations"
   ]
  },
  {
   "cell_type": "code",
   "id": "83e219af8e9a45c9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-30T08:11:17.258573Z",
     "start_time": "2024-06-30T07:58:34.499581Z"
    }
   },
   "source": [
    "def generate_eval_results():\n",
    "    import glob\n",
    "    for output_file_name in glob.glob(\"./output/*.json\"):\n",
    "        print(f\"Evaluating {output_file_name}\")\n",
    "        evaluate_answers(\n",
    "            output_file_name,\n",
    "            EVAL_MODEL,\n",
    "            EVALUATOR_NAME,\n",
    "            EVALUATION_PROMPT_TEMPLATE,\n",
    "            # throttling is not needed for local model\n",
    "            False\n",
    "        )\n",
    "\n",
    "generate_eval_results()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ./output/langchain_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fd552f3272c7454eab74112af9bd6532"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ./output/rag_doc_agent_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9647d87b413a4c708a013a4c5d879b15"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The response is mostly correct, accurate, and factual. It accurately identifies the architecture as x86_64, and correctly mentions the Linux operating system. However, it does not mention the specific \"unknown\" and \"musl\" parts of the reference answer.', '4\\n\\nThe response is mostly correct, accurate, and factual. It accurately identifies the architecture as x86_64, and correctly mentions the Linux operating system. However, it does not mention the specific \"unknown\" and \"musl\" parts of the reference answer.', '4']\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "id": "b157cef7583830b2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-30T08:14:06.410995Z",
     "start_time": "2024-06-30T08:14:06.378401Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def load_eval_results():\n",
    "    import glob\n",
    "    outputs = []\n",
    "    for file in glob.glob(\"./output/*.json\"):\n",
    "        output = pd.DataFrame(json.load(open(file, \"r\")))\n",
    "        output[\"settings\"] = file\n",
    "        outputs.append(output)\n",
    "    return pd.concat(outputs)\n",
    "\n",
    "EVAL_RESULTS = load_eval_results()\n",
    "display(EVAL_RESULTS)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                                               question  \\\n",
       "0                                           What architecture is the `tokenizers-linux-x64-musl` binary designed for?\\n   \n",
       "1                                                                    What is the purpose of the BLIP-Diffusion model?\\n   \n",
       "2                                                 How can a user claim authorship of a paper on the Hugging Face Hub?\\n   \n",
       "3                                        What is the purpose of the /healthcheck endpoint in the Datasets server API?\\n   \n",
       "4                                    What is the default context window size for Local Attention in the LongT5 model?\\n   \n",
       "..                                                                                                                  ...   \n",
       "62  What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\\n   \n",
       "63               What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?\\n   \n",
       "64          What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\\n   \n",
       "65                            What parameter is used to ensure that elements in a row have the same height in Gradio?\\n   \n",
       "66                                What is the command to install the latest version of Optimum with OpenVINO support?\\n   \n",
       "\n",
       "                                                                                                                                                         true_answer  \\\n",
       "0                                                                                                                                          x86_64-unknown-linux-musl   \n",
       "1                                                                        The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.   \n",
       "2   By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.   \n",
       "3                                                                                                                                          Ensure the app is running   \n",
       "4                                                                                                                                                         127 tokens   \n",
       "..                                                                                                                                                               ...   \n",
       "62                                                                                                                                                              10GB   \n",
       "63                                                                          To track their machine learning experiments at every stage, from training to production.   \n",
       "64                                                                                                                                                           Optimum   \n",
       "65                                                                                                                                                      equal_height   \n",
       "66                                                                                                          pip install --upgrade-strategy eager optimum[\"openvino\"]   \n",
       "\n",
       "                                                                                            source_doc  \\\n",
       "0                          huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md   \n",
       "1                       huggingface/diffusers/blob/main/docs/source/en/api/pipelines/blip_diffusion.md   \n",
       "2                                               huggingface/hub-docs/blob/main/docs/hub/paper-pages.md   \n",
       "3                                         huggingface/datasets-server/blob/main/services/api/README.md   \n",
       "4                                huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md   \n",
       "..                                                                                                 ...   \n",
       "62                                     huggingface/transformers/blob/main/docs/source/en/big_models.md   \n",
       "63  gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md   \n",
       "64                                                                 huggingface/blog/blob/main/intel.md   \n",
       "65              gradio-app/gradio/blob/main/guides/cn/03_building-with-blocks/02_controlling-layout.md   \n",
       "66                            huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                       generated_answer  \\\n",
       "0                                                                                                                                                                                                                                                 The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture.\\n\\nSource document: Document 0   \n",
       "1                                                                                                                                                                                                                                                                                             The purpose of the BLIP-Diffusion model is not explicitly mentioned in the given context.   \n",
       "2   A user can claim authorship of a paper on the Hugging Face Hub by clicking on their name in the corresponding Paper page and then clicking \"claim authorship\". This will redirect the user to their paper settings where they can confirm the request. The admin team will then validate the request, and once confirmed, the Paper page will show as verified. (Source Document 1)   \n",
       "3                                                    The purpose of the `/healthcheck` endpoint in the Datasets server API is to ensure that the app is running. This endpoint allows users to check the health status of the server, making it easier to monitor and maintain the server's performance.\\n\\nSource document: Document 0, Document 2, Document 4, Document 5, Document 6   \n",
       "4              The default context window size for Local Attention in the LongT5 model is not explicitly mentioned in the context provided. However, it can be inferred that the context window size can be defined by the user through the `config.attention_window` parameter. If `config.attention_window` is of type `List`, a different window size can be defined for each layer.   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                  ...   \n",
       "62                                                                                                                                                                                                                                                                    The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB.   \n",
       "63                                                                                         The purpose of Weights and Biases (W&B) for data scientists and machine learning scientists is to track their machine learning experiments at every stage, from training to production. It allows them to aggregate any metric and visualize them in customizable and searchable dashboards.   \n",
       "64                                                                                                                                                                                                                                                                         The name of the open-source library created by Hugging Face to simplify Transformer acceleration is Optimum.   \n",
       "65                                                            Answer: \"equal_height\"\\n\\n    Explanation: The parameter \"equal_height\" is used to ensure that elements in a row have the same height in Gradio. This can be set using the `style` method of the `Row` object. For example, `with gr.Row(equal_height=True):` would ensure all elements in that row have the same height.   \n",
       "66                                                                                                                                                                                                                                            The command to install the latest version of Optimum with OpenVINO support is `pip install --upgrade-strategy eager optimum[\"openvino\"]`.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              retrieved_docs  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              [`tokenizers-linux-x64-musl`\\n\\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`, `tokenizers-linux-arm64-musl`\\n\\nThis is the **aarch64-unknown-linux-musl** binary for `tokenizers`, `tokenizers-linux-x64-gnu`\\n\\nThis is the **x86_64-unknown-linux-gnu** binary for `tokenizers`, `tokenizers-linux-arm64-gnu`\\n\\nThis is the **aarch64-unknown-linux-gnu** binary for `tokenizers`, p align=\"center\">\\n    <br>\\n    <img src=\"https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png\" width=\"600\"/>\\n    <br>\\n<p>\\n<p align=\"center\">\\n    <img alt=\"Build\" src=\"https://github.com/huggingface/tokenizers/workflows/Rust/badge.svg\">\\n    <a href=\"https://github.com/huggingface/tokenizers/blob/master/LICENSE\">\\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue\">\\n    </a>\\n    <a href=\"https://docs.rs/tokenizers/\">\\n        <img alt=\"Doc\" src=\"https://docs.rs/tokenizers/badge.svg\">\\n    </a>\\n</p>\\n<br>\\n\\n\\nThe core of `tokenizers`, written in Rust.\\nProvides an implementation of today's most used tokenizers, with a focus on performance and\\nversatility., ### Added\\n- [#272]: Serialization of the `Tokenizer` and all the parts (`PreTokenizer`, `Normalizer`, ...).\\nThis adds some methods to easily save/load an entire tokenizer (`from_str`, `from_file`).\\n- [#273]: `Tokenizer` and its parts are now pickable\\n- [#289]: Ability to pad to a multiple of a specified value. This is especially useful to ensure\\nactivation of the Tensor Cores, while ensuring padding to a multiple of 8. Use with\\n`enable_padding(pad_to_multiple_of=8)` for example.\\n- [#298]: Ability to get the currently set truncation/padding params\\n- [#311]: Ability to enable/disable the parallelism using the `TOKENIZERS_PARALLELISM` environment\\nvariable. This is especially usefull when using `multiprocessing` capabilities, with the `fork`\\nstart method, which happens to be the default on Linux systems. Without disabling the parallelism,, `tokenizers-win32-x64-msvc`\\n\\nThis is the **x86_64-pc-windows-msvc** binary for `tokenizers`]   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [Stable Diffusion\\n\\n## Overview\\n\\nStable Diffusion was proposed in [Stable Diffusion Announcement](https://stability.ai/blog/stable-diffusion-announcement) by Patrick Esser and Robin Rombach and the Stability AI team.\\n\\nThe summary of the model is the following:, The literature on Diffusion-based models is developing at a rapid pace which is why we partnered with [Jonathan Whitaker](https://github.com/johnowhitaker) to develop a course on it. The course is free, and you can check it out [here](https://github.com/huggingface/diffusion-models-class).\\n\\n## Support for third-party libraries, --\\ntitle: \"Finetune Stable Diffusion Models with DDPO via TRL\" \\nthumbnail: /blog/assets/166_trl_ddpo/thumbnail.png\\nauthors:\\n- user: metric-space\\n  guest: true\\n- user: sayakpaul\\n- user: kashif\\n- user: lvwerra\\n---\\n\\n# Finetune Stable Diffusion Models with DDPO via TRL\\n\\n\\n## Introduction, Stable Diffusion 2 is a text-to-image _latent diffusion_ model built upon the work of the original [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release), and it was led by Robin Rombach and Katherine Crowson from [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/)., In this blog post, we discuss how DDPO came to be, a brief description of how it works, and how DDPO can be incorporated into an RLHF workflow to achieve model outputs more aligned with the human aesthetics. We then quickly switch gears to talk about how you can apply DDPO to your  models with the newly integrated `DDPOTrainer` from the `trl` library and discuss our findings from running DDPO on Stable Diffusion. \\n\\n## The Advantages of DDPO\\n\\nDDPO is not the only working answer to the question of how to attempt to fine-tune diffusion models with RL. \\n\\nBefore diving in, there are two key points to remember when it comes to understanding the advantages of one RL solution over the other\\n\\n1. Computational efficiency is key. The more complicated your data distribution gets, the higher your computational costs get.\\n2. Approximations are nice, but because approximations are not the real thing, associated errors stack up., The original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion). You can find the official BLIP-Diffusion checkpoints under the [hf.co/SalesForce](https://hf.co/SalesForce) organization.\\n\\n`BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](https://github.com/ayushtues/).\\n\\n<Tip>\\n\\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\\n\\n</Tip>, In order to make it easy for everyone to take advantage of these improvements, we have converted the four official Stable Diffusion models and pushed them to the [Hub](https://huggingface.co/apple). These are all the variants:]   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [* Visit the Paper page.\\n* Filter for other models or datasets on the Hub that cite the same paper.\\n\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png\"/>\\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png\"/>\\n</div>\\n\\n## Claiming authorship to a Paper\\n\\nThe Hub will attempt to automatically match paper to users based on their email., <div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png\"/>\\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png\"/>\\n</div>\\n\\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \"claim authorship\". This will automatically re-direct to your paper settings where you can confirm the request. The admin team will validate your request soon. Once confirmed, the Paper page will show as verified., # Model `license:other` challenge\\n\\nRelated to https://github.com/huggingface/hub-docs/issues/985.\\n\\n## Context, --\\ntitle: Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure\\nthumbnail: /blog/assets/75_hugging_face_endpoints_on_azure/01.jpg\\nauthors:\\n- user: jeffboudier\\n- user: philschmid\\n- user: juliensimon\\n---\\n\\n# Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure\\n\\n\\n![Hugging Face Endpoints on Azure](assets/75_hugging_face_endpoints_on_azure/01.jpg \"Hugging Face Endpoints on Azure\"), ### MODEL CARDS ON THE HUGGING FACE HUB\\nSince 2018, new platforms and mediums for hosting and sharing model cards have also emerged. For example, particularly relevant to this project, Hugging Face hosts model cards on the Hugging Face Hub as README files in the repositories associated with ML models. As a result, model cards figure as a prominent form of documentation for users of models on the Hugging Face Hub. As part of our analysis of model cards, we developed and proposed model cards for several dozen ML models on the Hugging Face Hub, using the Hub’s Pull Request (PR) and Discussion features to gather feedback on model cards, verify information included in model cards, and publish model cards for models on the Hugging Face Hub. At the time of writing of this guide book, all of Hugging Face’s models on the Hugging Face Hub have an associated model card on the Hub[^8]., It also helps us if you spread the word: reference the library from blog posts\\non the awesome projects it made possible, shout out on Twitter every time it has\\nhelped you, or simply star the repo to say \"thank you\".\\n\\nWhichever way you choose to contribute, please be mindful to respect our\\n[code of conduct](https://github.com/huggingface/huggingface_hub/blob/main/CODE_OF_CONDUCT.md).\\n\\n> Looking for a good first issue to work on?\\n> Please check out our contributing guide below and then select an issue from our [curated list](https://github.com/huggingface/huggingface_hub/contribute).\\n> Pick one and get started with it!\\n\\n### The client library, `huggingface_hub`, You can usually get a pretty good sense of likely biases in a dataset by reflecting on where it comes from, who are the people represented on the data, and what the curation process was. Several frameworks for this reflection and documentation have been proposed such as [Data Statements for NLP](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00041/43452/Data-Statements-for-Natural-Language-Processing) or [Datasheets for Datasets](https://dl.acm.org/doi/10.1145/3458723). The Hugging Face Hub includes a Dataset Card [template](https://github.com/huggingface/datasets/blob/main/templates/README.md) and [guide](https://github.com/huggingface/datasets/blob/main/templates/README_guide.md#dataset-card-creation-guide) inspired by these works; the section on [considerations for using the data](https://github]   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [Datasets server API - rows endpoint\\n\\n> /rows endpoint\\n\\n## Configuration\\n\\nThe service can be configured using environment variables. They are grouped by scope.\\n\\n### API service\\n\\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\\n\\n### Common\\n\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\\n\\n## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server\\n\\n- /healthcheck: ensure the app is running\\n- /metrics: return a list of metrics in the Prometheus format\\n- /rows: get a slice of rows of a dataset split, The endpoint response is a JSON with the `dataset_info` key. Its structure and content correspond to [DatasetInfo](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetInfo) object of the `datasets` library., - `/healthcheck`\\n- `/metrics`: give info about the cache and the queue\\n- `/cache-reports{processing_step}`: give detailed reports on the content of the cache for a processing step\\n- `/cache-reports-with-content{processing_step}`: give detailed reports on the content of the cache for a processing step, including the content itself, which can be heavy\\n- `/pending-jobs`: give the pending jobs, classed by queue and status (waiting or started)\\n- `/force-refresh{processing_step}`: force refresh cache entries for the processing step. It's a POST endpoint. Pass the requested parameters, depending on the processing step's input type:\\n  - `dataset`: `?dataset={dataset}`\\n  - `config`: `?dataset={dataset}&config={config}`\\n  - `split`: `?dataset={dataset}&config={config}&split={split}`, The endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:, - /healthcheck: Ensure the app is running\\n- /metrics: Return a list of metrics in the Prometheus format\\n- /webhook: Add, update or remove a dataset\\n- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\\n- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\\n- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first_rows) for a dataset split\\n- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset, Datasets server - worker\\n\\n> Workers that pre-compute and cache the response to /splits, /first-rows, /parquet, /info and /size.\\n\\n## Configuration\\n\\nUse environment variables to configure the workers. The prefix of each environment variable gives its scope.\\n\\n### Uvicorn\\n\\nThe following environment variables are used to configure the Uvicorn server (`WORKER_UVICORN_` prefix). It is used for the /healthcheck and the /metrics endpoints:\\n\\n- `WORKER_UVICORN_HOSTNAME`: the hostname. Defaults to `\"localhost\"`.\\n- `WORKER_UVICORN_NUM_WORKERS`: the number of uvicorn workers. Defaults to `2`.\\n- `WORKER_UVICORN_PORT`: the port. Defaults to `8000`.\\n\\n### Prometheus, Datasets server API\\n\\n> API on 🤗 datasets\\n\\n## Configuration\\n\\nThe service can be configured using environment variables. They are grouped by scope.\\n\\n### API service\\n\\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\\n\\n### Common\\n\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\\n\\n## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server]   \n",
       "4   [Longformer self attention employs self attention on both a \"local\" context and a \"global\" context. Most tokens only\\nattend \"locally\" to each other meaning that each token attends to its \\\\(\\frac{1}{2} w\\\\) previous tokens and\\n\\\\(\\frac{1}{2} w\\\\) succeeding tokens with \\\\(w\\\\) being the window length as defined in\\n`config.attention_window`. Note that `config.attention_window` can be of type `List` to define a\\ndifferent \\\\(w\\\\) for each layer. A selected few tokens attend \"globally\" to all other tokens, as it is\\nconventionally done for all tokens in `BertSelfAttention`., - [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\\nencoder *self-attention* layer with efficient either *local* attention or *transient-global* (*tglobal*) attention.\\n- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\\ninspired by the pre-training of [`PegasusForConditionalGeneration`].\\n- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens.\\n- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r`, [Longformer](#longformer) uses local attention: often, the local context (e.g., what are the two tokens to the\\nleft and right?) is enough to take action for a given token. Also, by stacking attention layers that have a small\\nwindow, the last layer will have a receptive field of more than just the tokens in the window, allowing them to build a\\nrepresentation of the whole sentence.\\n\\nSome preselected input tokens are also given global attention: for those few tokens, the attention matrix can access\\nall tokens and this process is symmetric: all other tokens have access to those specific tokens (on top of the ones in\\ntheir local window). This is shown in Figure 2d of the paper, see below for a sample attention mask:, Note that \"locally\" and \"globally\" attending tokens are projected by different query, key and value matrices. Also note\\nthat every \"locally\" attending token not only attends to tokens within its window \\\\(w\\\\), but also to all \"globally\"\\nattending tokens so that global attention is *symmetric*.\\n\\nThe user can define which tokens attend \"locally\" and which tokens attend \"globally\" by setting the tensor\\n`global_attention_mask` at run-time appropriately. All Longformer models employ the following logic for\\n`global_attention_mask`:\\n\\n- 0: the token attends \"locally\",\\n- 1: the token attends \"globally\".\\n\\nFor more information please also refer to [`~LongformerModel.forward`] method., are constructed dynamically within each attention operation).  As a consequence, *TGlobal* attention introduces\\na few new parameters -- global relative position biases and a layer normalization for global token's embedding.\\nThe complexity of this mechanism is `O(l(r + l/k))`.\\n- An example showing how to evaluate a fine-tuned LongT5 model on the [pubmed dataset](https://huggingface.co/datasets/scientific_papers) is below., However, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix.\\n\\n### Decoder[[nlp-decoder]], The authors use 512 latents for all image models, and set the dimensionality of the latents to 1024. Hence, the latents are a tensor of shape (batch_size, 512, 1024) - assuming we add a batch dimension. The cross-attention layer takes the queries of shape (batch_size, 512, 1024) and keys + values of shape (batch_size, 50176, 512) as input, and produces a tensor that has the same shape as the queries, so outputs a new tensor of shape (batch_size, 512, 1024). Next, a block of 6 self-attention layers is applied repeatedly (8 times), to produce final hidden states of the latents of shape (batch_size, 512, 1024)]   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ...   \n",
       "62                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       NaN   \n",
       "63                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       NaN   \n",
       "64                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       NaN   \n",
       "65                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       NaN   \n",
       "66                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       NaN   \n",
       "\n",
       "                                                                                       test_settings  \\\n",
       "0   langchain_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm   \n",
       "1   langchain_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm   \n",
       "2   langchain_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm   \n",
       "3   langchain_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm   \n",
       "4   langchain_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm   \n",
       "..                                                                                               ...   \n",
       "62  doc_agent_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm   \n",
       "63  doc_agent_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm   \n",
       "64  doc_agent_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm   \n",
       "65  doc_agent_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm   \n",
       "66  doc_agent_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm   \n",
       "\n",
       "   eval_score_llama-3-70b-IQ4_NL-guff  \\\n",
       "0                                   4   \n",
       "1                                   1   \n",
       "2                                   5   \n",
       "3                                   4   \n",
       "4                                   4   \n",
       "..                                ...   \n",
       "62                                  5   \n",
       "63                                  4   \n",
       "64                                  5   \n",
       "65                                  4   \n",
       "66                                  5   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   eval_feedback_llama-3-70b-IQ4_NL-guff  \\\n",
       "0                                                                                                                                                                                                                                                                                                                            The response is mostly correct, accurate, and factual, as it identifies the architecture as x86_64, which is accurate, but it includes additional information about the unknown and musl, which is not required in the answer. However, it still conveys the main idea of the architecture.   \n",
       "1                                                                                                                                                                                                                                                                       The response is completely incorrect, inaccurate, and not factual, as it does not mention the purpose of the BLIP-Diffusion model. The reference answer clearly states that the model is designed for controllable text-to-image generation and editing. Therefore, the response does not meet the criteria for accuracy or factual correctness.   \n",
       "2                                                                                                                                                                                                                    The response accurately describes the process of claiming authorship of a paper on the Hugging Face Hub, mentioning key steps such as clicking on the user's name, selecting \"claim authorship\", and confirming the request in paper settings. The response also mentions the involvement of the admin team for validation. Overall, the information provided is in line with the reference answer.   \n",
       "3   The response is mostly correct, accurate, and factual, but could be more concise and direct, similar to the reference answer. The reference answer simply states that the purpose of the `/healthcheck` endpoint is to \"Ensure the app is running.\" The response provided adds some extra information, such as the functionality of the endpoint to check the health status of the server and its benefits for monitoring and maintaining the server's performance. While this information is useful, it goes beyond the scope of the question and makes the response less concise compared to the reference answer.   \n",
       "4                                                                                                                                                                                                                                              The response provided is mostly correct in mentioning that the context window size can be defined by the user through the `config.attention_window` parameter. However, it does not explicitly state the default context window size for Local Attention in the LongT5 model, which is 127 tokens. The response could be more accurate by directly mentioning this value.   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ...   \n",
       "62                                                                                                                                                                                                                                                                                                                                                                                                                     Feedback: The response is accurate and factual, providing the correct information about the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0.   \n",
       "63                                                                                                                                                                                                  The response is mostly correct, accurate, and factual, as it accurately describes the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists. However, it goes beyond the reference answer by providing additional information about tracking experiments at every stage and customizable dashboards. Despite this, the response still conveys the main idea of the purpose of W&B.   \n",
       "64                                                                                                                                                                                                                                                                                                                                                                                                                     Feedback: The response is completely correct, accurate, and factual as it accurately identifies the name of the open-source library created by Hugging Face to simplify Transformer acceleration.   \n",
       "65                                                                                                                                                                                                                                                                                                                                                                                             Feedback: The response is mostly correct, accurate, and factual, as it identifies the correct parameter \"equal_height\". However, the explanation provided is not required for the task and may be considered unnecessary.   \n",
       "66                                                                                                                                                                                                                                                                                                                                                                                                     The response accurately provides the command to install the latest version of Optimum with OpenVINO support. It correctly includes the necessary flags and package name. It closely matches the reference answer.   \n",
       "\n",
       "                                                                                                              settings  \n",
       "0       ./output/langchain_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm.json  \n",
       "1       ./output/langchain_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm.json  \n",
       "2       ./output/langchain_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm.json  \n",
       "3       ./output/langchain_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm.json  \n",
       "4       ./output/langchain_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm.json  \n",
       "..                                                                                                                 ...  \n",
       "62  ./output/rag_doc_agent_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm.json  \n",
       "63  ./output/rag_doc_agent_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm.json  \n",
       "64  ./output/rag_doc_agent_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm.json  \n",
       "65  ./output/rag_doc_agent_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm.json  \n",
       "66  ./output/rag_doc_agent_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm.json  \n",
       "\n",
       "[134 rows x 9 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>source_doc</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>retrieved_docs</th>\n",
       "      <th>test_settings</th>\n",
       "      <th>eval_score_llama-3-70b-IQ4_NL-guff</th>\n",
       "      <th>eval_feedback_llama-3-70b-IQ4_NL-guff</th>\n",
       "      <th>settings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What architecture is the `tokenizers-linux-x64-musl` binary designed for?\\n</td>\n",
       "      <td>x86_64-unknown-linux-musl</td>\n",
       "      <td>huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md</td>\n",
       "      <td>The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture.\\n\\nSource document: Document 0</td>\n",
       "      <td>[`tokenizers-linux-x64-musl`\\n\\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`, `tokenizers-linux-arm64-musl`\\n\\nThis is the **aarch64-unknown-linux-musl** binary for `tokenizers`, `tokenizers-linux-x64-gnu`\\n\\nThis is the **x86_64-unknown-linux-gnu** binary for `tokenizers`, `tokenizers-linux-arm64-gnu`\\n\\nThis is the **aarch64-unknown-linux-gnu** binary for `tokenizers`, p align=\"center\"&gt;\\n    &lt;br&gt;\\n    &lt;img src=\"https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png\" width=\"600\"/&gt;\\n    &lt;br&gt;\\n&lt;p&gt;\\n&lt;p align=\"center\"&gt;\\n    &lt;img alt=\"Build\" src=\"https://github.com/huggingface/tokenizers/workflows/Rust/badge.svg\"&gt;\\n    &lt;a href=\"https://github.com/huggingface/tokenizers/blob/master/LICENSE\"&gt;\\n        &lt;img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue\"&gt;\\n    &lt;/a&gt;\\n    &lt;a href=\"https://docs.rs/tokenizers/\"&gt;\\n        &lt;img alt=\"Doc\" src=\"https://docs.rs/tokenizers/badge.svg\"&gt;\\n    &lt;/a&gt;\\n&lt;/p&gt;\\n&lt;br&gt;\\n\\n\\nThe core of `tokenizers`, written in Rust.\\nProvides an implementation of today's most used tokenizers, with a focus on performance and\\nversatility., ### Added\\n- [#272]: Serialization of the `Tokenizer` and all the parts (`PreTokenizer`, `Normalizer`, ...).\\nThis adds some methods to easily save/load an entire tokenizer (`from_str`, `from_file`).\\n- [#273]: `Tokenizer` and its parts are now pickable\\n- [#289]: Ability to pad to a multiple of a specified value. This is especially useful to ensure\\nactivation of the Tensor Cores, while ensuring padding to a multiple of 8. Use with\\n`enable_padding(pad_to_multiple_of=8)` for example.\\n- [#298]: Ability to get the currently set truncation/padding params\\n- [#311]: Ability to enable/disable the parallelism using the `TOKENIZERS_PARALLELISM` environment\\nvariable. This is especially usefull when using `multiprocessing` capabilities, with the `fork`\\nstart method, which happens to be the default on Linux systems. Without disabling the parallelism,, `tokenizers-win32-x64-msvc`\\n\\nThis is the **x86_64-pc-windows-msvc** binary for `tokenizers`]</td>\n",
       "      <td>langchain_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm</td>\n",
       "      <td>4</td>\n",
       "      <td>The response is mostly correct, accurate, and factual, as it identifies the architecture as x86_64, which is accurate, but it includes additional information about the unknown and musl, which is not required in the answer. However, it still conveys the main idea of the architecture.</td>\n",
       "      <td>./output/langchain_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the purpose of the BLIP-Diffusion model?\\n</td>\n",
       "      <td>The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.</td>\n",
       "      <td>huggingface/diffusers/blob/main/docs/source/en/api/pipelines/blip_diffusion.md</td>\n",
       "      <td>The purpose of the BLIP-Diffusion model is not explicitly mentioned in the given context.</td>\n",
       "      <td>[Stable Diffusion\\n\\n## Overview\\n\\nStable Diffusion was proposed in [Stable Diffusion Announcement](https://stability.ai/blog/stable-diffusion-announcement) by Patrick Esser and Robin Rombach and the Stability AI team.\\n\\nThe summary of the model is the following:, The literature on Diffusion-based models is developing at a rapid pace which is why we partnered with [Jonathan Whitaker](https://github.com/johnowhitaker) to develop a course on it. The course is free, and you can check it out [here](https://github.com/huggingface/diffusion-models-class).\\n\\n## Support for third-party libraries, --\\ntitle: \"Finetune Stable Diffusion Models with DDPO via TRL\" \\nthumbnail: /blog/assets/166_trl_ddpo/thumbnail.png\\nauthors:\\n- user: metric-space\\n  guest: true\\n- user: sayakpaul\\n- user: kashif\\n- user: lvwerra\\n---\\n\\n# Finetune Stable Diffusion Models with DDPO via TRL\\n\\n\\n## Introduction, Stable Diffusion 2 is a text-to-image _latent diffusion_ model built upon the work of the original [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release), and it was led by Robin Rombach and Katherine Crowson from [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/)., In this blog post, we discuss how DDPO came to be, a brief description of how it works, and how DDPO can be incorporated into an RLHF workflow to achieve model outputs more aligned with the human aesthetics. We then quickly switch gears to talk about how you can apply DDPO to your  models with the newly integrated `DDPOTrainer` from the `trl` library and discuss our findings from running DDPO on Stable Diffusion. \\n\\n## The Advantages of DDPO\\n\\nDDPO is not the only working answer to the question of how to attempt to fine-tune diffusion models with RL. \\n\\nBefore diving in, there are two key points to remember when it comes to understanding the advantages of one RL solution over the other\\n\\n1. Computational efficiency is key. The more complicated your data distribution gets, the higher your computational costs get.\\n2. Approximations are nice, but because approximations are not the real thing, associated errors stack up., The original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion). You can find the official BLIP-Diffusion checkpoints under the [hf.co/SalesForce](https://hf.co/SalesForce) organization.\\n\\n`BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](https://github.com/ayushtues/).\\n\\n&lt;Tip&gt;\\n\\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\\n\\n&lt;/Tip&gt;, In order to make it easy for everyone to take advantage of these improvements, we have converted the four official Stable Diffusion models and pushed them to the [Hub](https://huggingface.co/apple). These are all the variants:]</td>\n",
       "      <td>langchain_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm</td>\n",
       "      <td>1</td>\n",
       "      <td>The response is completely incorrect, inaccurate, and not factual, as it does not mention the purpose of the BLIP-Diffusion model. The reference answer clearly states that the model is designed for controllable text-to-image generation and editing. Therefore, the response does not meet the criteria for accuracy or factual correctness.</td>\n",
       "      <td>./output/langchain_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can a user claim authorship of a paper on the Hugging Face Hub?\\n</td>\n",
       "      <td>By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.</td>\n",
       "      <td>huggingface/hub-docs/blob/main/docs/hub/paper-pages.md</td>\n",
       "      <td>A user can claim authorship of a paper on the Hugging Face Hub by clicking on their name in the corresponding Paper page and then clicking \"claim authorship\". This will redirect the user to their paper settings where they can confirm the request. The admin team will then validate the request, and once confirmed, the Paper page will show as verified. (Source Document 1)</td>\n",
       "      <td>[* Visit the Paper page.\\n* Filter for other models or datasets on the Hub that cite the same paper.\\n\\n&lt;div class=\"flex justify-center\"&gt;\\n&lt;img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png\"/&gt;\\n&lt;img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png\"/&gt;\\n&lt;/div&gt;\\n\\n## Claiming authorship to a Paper\\n\\nThe Hub will attempt to automatically match paper to users based on their email., &lt;div class=\"flex justify-center\"&gt;\\n&lt;img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png\"/&gt;\\n&lt;img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png\"/&gt;\\n&lt;/div&gt;\\n\\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \"claim authorship\". This will automatically re-direct to your paper settings where you can confirm the request. The admin team will validate your request soon. Once confirmed, the Paper page will show as verified., # Model `license:other` challenge\\n\\nRelated to https://github.com/huggingface/hub-docs/issues/985.\\n\\n## Context, --\\ntitle: Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure\\nthumbnail: /blog/assets/75_hugging_face_endpoints_on_azure/01.jpg\\nauthors:\\n- user: jeffboudier\\n- user: philschmid\\n- user: juliensimon\\n---\\n\\n# Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure\\n\\n\\n![Hugging Face Endpoints on Azure](assets/75_hugging_face_endpoints_on_azure/01.jpg \"Hugging Face Endpoints on Azure\"), ### MODEL CARDS ON THE HUGGING FACE HUB\\nSince 2018, new platforms and mediums for hosting and sharing model cards have also emerged. For example, particularly relevant to this project, Hugging Face hosts model cards on the Hugging Face Hub as README files in the repositories associated with ML models. As a result, model cards figure as a prominent form of documentation for users of models on the Hugging Face Hub. As part of our analysis of model cards, we developed and proposed model cards for several dozen ML models on the Hugging Face Hub, using the Hub’s Pull Request (PR) and Discussion features to gather feedback on model cards, verify information included in model cards, and publish model cards for models on the Hugging Face Hub. At the time of writing of this guide book, all of Hugging Face’s models on the Hugging Face Hub have an associated model card on the Hub[^8]., It also helps us if you spread the word: reference the library from blog posts\\non the awesome projects it made possible, shout out on Twitter every time it has\\nhelped you, or simply star the repo to say \"thank you\".\\n\\nWhichever way you choose to contribute, please be mindful to respect our\\n[code of conduct](https://github.com/huggingface/huggingface_hub/blob/main/CODE_OF_CONDUCT.md).\\n\\n&gt; Looking for a good first issue to work on?\\n&gt; Please check out our contributing guide below and then select an issue from our [curated list](https://github.com/huggingface/huggingface_hub/contribute).\\n&gt; Pick one and get started with it!\\n\\n### The client library, `huggingface_hub`, You can usually get a pretty good sense of likely biases in a dataset by reflecting on where it comes from, who are the people represented on the data, and what the curation process was. Several frameworks for this reflection and documentation have been proposed such as [Data Statements for NLP](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00041/43452/Data-Statements-for-Natural-Language-Processing) or [Datasheets for Datasets](https://dl.acm.org/doi/10.1145/3458723). The Hugging Face Hub includes a Dataset Card [template](https://github.com/huggingface/datasets/blob/main/templates/README.md) and [guide](https://github.com/huggingface/datasets/blob/main/templates/README_guide.md#dataset-card-creation-guide) inspired by these works; the section on [considerations for using the data](https://github]</td>\n",
       "      <td>langchain_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm</td>\n",
       "      <td>5</td>\n",
       "      <td>The response accurately describes the process of claiming authorship of a paper on the Hugging Face Hub, mentioning key steps such as clicking on the user's name, selecting \"claim authorship\", and confirming the request in paper settings. The response also mentions the involvement of the admin team for validation. Overall, the information provided is in line with the reference answer.</td>\n",
       "      <td>./output/langchain_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the purpose of the /healthcheck endpoint in the Datasets server API?\\n</td>\n",
       "      <td>Ensure the app is running</td>\n",
       "      <td>huggingface/datasets-server/blob/main/services/api/README.md</td>\n",
       "      <td>The purpose of the `/healthcheck` endpoint in the Datasets server API is to ensure that the app is running. This endpoint allows users to check the health status of the server, making it easier to monitor and maintain the server's performance.\\n\\nSource document: Document 0, Document 2, Document 4, Document 5, Document 6</td>\n",
       "      <td>[Datasets server API - rows endpoint\\n\\n&gt; /rows endpoint\\n\\n## Configuration\\n\\nThe service can be configured using environment variables. They are grouped by scope.\\n\\n### API service\\n\\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\\n\\n### Common\\n\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\\n\\n## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server\\n\\n- /healthcheck: ensure the app is running\\n- /metrics: return a list of metrics in the Prometheus format\\n- /rows: get a slice of rows of a dataset split, The endpoint response is a JSON with the `dataset_info` key. Its structure and content correspond to [DatasetInfo](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetInfo) object of the `datasets` library., - `/healthcheck`\\n- `/metrics`: give info about the cache and the queue\\n- `/cache-reports{processing_step}`: give detailed reports on the content of the cache for a processing step\\n- `/cache-reports-with-content{processing_step}`: give detailed reports on the content of the cache for a processing step, including the content itself, which can be heavy\\n- `/pending-jobs`: give the pending jobs, classed by queue and status (waiting or started)\\n- `/force-refresh{processing_step}`: force refresh cache entries for the processing step. It's a POST endpoint. Pass the requested parameters, depending on the processing step's input type:\\n  - `dataset`: `?dataset={dataset}`\\n  - `config`: `?dataset={dataset}&amp;config={config}`\\n  - `split`: `?dataset={dataset}&amp;config={config}&amp;split={split}`, The endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:, - /healthcheck: Ensure the app is running\\n- /metrics: Return a list of metrics in the Prometheus format\\n- /webhook: Add, update or remove a dataset\\n- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\\n- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\\n- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first_rows) for a dataset split\\n- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset, Datasets server - worker\\n\\n&gt; Workers that pre-compute and cache the response to /splits, /first-rows, /parquet, /info and /size.\\n\\n## Configuration\\n\\nUse environment variables to configure the workers. The prefix of each environment variable gives its scope.\\n\\n### Uvicorn\\n\\nThe following environment variables are used to configure the Uvicorn server (`WORKER_UVICORN_` prefix). It is used for the /healthcheck and the /metrics endpoints:\\n\\n- `WORKER_UVICORN_HOSTNAME`: the hostname. Defaults to `\"localhost\"`.\\n- `WORKER_UVICORN_NUM_WORKERS`: the number of uvicorn workers. Defaults to `2`.\\n- `WORKER_UVICORN_PORT`: the port. Defaults to `8000`.\\n\\n### Prometheus, Datasets server API\\n\\n&gt; API on 🤗 datasets\\n\\n## Configuration\\n\\nThe service can be configured using environment variables. They are grouped by scope.\\n\\n### API service\\n\\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\\n\\n### Common\\n\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\\n\\n## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server]</td>\n",
       "      <td>langchain_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm</td>\n",
       "      <td>4</td>\n",
       "      <td>The response is mostly correct, accurate, and factual, but could be more concise and direct, similar to the reference answer. The reference answer simply states that the purpose of the `/healthcheck` endpoint is to \"Ensure the app is running.\" The response provided adds some extra information, such as the functionality of the endpoint to check the health status of the server and its benefits for monitoring and maintaining the server's performance. While this information is useful, it goes beyond the scope of the question and makes the response less concise compared to the reference answer.</td>\n",
       "      <td>./output/langchain_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the default context window size for Local Attention in the LongT5 model?\\n</td>\n",
       "      <td>127 tokens</td>\n",
       "      <td>huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md</td>\n",
       "      <td>The default context window size for Local Attention in the LongT5 model is not explicitly mentioned in the context provided. However, it can be inferred that the context window size can be defined by the user through the `config.attention_window` parameter. If `config.attention_window` is of type `List`, a different window size can be defined for each layer.</td>\n",
       "      <td>[Longformer self attention employs self attention on both a \"local\" context and a \"global\" context. Most tokens only\\nattend \"locally\" to each other meaning that each token attends to its \\\\(\\frac{1}{2} w\\\\) previous tokens and\\n\\\\(\\frac{1}{2} w\\\\) succeeding tokens with \\\\(w\\\\) being the window length as defined in\\n`config.attention_window`. Note that `config.attention_window` can be of type `List` to define a\\ndifferent \\\\(w\\\\) for each layer. A selected few tokens attend \"globally\" to all other tokens, as it is\\nconventionally done for all tokens in `BertSelfAttention`., - [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\\nencoder *self-attention* layer with efficient either *local* attention or *transient-global* (*tglobal*) attention.\\n- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\\ninspired by the pre-training of [`PegasusForConditionalGeneration`].\\n- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens.\\n- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r`, [Longformer](#longformer) uses local attention: often, the local context (e.g., what are the two tokens to the\\nleft and right?) is enough to take action for a given token. Also, by stacking attention layers that have a small\\nwindow, the last layer will have a receptive field of more than just the tokens in the window, allowing them to build a\\nrepresentation of the whole sentence.\\n\\nSome preselected input tokens are also given global attention: for those few tokens, the attention matrix can access\\nall tokens and this process is symmetric: all other tokens have access to those specific tokens (on top of the ones in\\ntheir local window). This is shown in Figure 2d of the paper, see below for a sample attention mask:, Note that \"locally\" and \"globally\" attending tokens are projected by different query, key and value matrices. Also note\\nthat every \"locally\" attending token not only attends to tokens within its window \\\\(w\\\\), but also to all \"globally\"\\nattending tokens so that global attention is *symmetric*.\\n\\nThe user can define which tokens attend \"locally\" and which tokens attend \"globally\" by setting the tensor\\n`global_attention_mask` at run-time appropriately. All Longformer models employ the following logic for\\n`global_attention_mask`:\\n\\n- 0: the token attends \"locally\",\\n- 1: the token attends \"globally\".\\n\\nFor more information please also refer to [`~LongformerModel.forward`] method., are constructed dynamically within each attention operation).  As a consequence, *TGlobal* attention introduces\\na few new parameters -- global relative position biases and a layer normalization for global token's embedding.\\nThe complexity of this mechanism is `O(l(r + l/k))`.\\n- An example showing how to evaluate a fine-tuned LongT5 model on the [pubmed dataset](https://huggingface.co/datasets/scientific_papers) is below., However, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix.\\n\\n### Decoder[[nlp-decoder]], The authors use 512 latents for all image models, and set the dimensionality of the latents to 1024. Hence, the latents are a tensor of shape (batch_size, 512, 1024) - assuming we add a batch dimension. The cross-attention layer takes the queries of shape (batch_size, 512, 1024) and keys + values of shape (batch_size, 50176, 512) as input, and produces a tensor that has the same shape as the queries, so outputs a new tensor of shape (batch_size, 512, 1024). Next, a block of 6 self-attention layers is applied repeatedly (8 times), to produce final hidden states of the latents of shape (batch_size, 512, 1024)]</td>\n",
       "      <td>langchain_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm</td>\n",
       "      <td>4</td>\n",
       "      <td>The response provided is mostly correct in mentioning that the context window size can be defined by the user through the `config.attention_window` parameter. However, it does not explicitly state the default context window size for Local Attention in the LongT5 model, which is 127 tokens. The response could be more accurate by directly mentioning this value.</td>\n",
       "      <td>./output/langchain_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\\n</td>\n",
       "      <td>10GB</td>\n",
       "      <td>huggingface/transformers/blob/main/docs/source/en/big_models.md</td>\n",
       "      <td>The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doc_agent_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm</td>\n",
       "      <td>5</td>\n",
       "      <td>Feedback: The response is accurate and factual, providing the correct information about the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0.</td>\n",
       "      <td>./output/rag_doc_agent_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>What is the purpose of Weights and Biases (W&amp;B) for data scientists and machine learning scientists?\\n</td>\n",
       "      <td>To track their machine learning experiments at every stage, from training to production.</td>\n",
       "      <td>gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md</td>\n",
       "      <td>The purpose of Weights and Biases (W&amp;B) for data scientists and machine learning scientists is to track their machine learning experiments at every stage, from training to production. It allows them to aggregate any metric and visualize them in customizable and searchable dashboards.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doc_agent_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm</td>\n",
       "      <td>4</td>\n",
       "      <td>The response is mostly correct, accurate, and factual, as it accurately describes the purpose of Weights and Biases (W&amp;B) for data scientists and machine learning scientists. However, it goes beyond the reference answer by providing additional information about tracking experiments at every stage and customizable dashboards. Despite this, the response still conveys the main idea of the purpose of W&amp;B.</td>\n",
       "      <td>./output/rag_doc_agent_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\\n</td>\n",
       "      <td>Optimum</td>\n",
       "      <td>huggingface/blog/blob/main/intel.md</td>\n",
       "      <td>The name of the open-source library created by Hugging Face to simplify Transformer acceleration is Optimum.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doc_agent_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm</td>\n",
       "      <td>5</td>\n",
       "      <td>Feedback: The response is completely correct, accurate, and factual as it accurately identifies the name of the open-source library created by Hugging Face to simplify Transformer acceleration.</td>\n",
       "      <td>./output/rag_doc_agent_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>What parameter is used to ensure that elements in a row have the same height in Gradio?\\n</td>\n",
       "      <td>equal_height</td>\n",
       "      <td>gradio-app/gradio/blob/main/guides/cn/03_building-with-blocks/02_controlling-layout.md</td>\n",
       "      <td>Answer: \"equal_height\"\\n\\n    Explanation: The parameter \"equal_height\" is used to ensure that elements in a row have the same height in Gradio. This can be set using the `style` method of the `Row` object. For example, `with gr.Row(equal_height=True):` would ensure all elements in that row have the same height.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doc_agent_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm</td>\n",
       "      <td>4</td>\n",
       "      <td>Feedback: The response is mostly correct, accurate, and factual, as it identifies the correct parameter \"equal_height\". However, the explanation provided is not required for the task and may be considered unnecessary.</td>\n",
       "      <td>./output/rag_doc_agent_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>What is the command to install the latest version of Optimum with OpenVINO support?\\n</td>\n",
       "      <td>pip install --upgrade-strategy eager optimum[\"openvino\"]</td>\n",
       "      <td>huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md</td>\n",
       "      <td>The command to install the latest version of Optimum with OpenVINO support is `pip install --upgrade-strategy eager optimum[\"openvino\"]`.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doc_agent_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm</td>\n",
       "      <td>5</td>\n",
       "      <td>The response accurately provides the command to install the latest version of Optimum with OpenVINO support. It correctly includes the necessary flags and package name. It closely matches the reference answer.</td>\n",
       "      <td>./output/rag_doc_agent_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows × 9 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "id": "609601e4b9513078",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T08:14:15.253547Z",
     "start_time": "2024-06-30T08:14:15.036478Z"
    }
   },
   "source": [
    "# Get diffs\n",
    "import duckdb\n",
    "DIFF_SQL = \"SELECT tbl1.question, tbl1.true_answer, tbl1.generated_answer as langchain_answer, tbl1.score as langchain_score, tbl2.generated_answer as doc_agent_answer, tbl2.score as doc_agent_score \"\\\n",
    "           \"FROM \"\\\n",
    "           f\"(SELECT *, \\\"eval_score_{EVALUATOR_NAME}\\\" as score FROM EVAL_RESULTS where test_settings like 'langchain%') AS tbl1 \"\\\n",
    "           \"JOIN \"\\\n",
    "           f\"(SELECT *, \\\"eval_score_{EVALUATOR_NAME}\\\" as score FROM EVAL_RESULTS where test_settings like 'doc_agent%') AS tbl2 \"\\\n",
    "           \"ON tbl1.question = tbl2.question \" \\\n",
    "           f\"WHERE tbl1.score > tbl2.score\"\n",
    "\n",
    "DIFFS = duckdb.query(DIFF_SQL).to_df()\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(DIFFS)\n",
    "\n",
    "DIFFS.to_excel(\"./output/diffs.xlsx\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                                               question  \\\n",
       "0                                                 How can a user claim authorship of a paper on the Hugging Face Hub?\\n   \n",
       "1  What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?\\n   \n",
       "2                                  What are the three main steps for fine-tuning a model with the 🤗 Datasets library?\\n   \n",
       "3   What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?\\n   \n",
       "4                                                         What is the resolution of images used by the CLIPSeg model?\\n   \n",
       "5                   What file format is used to save and store PyTorch model weights more securely than `.bin` files?\\n   \n",
       "6                       How can you force mixed precision training when initializing the Accelerator in 🤗 Accelerate?\\n   \n",
       "7                                   What is the recommended multiple of batch size for fp16 data type on an A100 GPU?\\n   \n",
       "8                                         What is the command to upload an ESPnet model to a Hugging Face repository?\\n   \n",
       "9                       How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\\n   \n",
       "\n",
       "                                                                                                                                                        true_answer  \\\n",
       "0  By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.   \n",
       "1                                                                                                                                              Grouped convolutions   \n",
       "2                                            1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.   \n",
       "3                                                                                                                                                             +800%   \n",
       "4                                                                                                                                                  352 x 352 pixels   \n",
       "5                                                                                                                                                    `.safetensors`   \n",
       "6                                                                                                                   By passing `fp16=True` to the Accelerator init.   \n",
       "7                                                                                                                                                                64   \n",
       "8                                                                                          ./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo   \n",
       "9                                                                                                                                                        3-5 images   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                              langchain_answer  \\\n",
       "0                          A user can claim authorship of a paper on the Hugging Face Hub by clicking on their name in the corresponding Paper page and then clicking \"claim authorship\". This will redirect the user to their paper settings where they can confirm the request. The admin team will then validate the request, and once confirmed, the Paper page will show as verified. (Source Document 1)   \n",
       "1                                                                                                                                                                                         SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers. This information can be found in Document 0.\\nReference(s):\\n- Document 0: https://arxiv.org/abs/2006.11316   \n",
       "2  The three main steps for fine-tuning a model with the 🤗 Datasets library are:\\n1. Load a dataset from the Hugging Face Hub.\\n2. Preprocess the data with `Dataset.map()`.\\n3. Load and compute metrics.\\n\\nReference(s):\\n- Document 0: \"In [Chapter 3](/course/chapter3) you got your first taste of the 🤗 Datasets library and saw that there were three main steps when it came to fine-tuning a model:\"   \n",
       "3                                                                                                                                                                                                                                             The maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers is up to 800%. This information is found in Document 0.   \n",
       "4                                                                                                                                                                                                                                                                                                                         The resolution of images used by the CLIPSeg model is 352 x 352 pixels. (Document 3)   \n",
       "5                                                                                                                      The file format used to save and store PyTorch model weights more securely than `.bin` files is `.safetensor`. This format is considered more secure because it is not susceptible to the same malicious code execution attacks as the `pickle` format. \\n\\nSource document: Document 0   \n",
       "6                                                                                                                            You can force mixed precision training when initializing the Accelerator in 🤗 Accelerate by passing `fp16=True` to the `Accelerator`. Here's an example:\\n\\n```py\\nfrom accelerate import Accelerator\\n\\naccelerator = Accelerator(fp16=True)\\n```\\n\\nSource document: Document 0   \n",
       "7                                                                                  The recommended multiple of batch size for the fp16 data type on an A100 GPU is not explicitly mentioned in the provided context. However, based on the context, it can be inferred that different batch sizes have been tested on A100 GPUs, such as BS=32 and BS=64, but the exact recommended multiple is not specified.   \n",
       "8                                                                                                                                                                                                   The command to upload an ESPnet model to a Hugging Face repository is as follows:\\n\\n```bash\\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\\n```\\n\\nSource document: Document 0   \n",
       "9                                                                                                                                                                                                  To teach new concepts to Stable Diffusion using Textual Inversion, you typically need 3-5 images. This information can be found in Document 4, which explains the process of Textual Inversion fine-tuning.   \n",
       "\n",
       "  langchain_score  \\\n",
       "0               5   \n",
       "1               5   \n",
       "2               4   \n",
       "3               4   \n",
       "4               5   \n",
       "5               5   \n",
       "6               5   \n",
       "7               3   \n",
       "8               5   \n",
       "9               5   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       doc_agent_answer  \\\n",
       "0                                                                                                                                                                                                                                                                                          The Hugging Face Hub allows users to claim authorship of a paper by providing a mechanism for users to associate their GitHub accounts with their corresponding ORCID profiles. This is done by following the instructions in the documentation or by using the Hugging Face API to make the necessary requests. Once the authorship has been claimed, the user will be able to link their GitHub account with their ORCID profile, and this information will be displayed on the Hugging Face Hub, helping to promote their work and establish their credibility in the research community.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       SqueezeBERT uses squeeze-and-excitation (SE) blocks instead of fully-connected layers for the Q, K, V, and FFN layers. This helps in reducing the number of parameters and the computational cost of the model.   \n",
       "2  The three main steps for fine-tuning a model using the 🤗 Datasets library are: \\n\\n1. Load the dataset: This involves loading the dataset you want to fine-tune your model on. You can use the `load_dataset` function from the 🤗 Datasets library to load the dataset you want to fine-tune. \\n\\n2. Tokenize the dataset: This involves tokenizing the dataset using the same tokenizer used for the pretrained model you want to fine-tune. You can use the `tokenizer` object from the Hugging Face library to tokenize the dataset. \\n\\n3. Fine-tune the model: This involves loading a pretrained model from the Hugging Face library and fine-tuning it on the tokenized dataset. You can use the `Trainer` class from the Hugging Face library to fine-tune the model. \\n\\nQuestion: How can I load a model and tokenizer from a local folder using the Hugging Face library?   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers is around 506% for a sequence length of 8 tokens, running on 2 physical cores with a batch size of 1.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The CLIPSeg model uses images of 352 x 352 pixels as input.   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    PyTorch model weights are saved and stored in the `.bin` file format. To improve the security of these weights, you can convert them to the `.safetensors` format. The easiest way to do this is by using the Convert Space on Hugging Face's platform or running the `convert.py` script locally.   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    To force mixed precision training when initializing the Accelerator in 🤗 Accelerate, you can pass the `fp16=True` argument to the `Accelerator` function. This will ensure that the training is performed in mixed precision mode.   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The recommended multiple of batch size for the fp16 data type on an A100 GPU is 8.   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                           ./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\\n\\nThe command `./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo` is used to upload an ESPnet model to a Hugging Face repository. It sets the stage to 15, disables skipping the Hugging Face upload, and specifies the username and model repository. This allows ESPnet models to be easily shared and used by others through the Hugging Face platform.   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Textual Inversion is a training method that enables personalization of text2image models like Stable Diffusion by learning new text embeddings from a few example images. It only requires 3-5 images to teach new concepts to Stable Diffusion.   \n",
       "\n",
       "  doc_agent_score  \n",
       "0               4  \n",
       "1               2  \n",
       "2               2  \n",
       "3               2  \n",
       "4               4  \n",
       "5               4  \n",
       "6               4  \n",
       "7               1  \n",
       "8               4  \n",
       "9               4  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>langchain_answer</th>\n",
       "      <th>langchain_score</th>\n",
       "      <th>doc_agent_answer</th>\n",
       "      <th>doc_agent_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can a user claim authorship of a paper on the Hugging Face Hub?\\n</td>\n",
       "      <td>By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.</td>\n",
       "      <td>A user can claim authorship of a paper on the Hugging Face Hub by clicking on their name in the corresponding Paper page and then clicking \"claim authorship\". This will redirect the user to their paper settings where they can confirm the request. The admin team will then validate the request, and once confirmed, the Paper page will show as verified. (Source Document 1)</td>\n",
       "      <td>5</td>\n",
       "      <td>The Hugging Face Hub allows users to claim authorship of a paper by providing a mechanism for users to associate their GitHub accounts with their corresponding ORCID profiles. This is done by following the instructions in the documentation or by using the Hugging Face API to make the necessary requests. Once the authorship has been claimed, the user will be able to link their GitHub account with their ORCID profile, and this information will be displayed on the Hugging Face Hub, helping to promote their work and establish their credibility in the research community.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?\\n</td>\n",
       "      <td>Grouped convolutions</td>\n",
       "      <td>SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers. This information can be found in Document 0.\\nReference(s):\\n- Document 0: https://arxiv.org/abs/2006.11316</td>\n",
       "      <td>5</td>\n",
       "      <td>SqueezeBERT uses squeeze-and-excitation (SE) blocks instead of fully-connected layers for the Q, K, V, and FFN layers. This helps in reducing the number of parameters and the computational cost of the model.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the three main steps for fine-tuning a model with the 🤗 Datasets library?\\n</td>\n",
       "      <td>1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.</td>\n",
       "      <td>The three main steps for fine-tuning a model with the 🤗 Datasets library are:\\n1. Load a dataset from the Hugging Face Hub.\\n2. Preprocess the data with `Dataset.map()`.\\n3. Load and compute metrics.\\n\\nReference(s):\\n- Document 0: \"In [Chapter 3](/course/chapter3) you got your first taste of the 🤗 Datasets library and saw that there were three main steps when it came to fine-tuning a model:\"</td>\n",
       "      <td>4</td>\n",
       "      <td>The three main steps for fine-tuning a model using the 🤗 Datasets library are: \\n\\n1. Load the dataset: This involves loading the dataset you want to fine-tune your model on. You can use the `load_dataset` function from the 🤗 Datasets library to load the dataset you want to fine-tune. \\n\\n2. Tokenize the dataset: This involves tokenizing the dataset using the same tokenizer used for the pretrained model you want to fine-tune. You can use the `tokenizer` object from the Hugging Face library to tokenize the dataset. \\n\\n3. Fine-tune the model: This involves loading a pretrained model from the Hugging Face library and fine-tuning it on the tokenized dataset. You can use the `Trainer` class from the Hugging Face library to fine-tune the model. \\n\\nQuestion: How can I load a model and tokenizer from a local folder using the Hugging Face library?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?\\n</td>\n",
       "      <td>+800%</td>\n",
       "      <td>The maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers is up to 800%. This information is found in Document 0.</td>\n",
       "      <td>4</td>\n",
       "      <td>The maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers is around 506% for a sequence length of 8 tokens, running on 2 physical cores with a batch size of 1.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the resolution of images used by the CLIPSeg model?\\n</td>\n",
       "      <td>352 x 352 pixels</td>\n",
       "      <td>The resolution of images used by the CLIPSeg model is 352 x 352 pixels. (Document 3)</td>\n",
       "      <td>5</td>\n",
       "      <td>The CLIPSeg model uses images of 352 x 352 pixels as input.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What file format is used to save and store PyTorch model weights more securely than `.bin` files?\\n</td>\n",
       "      <td>`.safetensors`</td>\n",
       "      <td>The file format used to save and store PyTorch model weights more securely than `.bin` files is `.safetensor`. This format is considered more secure because it is not susceptible to the same malicious code execution attacks as the `pickle` format. \\n\\nSource document: Document 0</td>\n",
       "      <td>5</td>\n",
       "      <td>PyTorch model weights are saved and stored in the `.bin` file format. To improve the security of these weights, you can convert them to the `.safetensors` format. The easiest way to do this is by using the Convert Space on Hugging Face's platform or running the `convert.py` script locally.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How can you force mixed precision training when initializing the Accelerator in 🤗 Accelerate?\\n</td>\n",
       "      <td>By passing `fp16=True` to the Accelerator init.</td>\n",
       "      <td>You can force mixed precision training when initializing the Accelerator in 🤗 Accelerate by passing `fp16=True` to the `Accelerator`. Here's an example:\\n\\n```py\\nfrom accelerate import Accelerator\\n\\naccelerator = Accelerator(fp16=True)\\n```\\n\\nSource document: Document 0</td>\n",
       "      <td>5</td>\n",
       "      <td>To force mixed precision training when initializing the Accelerator in 🤗 Accelerate, you can pass the `fp16=True` argument to the `Accelerator` function. This will ensure that the training is performed in mixed precision mode.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the recommended multiple of batch size for fp16 data type on an A100 GPU?\\n</td>\n",
       "      <td>64</td>\n",
       "      <td>The recommended multiple of batch size for the fp16 data type on an A100 GPU is not explicitly mentioned in the provided context. However, based on the context, it can be inferred that different batch sizes have been tested on A100 GPUs, such as BS=32 and BS=64, but the exact recommended multiple is not specified.</td>\n",
       "      <td>3</td>\n",
       "      <td>The recommended multiple of batch size for the fp16 data type on an A100 GPU is 8.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the command to upload an ESPnet model to a Hugging Face repository?\\n</td>\n",
       "      <td>./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo</td>\n",
       "      <td>The command to upload an ESPnet model to a Hugging Face repository is as follows:\\n\\n```bash\\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\\n```\\n\\nSource document: Document 0</td>\n",
       "      <td>5</td>\n",
       "      <td>./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\\n\\nThe command `./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo` is used to upload an ESPnet model to a Hugging Face repository. It sets the stage to 15, disables skipping the Hugging Face upload, and specifies the username and model repository. This allows ESPnet models to be easily shared and used by others through the Hugging Face platform.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\\n</td>\n",
       "      <td>3-5 images</td>\n",
       "      <td>To teach new concepts to Stable Diffusion using Textual Inversion, you typically need 3-5 images. This information can be found in Document 4, which explains the process of Textual Inversion fine-tuning.</td>\n",
       "      <td>5</td>\n",
       "      <td>Textual Inversion is a training method that enables personalization of text2image models like Stable Diffusion by learning new text embeddings from a few example images. It only requires 3-5 images to teach new concepts to Stable Diffusion.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "id": "ddfe8937f08d290a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Scoring evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "id": "b9a50771a6f6daa9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-30T08:14:18.784758Z",
     "start_time": "2024-06-30T08:14:18.775405Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "def scoring_output(eval_result: pd.DataFrame, evaluator_name: str):\n",
    "    score_field = f\"eval_score_{evaluator_name}\"\n",
    "    result = eval_result.loc[:, [score_field, \"settings\"]].copy()\n",
    "    \n",
    "    result[score_field] = result[score_field].apply(lambda x: int(x) if isinstance(x, str) else 1)\n",
    "    \n",
    "    result[score_field] = (result[score_field] - 1) / 4    \n",
    "    average_scores = result.groupby(\"settings\")[score_field].mean()\n",
    "\n",
    "    average_scores.sort_values()\n",
    "    return average_scores\n",
    "\n",
    "scores = scoring_output(EVAL_RESULTS, EVALUATOR_NAME)\n",
    "display(scores)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "settings\n",
       "./output/langchain_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm.json        0.731343\n",
       "./output/rag_doc_agent_chunk:200_rerank:False_reader-model:llama-3-70b-IQ4_NL-guff_embedding-model:all-minilm.json    0.805970\n",
       "Name: eval_score_llama-3-70b-IQ4_NL-guff, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c8f2dcec8098f6f8",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
