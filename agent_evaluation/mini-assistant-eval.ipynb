{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Evaluation for Assistant API\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Dataset chosen is the famous `hotspotqa` which is commonly used to evaluate QA and context understanding. \n",
    "\n",
    "This notebook is targeted at following goals:\n",
    "\n",
    "1. Investigate performance of opensource solutions with `mixtral-7bx8` and `LLMCompiler` as function calling strategy.\n",
    "2. Compares differences between the above solution and the official OpenAI Assistant API (with gpt-3.5-turbo).   \n"
   ],
   "id": "693c7feb77ce3e1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install datasets numpy langchain",
   "id": "e9f6aec51818b5f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "903811e1ec7b9d9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prepare dataset\n",
    "\n",
    "Only hard level questions in [validation split](https://huggingface.co/datasets/scholarly-shadows-syndicate/hotpotqa_with_qa_gpt35/viewer/default/validation) is used in this notebook. "
   ],
   "id": "ba4f85e4ae4f417"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"scholarly-shadows-syndicate/hotpotqa_with_qa_gpt35\", split=\"validation\", streaming=True).filter(lambda x: x[\"level\"] == \"hard\")\n"
   ],
   "id": "ba34d4d65079d246",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Benchmark runner\n",
    "\n",
    "* `BenchmarkRunner.run`: load validation dataset and run the QA task, and then save the result to `output_file_path`.\n",
    "* `Benchmarkrunner.get_metrics`: load runner result from `output_file_path` and calculate metric data.\n",
    "\n",
    "Only one search tool based on TAVILY API is used during this test and I borrow it from langchain. So make sure that `TAVILY_API_KEY` is set in env variables."
   ],
   "id": "8200a33105129fb2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from langchain.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "# from langchain.tools.tavily_search import TavilySearchResults\n",
    "# from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "\n",
    "# from langchain.agents import load_tools\n",
    "\n",
    "# tools = load_tools([\"google-serper\"])\n",
    "\n",
    "# tavily_tool = TavilySearchResults(api_wrapper=TavilySearchAPIWrapper(), max_results=5)\n",
    "# search_tool_schema = convert_to_openai_function(tools[0])\n",
    "# search_tool_schema[\"name\"] = \"search\"\n",
    "# print(search_tool_schema)\n",
    "\n",
    "# result = tools[0].run(\"country with most populations\")\n",
    "# print(result)\n",
    "\n",
    "# search_result = \"\\n\".join([item[\"content\"] for item in result])\n",
    "\n",
    "# search_result\n",
    "\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "from tool.wikipedia import ReActWikipedia, DocstoreExplorer\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import Optional, Type, Any\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForToolRun,\n",
    ")\n",
    "\n",
    "class SearchInput(BaseModel):\n",
    "    query: str = Field(description=\"entity to search for on Wikipedia, e.g., Mount Everest, cheetah, San Francisco, etc.\")\n",
    "\n",
    "\n",
    "class WikiSearch(BaseTool):\n",
    "    name = \"search\"\n",
    "    description = \"useful for when you need to answer questions about current events\"\n",
    "    args_schema: Type[BaseModel] = SearchInput\n",
    "    \n",
    "    web_searcher = ReActWikipedia()\n",
    "    docstore = DocstoreExplorer(web_searcher)\n",
    "\n",
    "    def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        return self.docstore.search(query)\n",
    "\n",
    "    async def _arun(self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None) -> str:\n",
    "        return await self.docstore.asearch(query)\n",
    "\n",
    "\n",
    "wiki_search = WikiSearch()\n",
    "print(wiki_search.name)\n",
    "print(wiki_search.description)\n",
    "print(wiki_search.args)\n",
    "print(wiki_search.invoke(\"Corliss Archer in Kiss and Tell\"))\n",
    "print(wiki_search.run({\"query\": \"Mount Everest\"}))"
   ],
   "id": "f37023d36a1fe60d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import logging\n",
    "import numpy as np\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def compare_answer(answer: str, label: str):\n",
    "    \"\"\"Compare the answer (from Agent) and label (GT).\n",
    "    Label can be either a string or a number.\n",
    "    If label is a number, we allow 10% margin.\n",
    "    Otherwise, we do the best-effort string matching.\n",
    "    \"\"\"\n",
    "    if answer is None:\n",
    "        return False\n",
    "\n",
    "    # see if label is a number, e.g. \"1.0\" or \"1\"\n",
    "    if is_number(label):\n",
    "        label = float(label)\n",
    "        # try cast answer to float and return false if it fails\n",
    "        try:\n",
    "            answer = float(answer)\n",
    "        except:\n",
    "            return False\n",
    "        # allow 10% margin\n",
    "        if label * 0.9 < answer < label * 1.1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    else:\n",
    "        label = normalize_answer(label)\n",
    "        answer = normalize_answer(answer)\n",
    "        return answer == label\n",
    "\n",
    "\n",
    "class BenchmarkRunner:\n",
    "    \n",
    "    thread_history = []\n",
    "    \n",
    "    \n",
    "    def __init__(self, \n",
    "                 openai_client: OpenAI, \n",
    "                 model_name: str, \n",
    "                 instructions: str,\n",
    "                 fail_fast: bool = False,\n",
    "                 output_file_path: str = \"output/hotqa_result.json\"):\n",
    "        \"\"\"\n",
    "        Benchmark an agent with an OpenAI client.\n",
    "        :param openai_client: \n",
    "        :param model_name: \n",
    "        :param instructions: useful to provide examples for joiner of LLMCompiler\n",
    "        :param output_file_path: \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fail_fast = fail_fast\n",
    "        self.logger = logging.getLogger(\"BenchmarkRunner\")\n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "        self.client = openai_client\n",
    "        self.output_file_path = output_file_path\n",
    "        self.search_tool = wiki_search\n",
    "        self.model_name = model_name\n",
    "        self.assistant = None\n",
    "        self.instructions = instructions\n",
    "        try:\n",
    "            self.result = json.load(open(output_file_path)) if os.path.exists(output_file_path) else {}\n",
    "        except:\n",
    "            self.result = {}\n",
    "\n",
    "    def cleanup(self):\n",
    "        for thread_id in self.thread_history:\n",
    "            self.logger.info(f\"delete thread {thread_id}\")\n",
    "            self.client.beta.threads.delete(thread_id=thread_id)\n",
    "        if self.assistant:\n",
    "            self.logger.info(f\"delete assistant {self.assistant.id}\")\n",
    "            self.client.beta.assistants.delete(assistant_id=self.assistant.id)\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        try:\n",
    "            self.cleanup()\n",
    "        except Exception as e:\n",
    "            self.logger.error(e)\n",
    "\n",
    "    def run(self):\n",
    "        self.logger.info(f\"run started\")\n",
    "        \n",
    "        self.assistant = client.beta.assistants.create(\n",
    "                name=\"benchmark-runner\",\n",
    "                model=self.model_name,\n",
    "                instructions=self.instructions,\n",
    "                tools=[{\"type\": \"function\", \"function\": convert_to_openai_function(self.search_tool)}]\n",
    "            )\n",
    "        self.logger.info(f\"assistant id: {self.assistant.id}\")\n",
    "        \n",
    "        \n",
    "        for item in load_dataset(\"scholarly-shadows-syndicate/hotpotqa_with_qa_gpt35\", split=\"validation\", streaming=True).filter(lambda x: x[\"level\"] == \"hard\"):\n",
    "            item_id = item['id']\n",
    "            self.logger.info(f\"item id={item_id}, contained in result? {item_id in self.result}\")\n",
    "            if item_id in self.result and self.result[item_id][\"ok\"]:\n",
    "                continue\n",
    "            run = self.client.beta.threads.create_and_run(\n",
    "                assistant_id=self.assistant.id,\n",
    "                thread={\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"user\", \"content\": item[\"question\"]}\n",
    "                    ]\n",
    "                },\n",
    "                stream=False)\n",
    "            self.logger.info(f\"run, id={run.id}, thread_id={run.thread_id}\")\n",
    "\n",
    "            self.thread_history.append(run.thread_id)\n",
    "            result_item = {\n",
    "                \"ok\": False,\n",
    "                \"answer\": \"\",\n",
    "                \"truth\": item[\"answer\"], \n",
    "                \"id\": item[\"id\"],\n",
    "                \"rt\": 0\n",
    "            }\n",
    "            while True:\n",
    "                ts_1 = time.time()\n",
    "                run = self.client.beta.threads.runs.retrieve(thread_id=run.thread_id, run_id=run.id)\n",
    "                if run.status == \"queued\" or run.status == \"in_progress\":\n",
    "                    time.sleep(1)\n",
    "                elif run.status == \"requires_action\":\n",
    "                    tool_messages = []\n",
    "                    for call in run.required_action.submit_tool_outputs.tool_calls:\n",
    "                        self.logger.info(f\"got tool call: {call.json()}\")\n",
    "                        if call.type == \"function\" and call.function.name == self.search_tool.name:\n",
    "                            try:\n",
    "                                tool_result  = self.search_tool.run(json.loads(call.function.arguments))\n",
    "                                tool_messages.append({\"tool_call_id\": call.id, \"output\": tool_result})\n",
    "                            except Exception as e:\n",
    "                                if self.fail_fast:\n",
    "                                    raise e\n",
    "                                self.logger.error(f\"Tool error {call.function.name}  with args: {call.function.arguments}\", e)\n",
    "                                break\n",
    "                        else:\n",
    "                            if self.fail_fast:\n",
    "                                raise RuntimeError(f\"Unknown tool call occurred, function name {call.function.name}\")\n",
    "                            self.logger.error(f\"Unknown tool call occurred, function name {call.function.name}\")\n",
    "                            break\n",
    "                    self.logger.info(f\"len(tool_messages)={len(tool_messages)}, len(tool_calls)={len(run.required_action.submit_tool_outputs.tool_calls)}\")\n",
    "                    if len(tool_messages) == len(run.required_action.submit_tool_outputs.tool_calls):\n",
    "                        run = self.client.beta.threads.runs.submit_tool_outputs(thread_id=run.thread_id, run_id=run.id, tool_outputs=tool_messages)\n",
    "                        self.logger.info(f\"run object status after submit: {run.status}\")\n",
    "                    else:\n",
    "                        if self.fail_fast:\n",
    "                            raise RuntimeError(\"Not every call is responded.\")\n",
    "                        self.logger.error(\"Not every call is responded.\")\n",
    "                        break\n",
    "                elif run.status == \"completed\": \n",
    "                    messages = self.client.beta.threads.messages.list(thread_id=run.thread_id, run_id=run.id, order=\"asc\")\n",
    "                    result_item[\"ok\"] = True\n",
    "                    result_item[\"answer\"] = messages.data[-1].content[0].text.value\n",
    "                    self.logger.info(\"begin printing trajectory =============================\")\n",
    "                    for message in messages.data:\n",
    "                        self.logger.info(f\"{message.role}: {message.content[0].text.value}\")\n",
    "                    self.logger.info(\"finish printing trajectory =============================\")\n",
    "                    break\n",
    "                else:\n",
    "                    if self.fail_fast:\n",
    "                        raise RuntimeError(f\"run is in other terminal status: {run.to_json()}\")\n",
    "                    self.logger.error(f\"run is in other terminal status: {run.to_json()}\")\n",
    "                    break    \n",
    "            \n",
    "            result_item[\"rt\"] = time.time() - ts_1\n",
    "            self.result[item_id] = result_item\n",
    "            self.logger.info(f\"id={result_item['id']}, ok={result_item['ok']}\")\n",
    "            \n",
    "            # write down the result\n",
    "            with open(self.output_file_path, \"w\") as output_json:\n",
    "                json.dump(self.result, output_json)\n",
    "        \n",
    "            \n",
    "    def get_metrics(self):\n",
    "        with open(self.output_file_path, \"r\") as result_file:\n",
    "            result = json.load(result_file)\n",
    "            result_items = result.values()\n",
    "            acc = np.average([compare_answer(item[\"answer\"], item[\"truth\"]) for item in result_items])\n",
    "            rt_avg = np.average([item[\"rt\"] for item in result_items])\n",
    "            rt_std = np.std([item[\"rt\"] for item in result_items])\n",
    "            success_rate = np.average([1 if item[\"ok\"] else 0 for item in result_items])\n",
    "            \n",
    "            logging.info(f\"Success rate: {success_rate}\")\n",
    "            logging.info(f\"Accuracy: {acc}\")\n",
    "            logging.info(f\"Latency: {rt_avg} +/- {rt_std}\")\n",
    "            \n",
    "            return success_rate, acc, rt_avg, rt_std\n",
    "\n",
    "\n",
    "\n",
    "DEFAULT_JOINER_INSTRUCTIONS_WITH_EXAMPLES = r'''Here are some examples with a tool named \"search\":\n",
    "\n",
    "Question: Which magazine was started first Arthur's Magazine or First for Women?\n",
    "search({\"query\": \"Arthur's Magazine\"})\n",
    "Observation: Arthur's Magazine (1844-1846) was an American literary periodical published in Philadelphia in the 19th century.\n",
    "search({\"query\": \"First for Women\"})\n",
    "Observation: First for Women is a woman's magazine published by Bauer Media Group in the USA.[1] The magazine was started in 1989.\n",
    "Thought: Arthur's Magazine was started in 1844. First for Women was started in 1989. 1844 (Arthur's Magazine) < 1989 (First for Women), so Arthur's Magazine was started first.\n",
    "Action: Finish(Arthur's Magazine)\n",
    "<END_OF_RESPONSE>\n",
    "\n",
    "Question: Were Pavel Urysohn and Leonid Levin known for the same type of work?\n",
    "search({\"query\": \"Pavel Urysohn\"})\n",
    "Observation: Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet mathematician who is best known for his contributions in dimension theory.\n",
    "search(Leonid Levin)\n",
    "Observation: Leonid Anatolievich Levin is a Soviet-American mathematician and computer scientist.\n",
    "Thought: Pavel Urysohn is a mathematician. Leonid Levin is a mathematician and computer scientist. So Pavel Urysohn and Leonid Levin have the same type of work.\n",
    "Action: Finish(yes)\n",
    "<END_OF_RESPONSE>\n",
    "\n",
    "Question: What profession does Nicholas Ray and Elia Kazan have in common?\n",
    "Observation: Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16, 1979) was an American film director best known for the 1955 film Rebel Without a Cause.\n",
    "Observation: Elia Kazan was an American film and theatre director.\n",
    "Thought: Professions of Nicholas Ray are director, screenwriter, and actor. Professions of Elia Kazan are director. So profession Nicholas Ray and Elia Kazan have in common is director.\n",
    "Action: Finish(director)\n",
    "<END_OF_RESPONSE>'''"
   ],
   "id": "22e9c051a92eef73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Benchmarks\n",
    "\n",
    "\n",
    "## With `mini-assistant`\n",
    "\n",
    "Start mini assistant server.\n",
    "\n",
    "* `llm_compiler` is used for agent execution\n",
    "* `mixtral 7bx8` is hosted by vLLM. Please make sure you have set up `HUGGING_FACE_HUB_TOKEN` env for vLLM.\n",
    "\n",
    "vLLM shell command using docker:\n",
    "\n",
    "```shell\n",
    "docker run --runtime nvidia --gpus all \\\n",
    "    -v /workspace/dropbox/huggingface_models:/root/.cache/huggingface \\\n",
    "    --env \"HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}\" \\\n",
    "    -p 8000:8000 \\\n",
    "    --ipc=host \\\n",
    "    vllm/vllm-openai:latest \\\n",
    "    --model TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ \\\n",
    "    --quantization marlin \\\n",
    "    --dtype=float16\n",
    "```\n",
    "\n",
    "mini-assistant shell command:\n",
    "\n",
    "```shell\n",
    "mkdir -p /tmp/mini-assistant-db\n",
    "mkdir -p /tmp/mini-assistant-files\n",
    "mini-assistant --db_file_path /tmp/assistant_eval.db \\\n",
    "  --file_store_path /tmp/mini-assistant-files \\\n",
    "  --agent_executor_type=llm_compiler \\\n",
    "  --model_provider=openai \\\n",
    "  --openai_port=8000 \\\n",
    "  --openai_host=192.168.0.134 \\\n",
    "  --openai_protocol=http \\\n",
    "  --port=9091 \\\n",
    "  --verbose\n",
    "```\n",
    "\n",
    "Please make sure to make necessary modification to `--openai_host`, `--openai_port` and `--openai_protocol` according to your own vLLM setup.  \n",
    "\n",
    "\n",
    "And kick off benchmarks in python script:"
   ],
   "id": "a1591179d0e975e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if True:\n",
    "    if not os.path.exists(\"./output\"):\n",
    "        os.mkdir(\"./output\")\n",
    "    client = OpenAI(base_url=\"http://localhost:9091/v1\")\n",
    "    with BenchmarkRunner(openai_client=client, instructions=DEFAULT_JOINER_INSTRUCTIONS_WITH_EXAMPLES, model_name=\"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\", output_file_path=\"./output/miniassistant_result.json\") as benchmark_runner:\n",
    "        benchmark_runner.run()\n",
    "        benchmark_runner.get_metrics()\n",
    "    "
   ],
   "id": "8642baf2b49664bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if False:\n",
    "    if not os.path.exists(\"./output\"):\n",
    "        os.mkdir(\"./output\")\n",
    "    client = OpenAI()\n",
    "    with BenchmarkRunner(openai_client=client, instructions=DEFAULT_JOINER_INSTRUCTIONS_WITH_EXAMPLES,  model_name=\"gpt-3.5-turbo\", output_file_path=\"./output/openai_result.json\") as benchmark_runner:\n",
    "        benchmark_runner.run()\n",
    "        benchmark_runner.get_metrics()"
   ],
   "id": "8e103a1a13e346e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Verdicts\n",
    "\n",
    "## Performance analysis on `LLMCompilerAgentExecutor`\n",
    "\n",
    "In version of `0.1.3`, `LLMCompilerAgentExecutor` enables paralleled function calling with opensource models. In the evaluation test with `hotpotqa` validation dataset, it achieves accuracy around 60%. This is a similar number to original paper. \n",
    "\n",
    "After checking bad cases, although no further experiments are done due my limited energy, some concerns are raised:\n",
    "\n",
    "1. `LLMCompiler` requires LLM to have exceptional reasoning and instruct following capabilities at least on par with `gpt-3.5-turbo`, or it many be almost unusable. And to have such traits, more often, 70B models seem to be a must. If we are talking about sparse MoE models, `mixtral-7bx8` is really the gatekeeper. 7B or 13B models simple don't do the tricks. In the end, this will limit the adoptions on local devices.  \n",
    "2. `Replan` seems to be unreliable. In original paper, the efficiency of re-planing is not discussed in details. In my experiments, if the model failed to produce good plan in first plan, it's unlikely it would have better result in second round. In fact, in its [official implementation](https://github.com/SqueezeAILab/LLMCompiler/blob/main/configs/hotpotqa/configs.py#L18), in all the benchmark configs, `max-replan` is limited to `1`, which disables re-plan in the first place.\n",
    "3. In the process of dependency resolution, `joiner` plays an important role to format former answers to an entity of single word. This simplifies the argument substitution for downstream function calls which depend on those results, but it has many limitations.\n",
    "4. Examples should be given in instructions if you are playing with models weaker than 70B llama3.\n",
    "\n",
    "Again, points shown above lack more quantitative experiments, and solely based on my personal observations.\n",
    "\n",
    "The bottom line is it has better throughput for agent services than the plain `ReAct` strategy. And with proper instructions and reasonable strong model, it can achieve solid performance with function calling.\n",
    "\n",
    "\n",
    "## Comparison analysis with OpenAI\n",
    "\n",
    "Definitely more work should be done with GPT4 and even newer GTP4o. But it will cost me more time and money. So I will look into it later.\n",
    "\n",
    "At the time being, my priority is to ensure the opensource implementation of `mini-assistant` is in the pace of other commercial offerings.   "
   ],
   "id": "2ec2b992cd832c59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4168e98a6825ce01",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
