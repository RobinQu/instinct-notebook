{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Evaluation for Assistant API\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Dataset chosen is the famous `hotspotqa` which is commonly used to evaluate QA and context understanding. \n",
    "\n",
    "This notebook is targeted at following goals:\n",
    "\n",
    "1. Investigate performance of opensource solutions with `mixtral-7bx8` and `LLMCompiler` as function calling strategy.\n",
    "2. Compares differences between the above solution and the official OpenAI Assistant API (with gpt-3.5-turbo).   \n"
   ],
   "id": "693c7feb77ce3e1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "!pip install datasets numpy langchain"
   ],
   "id": "e9f6aec51818b5f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "903811e1ec7b9d9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prepare dataset\n",
    "\n",
    "Only hard level questions in [validation split](https://huggingface.co/datasets/scholarly-shadows-syndicate/hotpotqa_with_qa_gpt35/viewer/default/validation) is used in this notebook. "
   ],
   "id": "ba4f85e4ae4f417"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"scholarly-shadows-syndicate/hotpotqa_with_qa_gpt35\")\n",
    "dataset[\"validation\"][0]"
   ],
   "id": "ba34d4d65079d246",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "from langchain.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "from langchain.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def compare_answer(answer: str, label: str):\n",
    "    \"\"\"Compare the answer (from Agent) and label (GT).\n",
    "    Label can be either a string or a number.\n",
    "    If label is a number, we allow 10% margin.\n",
    "    Otherwise, we do the best-effort string matching.\n",
    "    \"\"\"\n",
    "    if answer is None:\n",
    "        return False\n",
    "\n",
    "    # see if label is a number, e.g. \"1.0\" or \"1\"\n",
    "    if is_number(label):\n",
    "        label = float(label)\n",
    "        # try cast answer to float and return false if it fails\n",
    "        try:\n",
    "            answer = float(answer)\n",
    "        except:\n",
    "            return False\n",
    "        # allow 10% margin\n",
    "        if label * 0.9 < answer < label * 1.1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    else:\n",
    "        label = normalize_answer(label)\n",
    "        answer = normalize_answer(answer)\n",
    "        return answer == label\n",
    "\n",
    "\n",
    "class BenchmarkRunner:\n",
    "    \n",
    "    thread_history = []\n",
    "    logger = logging.getLogger(\"BenchmarkRunner\")\n",
    "\n",
    "    def __init__(self, client: OpenAI, output_file_path: str = \"output/hotqa_result.json\"):\n",
    "        super().__init__()\n",
    "        self.client = client\n",
    "        self.output_file_path = output_file_path\n",
    "        self.tavily_tool = TavilySearchResults(api_wrapper=TavilySearchAPIWrapper())\n",
    "        self.assistant = client.beta.assistants.create(name=\"benchmark-runner\", tools=[convert_to_openai_function(self.tavily_tool)])\n",
    "        self.result = json.load(open(output_file_path)) if os.path.exists(output_file_path) else []\n",
    "\n",
    "    def run(self):\n",
    "        self.logger.info(f\"run started, validation set size {dataset.get('validation').dataset_size}\")\n",
    "        for item in dataset[\"validation\"]: \n",
    "            if item[\"level\"] == \"hard\":\n",
    "                continue\n",
    "\n",
    "            run = self.client.beta.threads.create_and_run(\n",
    "                assistant_id=self.assistant.id,\n",
    "                thread={\n",
    "                    \"message\": [\n",
    "                        {\"role\": \"user\", \"content\": item[\"question\"]}\n",
    "                    ]\n",
    "                },\n",
    "                stream=False)\n",
    "\n",
    "            self.thread_history.append(run.thread_id)\n",
    "            result_item = {\n",
    "                \"ok\": False,\n",
    "                \"answer\": \"\",\n",
    "                \"truth\": item[\"answer\"], \n",
    "                \"id\": item[\"id\"],\n",
    "                \"rt\": 0\n",
    "            }\n",
    "            while True:\n",
    "                ts_1 = time.time()\n",
    "                run = self.client.beta.threads.runs.retrieve(thread_id=run.thread_id, run_id=run.id)\n",
    "                if run.status == \"queued\" or run.status == \"in_progress\":\n",
    "                    time.sleep(1)\n",
    "                elif run.status == \"requires_action\":\n",
    "                    tool_messages = []\n",
    "                    for call in run.required_action.submit_tool_outputs.tool_calls:\n",
    "                        self.logger.info(f\"got tool call: {call.json()}\")\n",
    "                        if call.type == \"function\" and call.function.name == \"tavily_search_results_json\":\n",
    "                            tool_result  = self.tavily_tool.invoke(call.function.arguments)\n",
    "                            tool_messages.append({\"tool_call_id\": call.id, \"output\": tool_result})\n",
    "                        else:\n",
    "                            self.logger.error(f\"Unknown tool call occurred, function name {call.function.name}\")\n",
    "                            break\n",
    "                    run = self.client.beta.threads.runs.submit_tool_outputs(thread_id=run.thread_id, run_id=run.id, tool_outputs=tool_messages)\n",
    "                    self.logger.info(f\"run object after submit: {run.to_json()}\")\n",
    "                elif run.status == \"completed\": \n",
    "                    messages = self.client.beta.threads.messages.list(thread_id=run.thread_id, order=\"asc\")\n",
    "                    result_item[\"ok\"] = True\n",
    "                    result_item[\"answer\"] = messages[-1].content[0].text.value\n",
    "                    self.logger.info(\"begin printing trajectory =============================\")\n",
    "                    for message in messages:\n",
    "                        self.logger.info(f\"{message.role}: {message.content[0].text.value}\")\n",
    "                    self.logger.info(\"finish printing trajectory =============================\")\n",
    "                    break\n",
    "                else:\n",
    "                    self.logger.error(f\"run is in other terminal status: {run.to_json()}\")\n",
    "                    break    \n",
    "            \n",
    "            result_item[\"rt\"] = time.time() - ts_1\n",
    "            self.result.append(result_item)\n",
    "            self.logger.info(f\"id={result_item['id']}, ok={result_item['ok']}\")\n",
    "            \n",
    "            # write down the result\n",
    "            with open(self.output_file_path, \"wb\") as output_json:\n",
    "                json.dump(self.result, output_json)\n",
    "        \n",
    "            \n",
    "    def get_metrics(self):\n",
    "        with open(self.output_file_path, \"r\") as result_file:\n",
    "            result = json.load(result_file)\n",
    "            acc = np.average([compare_answer(item[\"answer\"], item[\"truth\"]) for item in result])\n",
    "            rt_avg = np.average([item[\"rt\"] for item in result])\n",
    "            rt_std = np.std([item[\"rt\"] for item in result])\n",
    "            success_rate = np.average([1 if item[\"ok\"] else 0 for item in result])\n",
    "            \n",
    "            logging.info(f\"Success rate: {success_rate}\")\n",
    "            logging.info(f\"Accuracy: {acc}\")\n",
    "            logging.info(f\"Latency: {rt_avg} +/- {rt_std}\")\n",
    "            \n",
    "            return success_rate, acc, rt_avg, rt_std\n",
    "            "
   ],
   "id": "22e9c051a92eef73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c2a67ce23181fd39"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Benchmarks\n",
    "\n",
    "\n",
    "## With `mini-assistant`\n",
    "\n",
    "Start mini assistant server.\n",
    "\n",
    "* `llm_compiler` is used for agent execution\n",
    "* `mixtral 7bx8` is hosted by vLLM. Please make sure you have set up `HUGGING_FACE_HUB_TOKEN` env for vLLM.\n",
    "\n",
    "vLLM shell command using docker:\n",
    "\n",
    "```shell\n",
    "docker run --runtime nvidia --gpus all \\\n",
    "    -v /workspace/dropbox/huggingface_models:/root/.cache/huggingface \\\n",
    "    --env \"HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}\" \\\n",
    "    -p 8000:8000 \\\n",
    "    --ipc=host \\\n",
    "    vllm/vllm-openai:latest \\\n",
    "    --model TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ \\\n",
    "    --quantization marlin \\\n",
    "    --dtype=float16\n",
    "```\n",
    "\n",
    "mini-assistant shell command:\n",
    "\n",
    "```shell\n",
    "mini-assistant --db_file_path /tmp/assistant_eval.db \\\n",
    "   --file_store_path /tmp/mini-assistant-files \\\n",
    "   --agent-executor-type llm_compiler \\\n",
    "  --model_provider=openai \\\n",
    "  --openai_port=8000 \\\n",
    "  --openai_host=192.168.0.134 \\\n",
    "  --openai_protocol=http \\\n",
    "  --port=9091 \\\n",
    "  --verbose\n",
    "```\n",
    "\n",
    "Please make sure to make necessary modification to `--openai_host`, `--openai_port` and `--openai_protocol` according to your own vLLM setup.  \n",
    "\n",
    "\n",
    "And kick off benchmarks in python script:"
   ],
   "id": "a1591179d0e975e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if True:\n",
    "    client = OpenAI(base_url=\"http://localhost:9091\")\n",
    "    benchmark_runner = BenchmarkRunner(client=client, output_file_path=\"output/miniassistant_result.json\")\n",
    "    benchmark_runner.run()\n",
    "    benchmark_runner.get_metrics()\n",
    "    "
   ],
   "id": "8642baf2b49664bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## With OpenAI's offering\n",
    "\n",
    "Please make sure you have `OPENAI_API_KEY` setup in your environments.\n"
   ],
   "id": "f70bbead8d83f756"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if True:\n",
    "    client = OpenAI()\n",
    "    benchmark_runner = BenchmarkRunner(client=client, output_file_path=\"openai_result.json\")\n",
    "    benchmark_runner.run()\n",
    "    benchmark_runner.get_metrics()"
   ],
   "id": "8e103a1a13e346e6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
