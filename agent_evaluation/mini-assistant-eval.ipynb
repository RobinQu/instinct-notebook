{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Evaluation for Assistant API\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Dataset chosen is the famous `hotspotqa` which is commonly used to evaluate QA and context understanding. \n",
    "\n",
    "This notebook is targeted at following goals:\n",
    "\n",
    "1. Investigate performance of opensource solutions with `mixtral-7bx8` and `LLMCompiler` as function calling strategy.\n",
    "2. Compares differences between the above solution and the official OpenAI Assistant API (with gpt-3.5-turbo).   \n"
   ],
   "id": "693c7feb77ce3e1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T08:20:24.589445Z",
     "start_time": "2024-05-16T08:20:22.752064Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install datasets numpy langchain",
   "id": "e9f6aec51818b5f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (2.19.1)\r\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (1.26.4)\r\n",
      "Requirement already satisfied: langchain in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (0.1.19)\r\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from datasets) (3.14.0)\r\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from datasets) (16.1.0)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from datasets) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from datasets) (2.2.2)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from datasets) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from datasets) (4.66.4)\r\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from datasets) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\r\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from datasets) (3.9.5)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from datasets) (0.23.0)\r\n",
      "Requirement already satisfied: packaging in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from datasets) (23.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from datasets) (6.0.1)\r\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from langchain) (1.4.52)\r\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from langchain) (0.6.5)\r\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from langchain) (0.0.38)\r\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from langchain) (0.1.52)\r\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from langchain) (0.0.1)\r\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from langchain) (0.1.56)\r\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from langchain) (2.7.1)\r\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from langchain) (8.3.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\r\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\r\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (1.33)\r\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.18.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain) (2.4)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\r\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T08:20:24.614598Z",
     "start_time": "2024-05-16T08:20:24.590897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "903811e1ec7b9d9d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prepare dataset\n",
    "\n",
    "Only hard level questions in [validation split](https://huggingface.co/datasets/scholarly-shadows-syndicate/hotpotqa_with_qa_gpt35/viewer/default/validation) is used in this notebook. "
   ],
   "id": "ba4f85e4ae4f417"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T08:56:22.014859Z",
     "start_time": "2024-05-16T08:56:14.278714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"scholarly-shadows-syndicate/hotpotqa_with_qa_gpt35\", split=\"validation\", streaming=True).filter(lambda x: x[\"level\"] == \"hard\")\n"
   ],
   "id": "ba34d4d65079d246",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Benchmark runner\n",
    "\n",
    "* `BenchmarkRunner.run`: load validation dataset and run the QA task, and then save the result to `output_file_path`.\n",
    "* `Benchmarkrunner.get_metrics`: load runner result from `output_file_path` and calculate metric data.\n",
    "\n",
    "Only one search tool based on TAVILY API is used during this test and I borrow it from langchain. So make sure that `TAVILY_API_KEY` is set in env variables."
   ],
   "id": "8200a33105129fb2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T09:09:09.270730Z",
     "start_time": "2024-05-16T09:09:05.486331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "from langchain.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "tavily_tool = TavilySearchResults(api_wrapper=TavilySearchAPIWrapper(), max_results=5)\n",
    "\n",
    "print(convert_to_openai_function(tavily_tool))\n",
    "\n",
    "result = tavily_tool.invoke(\"country with most populations\")\n",
    "search_result = \"\\n\".join([item[\"content\"] for item in result])\n",
    "\n",
    "search_result\n",
    "\n"
   ],
   "id": "f37023d36a1fe60d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'tavily_search_results_json', 'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query']}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Countries in the world by population (2024) This list includes both countries and dependent territories. Data based on the latest United Nations Population Division estimates. Click on the name of the country or dependency for current estimates (live population clock), historical data, and projected figures. Fert.\\nThe five most populous countries in 2022 are China, India, followed by the European Union (which is not a country), the United States, the island nation of Indonesia, and Pakistan. The Smallest Countries. Among the smallest countries in the world in terms of population are the island nations in the Caribbean and the Southern Pacific Ocean ...\\nList of countries by population (United Nations) This is a list of countries and other inhabited territories of the world by total population, based on estimates published by the United Nations in the 2022 revision of World Population Prospects. It presents population estimates from 1950 to the present. [2]\\nHowever, a number of countries considered by the International Monetary Fund to be developing countries (that is, having not achieved a high degree of industrialization relative to their populations, and where the population typically has a medium to low standard of living) also have sizeable populations, including Nigeria (over 190 million), Bangladesh (almost 165 million) and Mexico (around 129 million), demonstrating that the issues affecting developing nations span multiple continents.\\n The BRIC countries (Brazil, Russia, India and China), generally regarded as the four major emerging economies expected to dominate in the 21st century, are all in the top ten most populous countries, indicating how important the sheer size of their populations are to their economic expansion.\\n 10M\\n25M\\n50M\\n75M\\n100M\\n200M\\n1B\\nDownload Table Data\\nEnter your email below, and you'll receive this table's data in your inbox momentarily.\\n In contrast, Canada, which is also a major economic player and one of the largest countries in the world by landmass, has a relatively small population for its size, with around 36.5 million residents.\\n Not surprisingly, the largest countries in the world in terms of population are China and India, with both countries now having populations of well over a billion.\\nNot included are other entities that are not sovereign states, such as the European Union,[a] and independent territories that do not have permanent populations, such as the Chagos Archipelago and various countries' claims to Antarctica.[2]\\nSovereign states and dependencies by population\\nNote: A numbered rank is assigned to the 193 member states of the United Nations, plus the two observer states to the United Nations General Assembly. Furthermore, the addition of figures from all countries may not equal the world total.\\nAreas that form integral parts of sovereign states, such as the countries of the United Kingdom, are counted as part of the sovereign states concerned. Also given in a percentage is each country's population compared with the world population, which the United Nations estimates at 8.08 billion as of 2023[update].[1]\\nMethod\\nFigures used in this chart are based on the most up-to-date estimates or projections by the national census authority, where available, and are usually rounded off.\\n Where updated national data are not available, figures are based on the estimates or projections for 2022 by the Population Division of the United Nations Department of Economic and Social Affairs.\\n For instance, the United Kingdom is considered a single entity, while the constituent countries of the Kingdom of the Netherlands are considered separately.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T09:59:15.595180Z",
     "start_time": "2024-05-16T09:59:15.546684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import logging\n",
    "import numpy as np\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def compare_answer(answer: str, label: str):\n",
    "    \"\"\"Compare the answer (from Agent) and label (GT).\n",
    "    Label can be either a string or a number.\n",
    "    If label is a number, we allow 10% margin.\n",
    "    Otherwise, we do the best-effort string matching.\n",
    "    \"\"\"\n",
    "    if answer is None:\n",
    "        return False\n",
    "\n",
    "    # see if label is a number, e.g. \"1.0\" or \"1\"\n",
    "    if is_number(label):\n",
    "        label = float(label)\n",
    "        # try cast answer to float and return false if it fails\n",
    "        try:\n",
    "            answer = float(answer)\n",
    "        except:\n",
    "            return False\n",
    "        # allow 10% margin\n",
    "        if label * 0.9 < answer < label * 1.1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    else:\n",
    "        label = normalize_answer(label)\n",
    "        answer = normalize_answer(answer)\n",
    "        return answer == label\n",
    "\n",
    "\n",
    "class BenchmarkRunner:\n",
    "    \n",
    "    thread_history = []\n",
    "    \n",
    "    def __init__(self, client: OpenAI, model_name: str, output_file_path: str = \"output/hotqa_result.json\"):\n",
    "        super().__init__()\n",
    "        self.logger = logging.getLogger(\"BenchmarkRunner\")\n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "        self.client = client\n",
    "        self.output_file_path = output_file_path\n",
    "        self.tavily_tool = tavily_tool\n",
    "        self.model_name = model_name\n",
    "        self.assistant = None\n",
    "        try:\n",
    "            self.result = json.load(open(output_file_path)) if os.path.exists(output_file_path) else []\n",
    "        except:\n",
    "            self.result = []\n",
    "\n",
    "    def cleanup(self):\n",
    "        for thread_id in self.thread_history:\n",
    "            self.logger.info(f\"delete thread {thread_id}\")\n",
    "            self.client.beta.threads.delete(thread_id=thread_id)\n",
    "        if self.assistant:\n",
    "            self.logger.info(f\"delete assistant {self.assistant.id}\")\n",
    "            self.client.beta.assistants.delete(assistant_id=self.assistant.id)\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        try:\n",
    "            self.cleanup()\n",
    "        except Exception as e:\n",
    "            self.logger.error(e)\n",
    "\n",
    "    def run(self):\n",
    "        self.logger.info(f\"run started\")\n",
    "        for item in load_dataset(\"scholarly-shadows-syndicate/hotpotqa_with_qa_gpt35\", split=\"validation\", streaming=True).filter(lambda x: x[\"level\"] == \"hard\"):\n",
    "            self.logger.info(f\"item id={item['id']}\")   \n",
    "            self.assistant = client.beta.assistants.create(\n",
    "                name=\"benchmark-runner\",\n",
    "                model=self.model_name,\n",
    "                tools=[{\"type\": \"function\", \"function\": convert_to_openai_function(self.tavily_tool)}]\n",
    "            )\n",
    "            self.logger.info(f\"assistant id: {self.assistant.id}\")\n",
    "            \n",
    "            run = self.client.beta.threads.create_and_run(\n",
    "                assistant_id=self.assistant.id,\n",
    "                thread={\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"user\", \"content\": item[\"question\"]}\n",
    "                    ]\n",
    "                },\n",
    "                stream=False)\n",
    "            self.logger.info(f\"run, id={run.id}, thread_id={run.thread_id}\")\n",
    "\n",
    "            self.thread_history.append(run.thread_id)\n",
    "            result_item = {\n",
    "                \"ok\": False,\n",
    "                \"answer\": \"\",\n",
    "                \"truth\": item[\"answer\"], \n",
    "                \"id\": item[\"id\"],\n",
    "                \"rt\": 0\n",
    "            }\n",
    "            while True:\n",
    "                ts_1 = time.time()\n",
    "                run = self.client.beta.threads.runs.retrieve(thread_id=run.thread_id, run_id=run.id)\n",
    "                if run.status == \"queued\" or run.status == \"in_progress\":\n",
    "                    time.sleep(1)\n",
    "                elif run.status == \"requires_action\":\n",
    "                    tool_messages = []\n",
    "                    for call in run.required_action.submit_tool_outputs.tool_calls:\n",
    "                        self.logger.info(f\"got tool call: {call.json()}\")\n",
    "                        if call.type == \"function\" and call.function.name == \"tavily_search_results_json\":\n",
    "                            tool_result  = self.tavily_tool.invoke(call.function.arguments)\n",
    "                            if isinstance(tool_result, list) and len(tool_result)>0 and isinstance(tool_result[0], dict):\n",
    "                                combined_content = \"\\n\".join([item[\"content\"] for item in tool_result])\n",
    "                                tool_messages.append({\"tool_call_id\": call.id, \"output\": combined_content})\n",
    "                        else:\n",
    "                            self.logger.error(f\"Unknown tool call occurred, function name {call.function.name}\")\n",
    "                            break\n",
    "                    if len(tool_messages) == len(run.required_action.submit_tool_outputs.tool_calls):\n",
    "                        run = self.client.beta.threads.runs.submit_tool_outputs(thread_id=run.thread_id, run_id=run.id, tool_outputs=tool_messages)\n",
    "                        self.logger.info(f\"run object status after submit: {run.status}\")\n",
    "                    else:\n",
    "                        self.logger.error(\"Not every call is responded.\")\n",
    "                        break\n",
    "                elif run.status == \"completed\": \n",
    "                    messages = self.client.beta.threads.messages.list(thread_id=run.thread_id, order=\"asc\")\n",
    "                    result_item[\"ok\"] = True\n",
    "                    result_item[\"answer\"] = messages.data[-1].content[0].text.value\n",
    "                    self.logger.info(\"begin printing trajectory =============================\")\n",
    "                    for message in messages.data:\n",
    "                        self.logger.info(f\"{message.role}: {message.content[0].text.value}\")\n",
    "                    self.logger.info(\"finish printing trajectory =============================\")\n",
    "                    break\n",
    "                else:\n",
    "                    self.logger.error(f\"run is in other terminal status: {run.to_json()}\")\n",
    "                    break    \n",
    "            \n",
    "            result_item[\"rt\"] = time.time() - ts_1\n",
    "            self.result.append(result_item)\n",
    "            self.logger.info(f\"id={result_item['id']}, ok={result_item['ok']}\")\n",
    "            \n",
    "            # write down the result\n",
    "            with open(self.output_file_path, \"w\") as output_json:\n",
    "                json.dump(self.result, output_json)\n",
    "        \n",
    "            \n",
    "    def get_metrics(self):\n",
    "        with open(self.output_file_path, \"r\") as result_file:\n",
    "            result = json.load(result_file)\n",
    "            acc = np.average([compare_answer(item[\"answer\"], item[\"truth\"]) for item in result])\n",
    "            rt_avg = np.average([item[\"rt\"] for item in result])\n",
    "            rt_std = np.std([item[\"rt\"] for item in result])\n",
    "            success_rate = np.average([1 if item[\"ok\"] else 0 for item in result])\n",
    "            \n",
    "            logging.info(f\"Success rate: {success_rate}\")\n",
    "            logging.info(f\"Accuracy: {acc}\")\n",
    "            logging.info(f\"Latency: {rt_avg} +/- {rt_std}\")\n",
    "            \n",
    "            return success_rate, acc, rt_avg, rt_std\n",
    "            "
   ],
   "id": "22e9c051a92eef73",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Benchmarks\n",
    "\n",
    "\n",
    "## With `mini-assistant`\n",
    "\n",
    "Start mini assistant server.\n",
    "\n",
    "* `llm_compiler` is used for agent execution\n",
    "* `mixtral 7bx8` is hosted by vLLM. Please make sure you have set up `HUGGING_FACE_HUB_TOKEN` env for vLLM.\n",
    "\n",
    "vLLM shell command using docker:\n",
    "\n",
    "```shell\n",
    "docker run --runtime nvidia --gpus all \\\n",
    "    -v /workspace/dropbox/huggingface_models:/root/.cache/huggingface \\\n",
    "    --env \"HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}\" \\\n",
    "    -p 8000:8000 \\\n",
    "    --ipc=host \\\n",
    "    vllm/vllm-openai:latest \\\n",
    "    --model TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ \\\n",
    "    --quantization marlin \\\n",
    "    --dtype=float16\n",
    "```\n",
    "\n",
    "mini-assistant shell command:\n",
    "\n",
    "```shell\n",
    "mkdir -p /tmp/mini-assistant-db\n",
    "mkdir -p /tmp/mini-assistant-files\n",
    "mini-assistant --db_file_path /tmp/assistant_eval.db \\\n",
    "  --file_store_path /tmp/mini-assistant-files \\\n",
    "  --agent_executor_type=llm_compiler \\\n",
    "  --model_provider=openai \\\n",
    "  --openai_port=8000 \\\n",
    "  --openai_host=192.168.0.134 \\\n",
    "  --openai_protocol=http \\\n",
    "  --port=9091 \\\n",
    "  --verbose\n",
    "```\n",
    "\n",
    "Please make sure to make necessary modification to `--openai_host`, `--openai_port` and `--openai_protocol` according to your own vLLM setup.  \n",
    "\n",
    "\n",
    "And kick off benchmarks in python script:"
   ],
   "id": "a1591179d0e975e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T02:52:29.732210Z",
     "start_time": "2024-05-17T02:50:16.643874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if True:\n",
    "    if not os.path.exists(\"./output\"):\n",
    "        os.mkdir(\"./output\")\n",
    "    client = OpenAI(base_url=\"http://localhost:9091/v1\")\n",
    "    with BenchmarkRunner(client=client, model_name=\"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\", output_file_path=\"./output/miniassistant_result.json\") as benchmark_runner:\n",
    "        benchmark_runner.run()\n",
    "        benchmark_runner.get_metrics()\n",
    "    "
   ],
   "id": "8642baf2b49664bc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:BenchmarkRunner:run started\n",
      "INFO:BenchmarkRunner:item id=5a8b57f25542995d1e6f1371\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/assistants \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:assistant id: asst_759510110350344192\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/threads/runs \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:run, id=run_759510110434230272, thread_id=thread_759510110434230273\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510110434230273/runs/run_759510110434230272 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510110434230273/runs/run_759510110434230272 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510110434230273/runs/run_759510110434230272 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510110434230273/runs/run_759510110434230272 \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:got tool call: {\"id\":\"call_759510121461055488\",\"function\":{\"arguments\":\" {\\\"query\\\": \\\"Scott Derrickson nationality\\\"} \",\"name\":\"tavily_search_results_json\"},\"type\":\"function\"}\n",
      "INFO:BenchmarkRunner:got tool call: {\"id\":\"call_759510121461055489\",\"function\":{\"arguments\":\" {\\\"query\\\": \\\"Ed Wood nationality\\\"} \",\"name\":\"tavily_search_results_json\"},\"type\":\"function\"}\n",
      "ERROR:BenchmarkRunner:Not every call is responded.\n",
      "INFO:BenchmarkRunner:id=5a8b57f25542995d1e6f1371, ok=False\n",
      "INFO:BenchmarkRunner:item id=5a8c7595554299585d9e36b6\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/assistants \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:assistant id: asst_759510147453157376\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/threads/runs \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:run, id=run_759510147570597888, thread_id=thread_759510147570597889\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510147570597889/runs/run_759510147570597888 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510147570597889/runs/run_759510147570597888 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510147570597889/runs/run_759510147570597888 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510147570597889/runs/run_759510147570597888 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510147570597889/runs/run_759510147570597888 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510147570597889/runs/run_759510147570597888 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510147570597889/runs/run_759510147570597888 \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:got tool call: {\"id\":\"call_759510171977252864\",\"function\":{\"arguments\":\" { \\\"query\\\": \\\"Corliss Archer actress in Kiss and Tell\\\" } \",\"name\":\"tavily_search_results_json\"},\"type\":\"function\"}\n",
      "INFO:BenchmarkRunner:got tool call: {\"id\":\"call_759510171981447168\",\"function\":{\"arguments\":\" { \\\"query\\\": \\\"Corliss Archer actress political positions\\\" } \",\"name\":\"tavily_search_results_json\"},\"type\":\"function\"}\n",
      "ERROR:BenchmarkRunner:Not every call is responded.\n",
      "INFO:BenchmarkRunner:id=5a8c7595554299585d9e36b6, ok=False\n",
      "INFO:BenchmarkRunner:item id=5a85ea095542994775f606a8\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/assistants \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:assistant id: asst_759510199621910528\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/threads/runs \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:run, id=run_759510199726768128, thread_id=thread_759510199726768129\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510199726768129/runs/run_759510199726768128 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510199726768129/runs/run_759510199726768128 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510199726768129/runs/run_759510199726768128 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510199726768129/runs/run_759510199726768128 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510199726768129/runs/run_759510199726768128 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510199726768129/runs/run_759510199726768128 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510199726768129/runs/run_759510199726768128 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510199726768129/runs/run_759510199726768128 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510199726768129/runs/run_759510199726768128 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510199726768129/runs/run_759510199726768128 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510199726768129/runs/run_759510199726768128 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510199726768129/runs/run_759510199726768128 \"HTTP/1.1 200 OK\"\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/pydantic/main.py:398: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `int` but got `str` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "ERROR:BenchmarkRunner:run is in other terminal status: {\n",
      "  \"id\": \"run_759510199726768128\",\n",
      "  \"assistant_id\": \"asst_759510199621910528\",\n",
      "  \"created_at\": \"1715914248923000\",\n",
      "  \"expires_at\": 1715914848945586,\n",
      "  \"failed_at\": 1715914259810990,\n",
      "  \"object\": \"thread.run\",\n",
      "  \"response_format\": \"auto\",\n",
      "  \"started_at\": 1715914248945586,\n",
      "  \"status\": \"failed\",\n",
      "  \"thread_id\": \"thread_759510199726768129\",\n",
      "  \"truncation_strategy\": {\n",
      "    \"type\": \"auto\"\n",
      "  },\n",
      "  \"temperature\": 0.0,\n",
      "  \"top_p\": 0.0,\n",
      "  \"modified_at\": \"1715914259811000\"\n",
      "}\n",
      "INFO:BenchmarkRunner:id=5a85ea095542994775f606a8, ok=False\n",
      "INFO:BenchmarkRunner:item id=5adbf0a255429947ff17385a\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/assistants \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:assistant id: asst_759510247378255872\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/threads/runs \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:run, id=run_759510247462141952, thread_id=thread_759510247462141953\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510247462141953/runs/run_759510247462141952 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510247462141953/runs/run_759510247462141952 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510247462141953/runs/run_759510247462141952 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510247462141953/runs/run_759510247462141952 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510247462141953/runs/run_759510247462141952 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510247462141953/runs/run_759510247462141952 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510247462141953/runs/run_759510247462141952 \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:got tool call: {\"id\":\"call_759510270040080384\",\"function\":{\"arguments\":\"{\\\"query\\\": \\\"Laleli Mosque location\\\"}\",\"name\":\"tavily\\\\_search\\\\_results\\\\_json\"},\"type\":\"function\"}\n",
      "ERROR:BenchmarkRunner:Unknown tool call occurred, function name tavily\\_search\\_results\\_json\n",
      "ERROR:BenchmarkRunner:Not every call is responded.\n",
      "INFO:BenchmarkRunner:id=5adbf0a255429947ff17385a, ok=False\n",
      "INFO:BenchmarkRunner:item id=5a8e3ea95542995a26add48d\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/assistants \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:assistant id: asst_759510273865285632\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/threads/runs \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:run, id=run_759510273961754624, thread_id=thread_759510273961754625\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510273961754625/runs/run_759510273961754624 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510273961754625/runs/run_759510273961754624 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510273961754625/runs/run_759510273961754624 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510273961754625/runs/run_759510273961754624 \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:got tool call: {\"id\":\"call_759510285680640000\",\"function\":{\"arguments\":\"{\\\"query\\\": \\\"director of Big Stone Gap\\\"}\",\"name\":\"tavily_search_results_json\"},\"type\":\"function\"}\n",
      "INFO:BenchmarkRunner:got tool call: {\"id\":\"call_759510285680640001\",\"function\":{\"arguments\":\"{\\\"query\\\": \\\"Big Stone Gap based in New York city\\\"}\",\"name\":\"tavily_search_results_json\"},\"type\":\"function\"}\n",
      "ERROR:BenchmarkRunner:Not every call is responded.\n",
      "INFO:BenchmarkRunner:id=5a8e3ea95542995a26add48d, ok=False\n",
      "INFO:BenchmarkRunner:item id=5abd94525542992ac4f382d2\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/assistants \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:assistant id: asst_759510322770870272\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/threads/runs \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:run, id=run_759510322846367744, thread_id=thread_759510322846367745\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510322846367745/runs/run_759510322846367744 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510322846367745/runs/run_759510322846367744 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510322846367745/runs/run_759510322846367744 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510322846367745/runs/run_759510322846367744 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510322846367745/runs/run_759510322846367744 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510322846367745/runs/run_759510322846367744 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510322846367745/runs/run_759510322846367744 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510322846367745/runs/run_759510322846367744 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510322846367745/runs/run_759510322846367744 \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:got tool call: {\"id\":\"call_759510355725516800\",\"function\":{\"arguments\":\"{\\\"query\\\": \\\"2014 S/S debut album South Korean boy group\\\"}\",\"name\":\"tavily\\\\_search\\\\_results\\\\_json\"},\"type\":\"function\"}\n",
      "ERROR:BenchmarkRunner:Unknown tool call occurred, function name tavily\\_search\\_results\\_json\n",
      "ERROR:BenchmarkRunner:Not every call is responded.\n",
      "INFO:BenchmarkRunner:id=5abd94525542992ac4f382d2, ok=False\n",
      "INFO:BenchmarkRunner:item id=5a85b2d95542997b5ce40028\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/assistants \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:assistant id: asst_759510357705228288\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/threads/runs \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:run, id=run_759510357784920064, thread_id=thread_759510357784920065\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510357784920065/runs/run_759510357784920064 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510357784920065/runs/run_759510357784920064 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510357784920065/runs/run_759510357784920064 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510357784920065/runs/run_759510357784920064 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510357784920065/runs/run_759510357784920064 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510357784920065/runs/run_759510357784920064 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510357784920065/runs/run_759510357784920064 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510357784920065/runs/run_759510357784920064 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510357784920065/runs/run_759510357784920064 \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:got tool call: {\"id\":\"call_759510389678407680\",\"function\":{\"arguments\":\"<search\\\\_query=\\\"Aladin stage name\\\" broaden=\\\"true\\\">\",\"name\":\"tavily\\\\_search\\\\_results\\\\_json\"},\"type\":\"function\"}\n",
      "ERROR:BenchmarkRunner:Unknown tool call occurred, function name tavily\\_search\\_results\\_json\n",
      "ERROR:BenchmarkRunner:Not every call is responded.\n",
      "INFO:BenchmarkRunner:id=5a85b2d95542997b5ce40028, ok=False\n",
      "INFO:BenchmarkRunner:item id=5a87ab905542996e4f3088c1\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/assistants \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:assistant id: asst_759510392585060352\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/threads/runs \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:run, id=run_759510392664752128, thread_id=thread_759510392664752129\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510392664752129/runs/run_759510392664752128 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510392664752129/runs/run_759510392664752128 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510392664752129/runs/run_759510392664752128 \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:got tool call: {\"id\":\"call_759510400541655040\",\"function\":{\"arguments\":\" {\\\"query\\\": \\\"Lewiston Maineiacs home arena capacity\\\"}\",\"name\":\"tavily_search_results_json\"},\"type\":\"function\"}\n",
      "ERROR:BenchmarkRunner:Not every call is responded.\n",
      "INFO:BenchmarkRunner:id=5a87ab905542996e4f3088c1, ok=False\n",
      "INFO:BenchmarkRunner:item id=5a7bbb64554299042af8f7cc\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/assistants \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:assistant id: asst_759510415649538048\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/threads/runs \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:run, id=run_759510415725035520, thread_id=thread_759510415725035521\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510415725035521/runs/run_759510415725035520 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510415725035521/runs/run_759510415725035520 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510415725035521/runs/run_759510415725035520 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510415725035521/runs/run_759510415725035520 \"HTTP/1.1 200 OK\"\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/pydantic/main.py:398: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `int` but got `str` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "ERROR:BenchmarkRunner:run is in other terminal status: {\n",
      "  \"id\": \"run_759510415725035520\",\n",
      "  \"assistant_id\": \"asst_759510415649538048\",\n",
      "  \"created_at\": \"1715914300422000\",\n",
      "  \"expires_at\": 1715914900446826,\n",
      "  \"failed_at\": 1715914303120559,\n",
      "  \"object\": \"thread.run\",\n",
      "  \"response_format\": \"auto\",\n",
      "  \"started_at\": 1715914300446825,\n",
      "  \"status\": \"failed\",\n",
      "  \"thread_id\": \"thread_759510415725035521\",\n",
      "  \"truncation_strategy\": {\n",
      "    \"type\": \"auto\"\n",
      "  },\n",
      "  \"temperature\": 0.0,\n",
      "  \"top_p\": 0.0,\n",
      "  \"modified_at\": \"1715914303121000\"\n",
      "}\n",
      "INFO:BenchmarkRunner:id=5a7bbb64554299042af8f7cc, ok=False\n",
      "INFO:BenchmarkRunner:item id=5a8db19d5542994ba4e3dd00\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/assistants \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:assistant id: asst_759510428903538688\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/threads/runs \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:run, id=run_759510428970647552, thread_id=thread_759510428970647553\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510428970647553/runs/run_759510428970647552 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510428970647553/runs/run_759510428970647552 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510428970647553/runs/run_759510428970647552 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510428970647553/runs/run_759510428970647552 \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:got tool call: {\"id\":\"call_759510439468990464\",\"function\":{\"arguments\":\" {\\\"query\\\": \\\"Local H nationality\\\"} \",\"name\":\"tavily\\\\_search\\\\_results\\\\_json\"},\"type\":\"function\"}\n",
      "ERROR:BenchmarkRunner:Unknown tool call occurred, function name tavily\\_search\\_results\\_json\n",
      "ERROR:BenchmarkRunner:Not every call is responded.\n",
      "INFO:BenchmarkRunner:id=5a8db19d5542994ba4e3dd00, ok=False\n",
      "INFO:BenchmarkRunner:item id=5a7166395542994082a3e814\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/assistants \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:assistant id: asst_759510442203676672\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/threads/runs \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:run, id=run_759510442287562752, thread_id=thread_759510442287562753\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510442287562753/runs/run_759510442287562752 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510442287562753/runs/run_759510442287562752 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510442287562753/runs/run_759510442287562752 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510442287562753/runs/run_759510442287562752 \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:got tool call: {\"id\":\"call_759510452634910720\",\"function\":{\"arguments\":\"<{\\\"query\\\": \\\"university whose main campus is in Lawrence, Kansas and whose branch campuses are in the Kansas City metropolitan area\\\"}>\",\"name\":\"tavily\\\\_search\\\\_results\\\\_json\"},\"type\":\"function\"}\n",
      "ERROR:BenchmarkRunner:Unknown tool call occurred, function name tavily\\_search\\_results\\_json\n",
      "ERROR:BenchmarkRunner:Not every call is responded.\n",
      "INFO:BenchmarkRunner:id=5a7166395542994082a3e814, ok=False\n",
      "INFO:BenchmarkRunner:item id=5a877e5d5542993e715abf7d\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/assistants \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:assistant id: asst_759510455528980480\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/threads/runs \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:run, id=run_759510455621255168, thread_id=thread_759510455621255169\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510455621255169/runs/run_759510455621255168 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510455621255169/runs/run_759510455621255168 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510455621255169/runs/run_759510455621255168 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510455621255169/runs/run_759510455621255168 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510455621255169/runs/run_759510455621255168 \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:got tool call: {\"id\":\"call_759510469277908992\",\"function\":{\"arguments\":\" {\\\"query\\\": \\\"screenwriter with credits for 'Evolution'\\\"} \",\"name\":\"tavily_search\\\\_results\\\\_json\"},\"type\":\"function\"}\n",
      "ERROR:BenchmarkRunner:Unknown tool call occurred, function name tavily_search\\_results\\_json\n",
      "ERROR:BenchmarkRunner:Not every call is responded.\n",
      "INFO:BenchmarkRunner:id=5a877e5d5542993e715abf7d, ok=False\n",
      "INFO:BenchmarkRunner:item id=5ab3b0bf5542992ade7c6e39\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/assistants \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:assistant id: asst_759510473228943360\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/threads/runs \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:run, id=run_759510473325412352, thread_id=thread_759510473325412353\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510473325412353/runs/run_759510473325412352 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510473325412353/runs/run_759510473325412352 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510473325412353/runs/run_759510473325412352 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510473325412353/runs/run_759510473325412352 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510473325412353/runs/run_759510473325412352 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510473325412353/runs/run_759510473325412352 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510473325412353/runs/run_759510473325412352 \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:got tool call: {\"id\":\"call_759510498113748992\",\"function\":{\"arguments\":\"<{\\\"type\\\": \\\"object\\\", \\\"properties\\\": {\\\"query\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"search query to look up\\\"}}, \\\"required\\\": [\\\"query\\\"], \\\"query\\\": \\\"Guns N Roses Arnold Schwarzenegger movie promo\\\"}>\",\"name\":\"tavily_search\\\\_results\\\\_json\"},\"type\":\"function\"}\n",
      "ERROR:BenchmarkRunner:Unknown tool call occurred, function name tavily_search\\_results\\_json\n",
      "ERROR:BenchmarkRunner:Not every call is responded.\n",
      "INFO:BenchmarkRunner:id=5ab3b0bf5542992ade7c6e39, ok=False\n",
      "INFO:BenchmarkRunner:item id=5ab56e32554299637185c594\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/assistants \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:assistant id: asst_759510499518840832\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/threads/runs \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:run, id=run_759510499594338304, thread_id=thread_759510499594338305\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510499594338305/runs/run_759510499594338304 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510499594338305/runs/run_759510499594338304 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510499594338305/runs/run_759510499594338304 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510499594338305/runs/run_759510499594338304 \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:got tool call: {\"id\":\"call_759510510818295808\",\"function\":{\"arguments\":\" {\\\"query\\\": \\\"Random House Tower usage\\\"} \",\"name\":\"tavily_search\\\\_results\\\\_json\"},\"type\":\"function\"}\n",
      "ERROR:BenchmarkRunner:Unknown tool call occurred, function name tavily_search\\_results\\_json\n",
      "ERROR:BenchmarkRunner:Not every call is responded.\n",
      "INFO:BenchmarkRunner:id=5ab56e32554299637185c594, ok=False\n",
      "INFO:BenchmarkRunner:item id=5ab6d09255429954757d337d\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/assistants \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:assistant id: asst_759510512835756032\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/threads/runs \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:run, id=run_759510512923836416, thread_id=thread_759510512923836417\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510512923836417/runs/run_759510512923836416 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510512923836417/runs/run_759510512923836416 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510512923836417/runs/run_759510512923836416 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510512923836417/runs/run_759510512923836416 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510512923836417/runs/run_759510512923836416 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510512923836417/runs/run_759510512923836416 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510512923836417/runs/run_759510512923836416 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510512923836417/runs/run_759510512923836416 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510512923836417/runs/run_759510512923836416 \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:got tool call: {\"id\":\"call_759510545777819648\",\"function\":{\"arguments\":\" {\\\"query\\\": \\\"Manchester United managers\\\"}\",\"name\":\"tavily\\\\_search\\\\_results\\\\_json\"},\"type\":\"function\"}\n",
      "ERROR:BenchmarkRunner:Unknown tool call occurred, function name tavily\\_search\\_results\\_json\n",
      "ERROR:BenchmarkRunner:Not every call is responded.\n",
      "INFO:BenchmarkRunner:id=5ab6d09255429954757d337d, ok=False\n",
      "INFO:BenchmarkRunner:item id=5a75e05c55429976ec32bc5f\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/assistants \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:assistant id: asst_759510547900137472\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/threads/runs \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:run, id=run_759510547975634944, thread_id=thread_759510547975634945\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510547975634945/runs/run_759510547975634944 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510547975634945/runs/run_759510547975634944 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510547975634945/runs/run_759510547975634944 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510547975634945/runs/run_759510547975634944 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510547975634945/runs/run_759510547975634944 \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:got tool call: {\"id\":\"call_759510563767189504\",\"function\":{\"arguments\":\"{\\\"query\\\": \\\"Brown State Fishing Lake\\\"}\",\"name\":\"tavily\\\\_search\\\\_results\\\\_json\"},\"type\":\"function\"}\n",
      "ERROR:BenchmarkRunner:Unknown tool call occurred, function name tavily\\_search\\_results\\_json\n",
      "ERROR:BenchmarkRunner:Not every call is responded.\n",
      "INFO:BenchmarkRunner:id=5a75e05c55429976ec32bc5f, ok=False\n",
      "INFO:BenchmarkRunner:item id=5ab3e45655429976abd1bcd4\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/assistants \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:assistant id: asst_759510565616877568\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/threads/runs \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:run, id=run_759510565696569344, thread_id=thread_759510565696569345\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510565696569345/runs/run_759510565696569344 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510565696569345/runs/run_759510565696569344 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510565696569345/runs/run_759510565696569344 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510565696569345/runs/run_759510565696569344 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510565696569345/runs/run_759510565696569344 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510565696569345/runs/run_759510565696569344 \"HTTP/1.1 200 OK\"\n",
      "ERROR:BenchmarkRunner:run is in other terminal status: {\n",
      "  \"id\": \"run_759510565696569344\",\n",
      "  \"assistant_id\": \"asst_759510565616877568\",\n",
      "  \"created_at\": \"1715914336177000\",\n",
      "  \"expires_at\": 1715914936197357,\n",
      "  \"failed_at\": 1715914340820770,\n",
      "  \"object\": \"thread.run\",\n",
      "  \"response_format\": \"auto\",\n",
      "  \"started_at\": 1715914336197357,\n",
      "  \"status\": \"failed\",\n",
      "  \"thread_id\": \"thread_759510565696569345\",\n",
      "  \"truncation_strategy\": {\n",
      "    \"type\": \"auto\"\n",
      "  },\n",
      "  \"temperature\": 0.0,\n",
      "  \"top_p\": 0.0,\n",
      "  \"modified_at\": \"1715914340821000\"\n",
      "}\n",
      "INFO:BenchmarkRunner:id=5ab3e45655429976abd1bcd4, ok=False\n",
      "INFO:BenchmarkRunner:item id=5ab29c24554299449642c932\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/assistants \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:assistant id: asst_759510587544698880\n",
      "INFO:httpx:HTTP Request: POST http://localhost:9091/v1/threads/runs \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:run, id=run_759510587624390656, thread_id=thread_759510587624390657\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510587624390657/runs/run_759510587624390656 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510587624390657/runs/run_759510587624390656 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510587624390657/runs/run_759510587624390656 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510587624390657/runs/run_759510587624390656 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:9091/v1/threads/thread_759510587624390657/runs/run_759510587624390656 \"HTTP/1.1 200 OK\"\n",
      "INFO:BenchmarkRunner:got tool call: {\"id\":\"call_759510600526069760\",\"function\":{\"arguments\":\" {\\\"query\\\": \\\"Giuseppe Verdi biography\\\"} \",\"name\":\"tavily\\\\_search\\\\_results\\\\_json\"},\"type\":\"function\"}\n",
      "ERROR:BenchmarkRunner:Unknown tool call occurred, function name tavily\\_search\\_results\\_json\n",
      "ERROR:BenchmarkRunner:Not every call is responded.\n",
      "INFO:BenchmarkRunner:id=5ab29c24554299449642c932, ok=False\n",
      "INFO:BenchmarkRunner:item id=5ae0d4c9554299603e418468\n",
      "INFO:openai._base_client:Retrying request to /assistants in 0.900510 seconds\n",
      "INFO:openai._base_client:Retrying request to /assistants in 1.812212 seconds\n",
      "INFO:BenchmarkRunner:delete thread thread_759295601195089921\n",
      "INFO:openai._base_client:Retrying request to /threads/thread_759295601195089921 in 0.976470 seconds\n",
      "INFO:openai._base_client:Retrying request to /threads/thread_759295601195089921 in 1.737568 seconds\n",
      "ERROR:BenchmarkRunner:Connection error.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mConnectError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/httpx/_transports/default.py:69\u001B[0m, in \u001B[0;36mmap_httpcore_exceptions\u001B[0;34m()\u001B[0m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 69\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/httpx/_transports/default.py:233\u001B[0m, in \u001B[0;36mHTTPTransport.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[0;32m--> 233\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pool\u001B[38;5;241m.\u001B[39mhandle_request(req)\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp\u001B[38;5;241m.\u001B[39mstream, typing\u001B[38;5;241m.\u001B[39mIterable)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001B[0m, in \u001B[0;36mConnectionPool.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_connections(closing)\n\u001B[0;32m--> 216\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    218\u001B[0m \u001B[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001B[39;00m\n\u001B[1;32m    219\u001B[0m \u001B[38;5;66;03m# the point at which the response is closed.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001B[0m, in \u001B[0;36mConnectionPool.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    194\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    195\u001B[0m     \u001B[38;5;66;03m# Send the request on the assigned connection.\u001B[39;00m\n\u001B[0;32m--> 196\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39mhandle_request(\n\u001B[1;32m    197\u001B[0m         pool_request\u001B[38;5;241m.\u001B[39mrequest\n\u001B[1;32m    198\u001B[0m     )\n\u001B[1;32m    199\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ConnectionNotAvailable:\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;66;03m# In some cases a connection may initially be available to\u001B[39;00m\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;66;03m# handle a request, but then become unavailable.\u001B[39;00m\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;66;03m# In this case we clear the connection and try again.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/httpcore/_sync/connection.py:99\u001B[0m, in \u001B[0;36mHTTPConnection.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connect_failed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 99\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connection\u001B[38;5;241m.\u001B[39mhandle_request(request)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/httpcore/_sync/connection.py:76\u001B[0m, in \u001B[0;36mHTTPConnection.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connection \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 76\u001B[0m     stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connect(request)\n\u001B[1;32m     78\u001B[0m     ssl_object \u001B[38;5;241m=\u001B[39m stream\u001B[38;5;241m.\u001B[39mget_extra_info(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mssl_object\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/httpcore/_sync/connection.py:122\u001B[0m, in \u001B[0;36mHTTPConnection._connect\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconnect_tcp\u001B[39m\u001B[38;5;124m\"\u001B[39m, logger, request, kwargs) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[0;32m--> 122\u001B[0m     stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_network_backend\u001B[38;5;241m.\u001B[39mconnect_tcp(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    123\u001B[0m     trace\u001B[38;5;241m.\u001B[39mreturn_value \u001B[38;5;241m=\u001B[39m stream\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/httpcore/_backends/sync.py:205\u001B[0m, in \u001B[0;36mSyncBackend.connect_tcp\u001B[0;34m(self, host, port, timeout, local_address, socket_options)\u001B[0m\n\u001B[1;32m    200\u001B[0m exc_map: ExceptionMapping \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    201\u001B[0m     socket\u001B[38;5;241m.\u001B[39mtimeout: ConnectTimeout,\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;167;01mOSError\u001B[39;00m: ConnectError,\n\u001B[1;32m    203\u001B[0m }\n\u001B[0;32m--> 205\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[1;32m    206\u001B[0m     sock \u001B[38;5;241m=\u001B[39m socket\u001B[38;5;241m.\u001B[39mcreate_connection(\n\u001B[1;32m    207\u001B[0m         address,\n\u001B[1;32m    208\u001B[0m         timeout,\n\u001B[1;32m    209\u001B[0m         source_address\u001B[38;5;241m=\u001B[39msource_address,\n\u001B[1;32m    210\u001B[0m     )\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/contextlib.py:158\u001B[0m, in \u001B[0;36m_GeneratorContextManager.__exit__\u001B[0;34m(self, typ, value, traceback)\u001B[0m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 158\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgen\u001B[38;5;241m.\u001B[39mthrow(typ, value, traceback)\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m    160\u001B[0m     \u001B[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001B[39;00m\n\u001B[1;32m    161\u001B[0m     \u001B[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001B[39;00m\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/httpcore/_exceptions.py:14\u001B[0m, in \u001B[0;36mmap_exceptions\u001B[0;34m(map)\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(exc, from_exc):\n\u001B[0;32m---> 14\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m to_exc(exc) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexc\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mConnectError\u001B[0m: [Errno 61] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mConnectError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/openai/_base_client.py:952\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m    951\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 952\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39msend(\n\u001B[1;32m    953\u001B[0m         request,\n\u001B[1;32m    954\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_stream_response_body(request\u001B[38;5;241m=\u001B[39mrequest),\n\u001B[1;32m    955\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    956\u001B[0m     )\n\u001B[1;32m    957\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mTimeoutException \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/httpx/_client.py:914\u001B[0m, in \u001B[0;36mClient.send\u001B[0;34m(self, request, stream, auth, follow_redirects)\u001B[0m\n\u001B[1;32m    912\u001B[0m auth \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_request_auth(request, auth)\n\u001B[0;32m--> 914\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_send_handling_auth(\n\u001B[1;32m    915\u001B[0m     request,\n\u001B[1;32m    916\u001B[0m     auth\u001B[38;5;241m=\u001B[39mauth,\n\u001B[1;32m    917\u001B[0m     follow_redirects\u001B[38;5;241m=\u001B[39mfollow_redirects,\n\u001B[1;32m    918\u001B[0m     history\u001B[38;5;241m=\u001B[39m[],\n\u001B[1;32m    919\u001B[0m )\n\u001B[1;32m    920\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/httpx/_client.py:942\u001B[0m, in \u001B[0;36mClient._send_handling_auth\u001B[0;34m(self, request, auth, follow_redirects, history)\u001B[0m\n\u001B[1;32m    941\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 942\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_send_handling_redirects(\n\u001B[1;32m    943\u001B[0m         request,\n\u001B[1;32m    944\u001B[0m         follow_redirects\u001B[38;5;241m=\u001B[39mfollow_redirects,\n\u001B[1;32m    945\u001B[0m         history\u001B[38;5;241m=\u001B[39mhistory,\n\u001B[1;32m    946\u001B[0m     )\n\u001B[1;32m    947\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/httpx/_client.py:979\u001B[0m, in \u001B[0;36mClient._send_handling_redirects\u001B[0;34m(self, request, follow_redirects, history)\u001B[0m\n\u001B[1;32m    977\u001B[0m     hook(request)\n\u001B[0;32m--> 979\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_send_single_request(request)\n\u001B[1;32m    980\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/httpx/_client.py:1015\u001B[0m, in \u001B[0;36mClient._send_single_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m   1014\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request\u001B[38;5;241m=\u001B[39mrequest):\n\u001B[0;32m-> 1015\u001B[0m     response \u001B[38;5;241m=\u001B[39m transport\u001B[38;5;241m.\u001B[39mhandle_request(request)\n\u001B[1;32m   1017\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response\u001B[38;5;241m.\u001B[39mstream, SyncByteStream)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/httpx/_transports/default.py:232\u001B[0m, in \u001B[0;36mHTTPTransport.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    220\u001B[0m req \u001B[38;5;241m=\u001B[39m httpcore\u001B[38;5;241m.\u001B[39mRequest(\n\u001B[1;32m    221\u001B[0m     method\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[1;32m    222\u001B[0m     url\u001B[38;5;241m=\u001B[39mhttpcore\u001B[38;5;241m.\u001B[39mURL(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    230\u001B[0m     extensions\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mextensions,\n\u001B[1;32m    231\u001B[0m )\n\u001B[0;32m--> 232\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[1;32m    233\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pool\u001B[38;5;241m.\u001B[39mhandle_request(req)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/contextlib.py:158\u001B[0m, in \u001B[0;36m_GeneratorContextManager.__exit__\u001B[0;34m(self, typ, value, traceback)\u001B[0m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 158\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgen\u001B[38;5;241m.\u001B[39mthrow(typ, value, traceback)\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m    160\u001B[0m     \u001B[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001B[39;00m\n\u001B[1;32m    161\u001B[0m     \u001B[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001B[39;00m\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/httpx/_transports/default.py:86\u001B[0m, in \u001B[0;36mmap_httpcore_exceptions\u001B[0;34m()\u001B[0m\n\u001B[1;32m     85\u001B[0m message \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(exc)\n\u001B[0;32m---> 86\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m mapped_exc(message) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexc\u001B[39;00m\n",
      "\u001B[0;31mConnectError\u001B[0m: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[85], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m client \u001B[38;5;241m=\u001B[39m OpenAI(base_url\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttp://localhost:9091/v1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m BenchmarkRunner(client\u001B[38;5;241m=\u001B[39mclient, model_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\u001B[39m\u001B[38;5;124m\"\u001B[39m, output_file_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./output/miniassistant_result.json\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m benchmark_runner:\n\u001B[0;32m----> 6\u001B[0m     benchmark_runner\u001B[38;5;241m.\u001B[39mrun()\n\u001B[1;32m      7\u001B[0m     benchmark_runner\u001B[38;5;241m.\u001B[39mget_metrics()\n",
      "Cell \u001B[0;32mIn[67], line 109\u001B[0m, in \u001B[0;36mBenchmarkRunner.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m load_dataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscholarly-shadows-syndicate/hotpotqa_with_qa_gpt35\u001B[39m\u001B[38;5;124m\"\u001B[39m, split\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation\u001B[39m\u001B[38;5;124m\"\u001B[39m, streaming\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39mfilter(\u001B[38;5;28;01mlambda\u001B[39;00m x: x[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlevel\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhard\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mitem id=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mitem[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)   \n\u001B[0;32m--> 109\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39massistant \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mbeta\u001B[38;5;241m.\u001B[39massistants\u001B[38;5;241m.\u001B[39mcreate(\n\u001B[1;32m    110\u001B[0m         name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbenchmark-runner\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    111\u001B[0m         model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_name,\n\u001B[1;32m    112\u001B[0m         tools\u001B[38;5;241m=\u001B[39m[{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunction\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunction\u001B[39m\u001B[38;5;124m\"\u001B[39m: convert_to_openai_function(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtavily_tool)}]\n\u001B[1;32m    113\u001B[0m     )\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124massistant id: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39massistant\u001B[38;5;241m.\u001B[39mid\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    116\u001B[0m     run \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mbeta\u001B[38;5;241m.\u001B[39mthreads\u001B[38;5;241m.\u001B[39mcreate_and_run(\n\u001B[1;32m    117\u001B[0m         assistant_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39massistant\u001B[38;5;241m.\u001B[39mid,\n\u001B[1;32m    118\u001B[0m         thread\u001B[38;5;241m=\u001B[39m{\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    122\u001B[0m         },\n\u001B[1;32m    123\u001B[0m         stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/openai/resources/beta/assistants.py:153\u001B[0m, in \u001B[0;36mAssistants.create\u001B[0;34m(self, model, description, instructions, metadata, name, response_format, temperature, tool_resources, tools, top_p, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;124;03mCreate an assistant with a model and instructions.\u001B[39;00m\n\u001B[1;32m     90\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;124;03m  timeout: Override the client-level default timeout for this request, in seconds\u001B[39;00m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    152\u001B[0m extra_headers \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOpenAI-Beta\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124massistants=v2\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m(extra_headers \u001B[38;5;129;01mor\u001B[39;00m {})}\n\u001B[0;32m--> 153\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post(\n\u001B[1;32m    154\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/assistants\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    155\u001B[0m     body\u001B[38;5;241m=\u001B[39mmaybe_transform(\n\u001B[1;32m    156\u001B[0m         {\n\u001B[1;32m    157\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m: model,\n\u001B[1;32m    158\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdescription\u001B[39m\u001B[38;5;124m\"\u001B[39m: description,\n\u001B[1;32m    159\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstructions\u001B[39m\u001B[38;5;124m\"\u001B[39m: instructions,\n\u001B[1;32m    160\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[1;32m    161\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m: name,\n\u001B[1;32m    162\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse_format\u001B[39m\u001B[38;5;124m\"\u001B[39m: response_format,\n\u001B[1;32m    163\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemperature\u001B[39m\u001B[38;5;124m\"\u001B[39m: temperature,\n\u001B[1;32m    164\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtool_resources\u001B[39m\u001B[38;5;124m\"\u001B[39m: tool_resources,\n\u001B[1;32m    165\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtools\u001B[39m\u001B[38;5;124m\"\u001B[39m: tools,\n\u001B[1;32m    166\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_p\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_p,\n\u001B[1;32m    167\u001B[0m         },\n\u001B[1;32m    168\u001B[0m         assistant_create_params\u001B[38;5;241m.\u001B[39mAssistantCreateParams,\n\u001B[1;32m    169\u001B[0m     ),\n\u001B[1;32m    170\u001B[0m     options\u001B[38;5;241m=\u001B[39mmake_request_options(\n\u001B[1;32m    171\u001B[0m         extra_headers\u001B[38;5;241m=\u001B[39mextra_headers, extra_query\u001B[38;5;241m=\u001B[39mextra_query, extra_body\u001B[38;5;241m=\u001B[39mextra_body, timeout\u001B[38;5;241m=\u001B[39mtimeout\n\u001B[1;32m    172\u001B[0m     ),\n\u001B[1;32m    173\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mAssistant,\n\u001B[1;32m    174\u001B[0m )\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/openai/_base_client.py:1240\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[1;32m   1227\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1228\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1235\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1236\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m   1237\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[1;32m   1238\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[1;32m   1239\u001B[0m     )\n\u001B[0;32m-> 1240\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest(cast_to, opts, stream\u001B[38;5;241m=\u001B[39mstream, stream_cls\u001B[38;5;241m=\u001B[39mstream_cls))\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/openai/_base_client.py:921\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m    912\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[1;32m    913\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    914\u001B[0m     cast_to: Type[ResponseT],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    919\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    920\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m--> 921\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\n\u001B[1;32m    922\u001B[0m         cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[1;32m    923\u001B[0m         options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m    924\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m    925\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[1;32m    926\u001B[0m         remaining_retries\u001B[38;5;241m=\u001B[39mremaining_retries,\n\u001B[1;32m    927\u001B[0m     )\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/openai/_base_client.py:976\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m    973\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEncountered Exception\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    975\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retries \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 976\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retry_request(\n\u001B[1;32m    977\u001B[0m         options,\n\u001B[1;32m    978\u001B[0m         cast_to,\n\u001B[1;32m    979\u001B[0m         retries,\n\u001B[1;32m    980\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m    981\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[1;32m    982\u001B[0m         response_headers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    983\u001B[0m     )\n\u001B[1;32m    985\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRaising connection error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    986\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m APIConnectionError(request\u001B[38;5;241m=\u001B[39mrequest) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/openai/_base_client.py:1053\u001B[0m, in \u001B[0;36mSyncAPIClient._retry_request\u001B[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1049\u001B[0m \u001B[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001B[39;00m\n\u001B[1;32m   1050\u001B[0m \u001B[38;5;66;03m# different thread if necessary.\u001B[39;00m\n\u001B[1;32m   1051\u001B[0m time\u001B[38;5;241m.\u001B[39msleep(timeout)\n\u001B[0;32m-> 1053\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\n\u001B[1;32m   1054\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m   1055\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[1;32m   1056\u001B[0m     remaining_retries\u001B[38;5;241m=\u001B[39mremaining,\n\u001B[1;32m   1057\u001B[0m     stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m   1058\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[1;32m   1059\u001B[0m )\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/openai/_base_client.py:976\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m    973\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEncountered Exception\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    975\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retries \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 976\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retry_request(\n\u001B[1;32m    977\u001B[0m         options,\n\u001B[1;32m    978\u001B[0m         cast_to,\n\u001B[1;32m    979\u001B[0m         retries,\n\u001B[1;32m    980\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m    981\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[1;32m    982\u001B[0m         response_headers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    983\u001B[0m     )\n\u001B[1;32m    985\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRaising connection error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    986\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m APIConnectionError(request\u001B[38;5;241m=\u001B[39mrequest) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/instinct-lab/lib/python3.11/site-packages/openai/_base_client.py:1051\u001B[0m, in \u001B[0;36mSyncAPIClient._retry_request\u001B[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1047\u001B[0m log\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRetrying request to \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m in \u001B[39m\u001B[38;5;132;01m%f\u001B[39;00m\u001B[38;5;124m seconds\u001B[39m\u001B[38;5;124m\"\u001B[39m, options\u001B[38;5;241m.\u001B[39murl, timeout)\n\u001B[1;32m   1049\u001B[0m \u001B[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001B[39;00m\n\u001B[1;32m   1050\u001B[0m \u001B[38;5;66;03m# different thread if necessary.\u001B[39;00m\n\u001B[0;32m-> 1051\u001B[0m time\u001B[38;5;241m.\u001B[39msleep(timeout)\n\u001B[1;32m   1053\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\n\u001B[1;32m   1054\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m   1055\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1058\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[1;32m   1059\u001B[0m )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## With OpenAI's offering\n",
    "\n",
    "Please make sure you have `OPENAI_API_KEY` setup in your environments.\n"
   ],
   "id": "f70bbead8d83f756"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T10:14:38.564386Z",
     "start_time": "2024-05-16T10:14:38.484966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if True:\n",
    "    if not os.path.exists(\"./output\"):\n",
    "        os.mkdir(\"./output\")\n",
    "    client = OpenAI()\n",
    "    with BenchmarkRunner(client=client, model_name=\"gpt-3.5-turbo\", output_file_path=\"./output/openai_result.json\") as benchmark_runner:\n",
    "        # benchmark_runner.run()\n",
    "        benchmark_runner.get_metrics()"
   ],
   "id": "8e103a1a13e346e6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Success rate: 0.3333333333333333\n",
      "INFO:root:Accuracy: 0.0\n",
      "INFO:root:Latency: 3.13826158841451 +/- 2.1034252851239037\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T08:20:39.969347Z",
     "start_time": "2024-05-16T08:20:39.969284Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "4168e98a6825ce01",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
